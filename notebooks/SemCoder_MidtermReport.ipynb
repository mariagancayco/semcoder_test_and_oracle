{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ddda6f48631c47c3b8801fbc5806a71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48da9154f8c2415690fcd0cfce4efea6",
              "IPY_MODEL_13d79001163742b4a7032fb96421d622",
              "IPY_MODEL_6b274135602d4e488f1f5e80747ce0ed"
            ],
            "layout": "IPY_MODEL_bd28af9b1ee84d6c8d43b1a766bfb7e5"
          }
        },
        "48da9154f8c2415690fcd0cfce4efea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954c3bf683f94ddfa7dc35181c4165bf",
            "placeholder": "​",
            "style": "IPY_MODEL_948919ac87be4196b958455eb2edb32e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "13d79001163742b4a7032fb96421d622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0817a9a0ba2493da53441c71d2fcf3b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bd868ad102b4a4f8a36b382b4a74220",
            "value": 2
          }
        },
        "6b274135602d4e488f1f5e80747ce0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d2def7e0224cf6990e9979da8b0815",
            "placeholder": "​",
            "style": "IPY_MODEL_b357f7f8020440a4a228b04711f2b71d",
            "value": " 2/2 [01:03&lt;00:00, 29.02s/it]"
          }
        },
        "bd28af9b1ee84d6c8d43b1a766bfb7e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "954c3bf683f94ddfa7dc35181c4165bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "948919ac87be4196b958455eb2edb32e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0817a9a0ba2493da53441c71d2fcf3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd868ad102b4a4f8a36b382b4a74220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1d2def7e0224cf6990e9979da8b0815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b357f7f8020440a4a228b04711f2b71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c18ddf333754617a8019b69d70e9692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a49049d68854433d99ed756cbb96ffc1",
              "IPY_MODEL_9b6c8fce53c84d9499660b8d5be6efea",
              "IPY_MODEL_e53f2da5b0b447daaaa463c42ca7db70"
            ],
            "layout": "IPY_MODEL_998c7ee23ff64b61881306553382e8ff"
          }
        },
        "a49049d68854433d99ed756cbb96ffc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ecc832797c4c028102869be152ceeb",
            "placeholder": "​",
            "style": "IPY_MODEL_4ca29893312e417099586ca3b0e6e7d7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9b6c8fce53c84d9499660b8d5be6efea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95fdfce28a714c6e82674ceb642f2a7c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b1c4fcf72f345d392bd498651cf5157",
            "value": 3
          }
        },
        "e53f2da5b0b447daaaa463c42ca7db70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f794df291254f4c90b8b37e3f78a060",
            "placeholder": "​",
            "style": "IPY_MODEL_62995ef68fbc4c4c8b6b24859554e3c0",
            "value": " 3/3 [00:10&lt;00:00,  3.35s/it]"
          }
        },
        "998c7ee23ff64b61881306553382e8ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ecc832797c4c028102869be152ceeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca29893312e417099586ca3b0e6e7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95fdfce28a714c6e82674ceb642f2a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b1c4fcf72f345d392bd498651cf5157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f794df291254f4c90b8b37e3f78a060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62995ef68fbc4c4c8b6b24859554e3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19c4ec4df401439195571d6ad430f03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4829247ce0054f94be3a782141aaa8b0",
              "IPY_MODEL_225f678733c3429f9e40730a51831a2e",
              "IPY_MODEL_81ac4addca0c4f23b8fa0460a07c068d"
            ],
            "layout": "IPY_MODEL_2e458380c66446a78dee0c31b4653ad4"
          }
        },
        "4829247ce0054f94be3a782141aaa8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2859dc3a4a7a4ea78f03d93e480c7e37",
            "placeholder": "​",
            "style": "IPY_MODEL_0d20ece86a0547e3ba9cb1b0f558407c",
            "value": "README.md: 100%"
          }
        },
        "225f678733c3429f9e40730a51831a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11c727d75b144799a08ce360320332c",
            "max": 12909,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa6c0df8534d4144b5510a6b2e68f68a",
            "value": 12909
          }
        },
        "81ac4addca0c4f23b8fa0460a07c068d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc188a91b5454acea7d1f97cf67f87ba",
            "placeholder": "​",
            "style": "IPY_MODEL_0d3a0129ced94a49b78b5df51437dc12",
            "value": " 12.9k/12.9k [00:00&lt;00:00, 1.14MB/s]"
          }
        },
        "2e458380c66446a78dee0c31b4653ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2859dc3a4a7a4ea78f03d93e480c7e37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d20ece86a0547e3ba9cb1b0f558407c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11c727d75b144799a08ce360320332c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6c0df8534d4144b5510a6b2e68f68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc188a91b5454acea7d1f97cf67f87ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3a0129ced94a49b78b5df51437dc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e7ab72766b649df976c582fd563369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdcc5c7eef5a4edeb961b0c66b8c4050",
              "IPY_MODEL_327a4ec4945b4eda9c2c8dc53814d222",
              "IPY_MODEL_fd188418cb114ddc9130cee9e020ee19"
            ],
            "layout": "IPY_MODEL_b346a2ec23ac45b1a6c47d6594a4cb1b"
          }
        },
        "bdcc5c7eef5a4edeb961b0c66b8c4050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bb13e27ec52475aaa48fae75de184a8",
            "placeholder": "​",
            "style": "IPY_MODEL_f9a0a8293faf458d850ae08f92b75d5e",
            "value": "code_search_net.py: 100%"
          }
        },
        "327a4ec4945b4eda9c2c8dc53814d222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b542683d3f1e4f24b2de613a82109338",
            "max": 8440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a83418a1adb1434e88659c57e3633872",
            "value": 8440
          }
        },
        "fd188418cb114ddc9130cee9e020ee19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96f53d4516d04776a40c4b81761a407f",
            "placeholder": "​",
            "style": "IPY_MODEL_5b5588c263104e62804bba259ca71812",
            "value": " 8.44k/8.44k [00:00&lt;00:00, 687kB/s]"
          }
        },
        "b346a2ec23ac45b1a6c47d6594a4cb1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bb13e27ec52475aaa48fae75de184a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a0a8293faf458d850ae08f92b75d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b542683d3f1e4f24b2de613a82109338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83418a1adb1434e88659c57e3633872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96f53d4516d04776a40c4b81761a407f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5588c263104e62804bba259ca71812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f1052040f9e445f9c01acead63a0968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8bcf8b0b76e454ba8ebcc48d0b24060",
              "IPY_MODEL_a614f8c210a04230ad4cdb63b1136e66",
              "IPY_MODEL_cf9b1fadeca04c6ab6ea61971547780f"
            ],
            "layout": "IPY_MODEL_3771a2d20b5841fc942760906afe7243"
          }
        },
        "a8bcf8b0b76e454ba8ebcc48d0b24060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc531772bfac4d228aa1e3a9c8781391",
            "placeholder": "​",
            "style": "IPY_MODEL_50e7c0f638434e7390c6a9b23e66889b",
            "value": "python.zip: 100%"
          }
        },
        "a614f8c210a04230ad4cdb63b1136e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd1e8671de6044679051882f6a7b8089",
            "max": 940909997,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e659049f8d94e6d9e0f51f4cd2b4339",
            "value": 940909997
          }
        },
        "cf9b1fadeca04c6ab6ea61971547780f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409532dfe8e44c278c984639b4a62840",
            "placeholder": "​",
            "style": "IPY_MODEL_795b1c5c1d3d475cae7e29c19958886d",
            "value": " 941M/941M [00:04&lt;00:00, 237MB/s]"
          }
        },
        "3771a2d20b5841fc942760906afe7243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc531772bfac4d228aa1e3a9c8781391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50e7c0f638434e7390c6a9b23e66889b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd1e8671de6044679051882f6a7b8089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e659049f8d94e6d9e0f51f4cd2b4339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "409532dfe8e44c278c984639b4a62840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "795b1c5c1d3d475cae7e29c19958886d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f708ae6037034eb2a884ad771d2564c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb4ed2e8a9fb45389d2e05416c186f19",
              "IPY_MODEL_74e686bde64f44daa0b8c9f38516913d",
              "IPY_MODEL_aac120b89276498b91d54a015eb47cc6"
            ],
            "layout": "IPY_MODEL_268407b6f4e04b7f91017145c723b970"
          }
        },
        "fb4ed2e8a9fb45389d2e05416c186f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3e152b11494c429d3b54ad9cb6ce62",
            "placeholder": "​",
            "style": "IPY_MODEL_6c4297a411db453aaf1f1d856e69c5eb",
            "value": "Generating train split: 100%"
          }
        },
        "74e686bde64f44daa0b8c9f38516913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf10fc1eec84b63b48344b46a670fa6",
            "max": 412178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71a3f23173ee44a09d176f8dbffd941d",
            "value": 412178
          }
        },
        "aac120b89276498b91d54a015eb47cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52215d0f51744af2b08d78a8b9fe089f",
            "placeholder": "​",
            "style": "IPY_MODEL_30995a1a000c480b9ead286256d411b0",
            "value": " 412178/412178 [03:03&lt;00:00, 2508.47 examples/s]"
          }
        },
        "268407b6f4e04b7f91017145c723b970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3e152b11494c429d3b54ad9cb6ce62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c4297a411db453aaf1f1d856e69c5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0bf10fc1eec84b63b48344b46a670fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a3f23173ee44a09d176f8dbffd941d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52215d0f51744af2b08d78a8b9fe089f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30995a1a000c480b9ead286256d411b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e7cf688001747d6a8d743e607073904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3335971477734f1888cd731856615d6e",
              "IPY_MODEL_ac9b1143dccd4bd0b759ae3244a4f354",
              "IPY_MODEL_7d941719d4104ec1886da2fb32e934ef"
            ],
            "layout": "IPY_MODEL_5e7ab2beec2242e7ae2a2543cc693673"
          }
        },
        "3335971477734f1888cd731856615d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f63d30dabd2d4c21866804edaa96bc1f",
            "placeholder": "​",
            "style": "IPY_MODEL_8ece531304844e6691f056f6b0743f24",
            "value": "Generating test split: 100%"
          }
        },
        "ac9b1143dccd4bd0b759ae3244a4f354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a47b146c10b4fa6b8fc4a68fad3961d",
            "max": 22176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32fadb5a2e62443e95d0fd6b282f2ea1",
            "value": 22176
          }
        },
        "7d941719d4104ec1886da2fb32e934ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8131d47077dc417493a336ad138a476f",
            "placeholder": "​",
            "style": "IPY_MODEL_3d0bc65796d4481887c984e300d107b7",
            "value": " 22176/22176 [00:09&lt;00:00, 2459.75 examples/s]"
          }
        },
        "5e7ab2beec2242e7ae2a2543cc693673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63d30dabd2d4c21866804edaa96bc1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ece531304844e6691f056f6b0743f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a47b146c10b4fa6b8fc4a68fad3961d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32fadb5a2e62443e95d0fd6b282f2ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8131d47077dc417493a336ad138a476f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d0bc65796d4481887c984e300d107b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7037d967a823447990856387519db5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82a405e046bf40a3887c07bb2e7ad640",
              "IPY_MODEL_b2020366bb64410aacbde9fb3ee5c5bb",
              "IPY_MODEL_abd7424b736b438d8f513616b30a7423"
            ],
            "layout": "IPY_MODEL_90e7bb42dd9f49e085179dcc769d3ce8"
          }
        },
        "82a405e046bf40a3887c07bb2e7ad640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_613e8244d92d4052b5e5540e2080b95e",
            "placeholder": "​",
            "style": "IPY_MODEL_4865fc99c4fd4e019d8d5acc425cda60",
            "value": "Generating validation split: 100%"
          }
        },
        "b2020366bb64410aacbde9fb3ee5c5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3dedd76209349ed99ab2dcf7633c24b",
            "max": 23107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5418e3ad23f04aef88ff145cd1794428",
            "value": 23107
          }
        },
        "abd7424b736b438d8f513616b30a7423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_973ff844f1584e01aef9ecaab1ce5c38",
            "placeholder": "​",
            "style": "IPY_MODEL_074f0e69b19642708c8c4e9f2f293310",
            "value": " 23107/23107 [00:10&lt;00:00, 2317.96 examples/s]"
          }
        },
        "90e7bb42dd9f49e085179dcc769d3ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "613e8244d92d4052b5e5540e2080b95e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4865fc99c4fd4e019d8d5acc425cda60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3dedd76209349ed99ab2dcf7633c24b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5418e3ad23f04aef88ff145cd1794428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "973ff844f1584e01aef9ecaab1ce5c38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "074f0e69b19642708c8c4e9f2f293310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06be9c451ac84dec8d2d8ed6e3d2765d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9690c03015d34ed59c5731b58be59e87",
              "IPY_MODEL_e6c9f80643a84fa58307aebff3e30949",
              "IPY_MODEL_73965977423846f593f3860d6455ee96"
            ],
            "layout": "IPY_MODEL_b195978fc2674ad3aa2afd1f39d8090a"
          }
        },
        "9690c03015d34ed59c5731b58be59e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a84ccc6c7484660b25e5cec55b7b382",
            "placeholder": "​",
            "style": "IPY_MODEL_d81c806343664e5bb23ab813dba46ba8",
            "value": "Map: 100%"
          }
        },
        "e6c9f80643a84fa58307aebff3e30949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11ecf5f4e43e4fb991c2502248bfac37",
            "max": 412178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1a0ff599b45444e807e679f4b8176af",
            "value": 412178
          }
        },
        "73965977423846f593f3860d6455ee96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fb52b6173fb47d3bd62089e39add48f",
            "placeholder": "​",
            "style": "IPY_MODEL_925aeaa1efd245b7a991ae647adcbd63",
            "value": " 412178/412178 [08:23&lt;00:00, 857.43 examples/s]"
          }
        },
        "b195978fc2674ad3aa2afd1f39d8090a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a84ccc6c7484660b25e5cec55b7b382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81c806343664e5bb23ab813dba46ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11ecf5f4e43e4fb991c2502248bfac37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a0ff599b45444e807e679f4b8176af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fb52b6173fb47d3bd62089e39add48f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925aeaa1efd245b7a991ae647adcbd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80f1be4bc0f440989fa54834cc23f3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_625df984e0b342308383ee977d2c5e66",
              "IPY_MODEL_1c5ec9de135949cfab536192422012de",
              "IPY_MODEL_6917dfcd9eab4f3db206af965fac41ac"
            ],
            "layout": "IPY_MODEL_530bced6d7104f23a59129a32e89d998"
          }
        },
        "625df984e0b342308383ee977d2c5e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be1cc3bd469741aeb7a45482c428f1b5",
            "placeholder": "​",
            "style": "IPY_MODEL_be25139eaf2440048a252fb8eb988698",
            "value": "Map: 100%"
          }
        },
        "1c5ec9de135949cfab536192422012de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f679fd4144848769e668ada81f2806f",
            "max": 22176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0509adf1a9e4271af1cc56bb8ff9079",
            "value": 22176
          }
        },
        "6917dfcd9eab4f3db206af965fac41ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c60c41fa730644b79db2c6ba98c87b3f",
            "placeholder": "​",
            "style": "IPY_MODEL_160d874001744345b64310d5606c2010",
            "value": " 22176/22176 [00:27&lt;00:00, 821.77 examples/s]"
          }
        },
        "530bced6d7104f23a59129a32e89d998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1cc3bd469741aeb7a45482c428f1b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be25139eaf2440048a252fb8eb988698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f679fd4144848769e668ada81f2806f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0509adf1a9e4271af1cc56bb8ff9079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c60c41fa730644b79db2c6ba98c87b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160d874001744345b64310d5606c2010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "161aac2bf91947f183444437c208bcca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c8195ccde864bb98c386d7a69a77b41",
              "IPY_MODEL_92c65a97f66a4a76b3fdb4fda333f6a1",
              "IPY_MODEL_6372dda30b714621a5d287f2c2bc6699"
            ],
            "layout": "IPY_MODEL_63844507105248fc9ea144f3b1d57d15"
          }
        },
        "5c8195ccde864bb98c386d7a69a77b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3680ec570f444fa4bf54abe70e0155a2",
            "placeholder": "​",
            "style": "IPY_MODEL_5890611817124b1aaa4ed4028ce33457",
            "value": "Map: 100%"
          }
        },
        "92c65a97f66a4a76b3fdb4fda333f6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ffe736baa774469941a80be377ccbd2",
            "max": 23107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_849c92405fd6482e895d9d72b13eedd0",
            "value": 23107
          }
        },
        "6372dda30b714621a5d287f2c2bc6699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210f6abacea54e318353039d46eb31f0",
            "placeholder": "​",
            "style": "IPY_MODEL_6cd2a04b6a1d4123bf462eb371a822bc",
            "value": " 23107/23107 [00:29&lt;00:00, 835.47 examples/s]"
          }
        },
        "63844507105248fc9ea144f3b1d57d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3680ec570f444fa4bf54abe70e0155a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5890611817124b1aaa4ed4028ce33457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ffe736baa774469941a80be377ccbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "849c92405fd6482e895d9d72b13eedd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "210f6abacea54e318353039d46eb31f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cd2a04b6a1d4123bf462eb371a822bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3990650d728481e91f1cc89379c4e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba766732890e42e8a6e84f77e8ea4f8a",
              "IPY_MODEL_a040e4796c054310ae515405a6914cba",
              "IPY_MODEL_71dda3747cd74c7c8892f0992d02b199"
            ],
            "layout": "IPY_MODEL_81e32a50b8e6494cb2fc7c25231885f5"
          }
        },
        "ba766732890e42e8a6e84f77e8ea4f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38d304c29f504e6787f890df409d3098",
            "placeholder": "​",
            "style": "IPY_MODEL_aa9eba186636494cad4b3cd1a6758465",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a040e4796c054310ae515405a6914cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5e12a82c57349e1b5b739892bcd69dd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9af42b6d50e548828ab12b8fa3a6d965",
            "value": 3
          }
        },
        "71dda3747cd74c7c8892f0992d02b199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb6ed04808954e0f9ca77dd3a88737be",
            "placeholder": "​",
            "style": "IPY_MODEL_bade62dc64c14dcbb93d276eb07824dd",
            "value": " 3/3 [00:06&lt;00:00,  2.21s/it]"
          }
        },
        "81e32a50b8e6494cb2fc7c25231885f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d304c29f504e6787f890df409d3098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9eba186636494cad4b3cd1a6758465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e12a82c57349e1b5b739892bcd69dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af42b6d50e548828ab12b8fa3a6d965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb6ed04808954e0f9ca77dd3a88737be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bade62dc64c14dcbb93d276eb07824dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Generative Models for Code** -- Midterm Report<br><br>\n",
        "**Maria Gancayco (mig2131@columbia.edu)**<br>\n",
        "**Stephen Wright (svw2112@columbia.edu)**<br>\n",
        "*Due:* Wednesday, 13 Nov 2024 at 11:59pm ET"
      ],
      "metadata": {
        "id": "0YBqgsFTo1AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "###########################################\n",
        "# Setup: Environment and Memory Management\n",
        "###########################################\n",
        "This module initializes the GPU environment and sets up memory management utilities\n",
        "for efficient model handling during the evaluation process.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# Check and display GPU availability for transparency\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
        "\n",
        "# Memory management utilities\n",
        "def clear_memory() -> None:\n",
        "    \"\"\"\n",
        "    Clears GPU memory cache and performs garbage collection.\n",
        "\n",
        "    This function is crucial for maintaining optimal memory usage during model evaluation,\n",
        "    especially when loading and comparing multiple large language models.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # Clear CUDA cache\n",
        "    gc.collect()  # Trigger Python garbage collection\n",
        "\n",
        "def get_memory_status() -> None:\n",
        "    \"\"\"\n",
        "    Displays current GPU memory usage statistics.\n",
        "\n",
        "    Reports both allocated and reserved memory in megabytes (MB).\n",
        "    This helps monitor memory consumption during model operations.\n",
        "\n",
        "    Note:\n",
        "        - Allocated memory: Actually used GPU memory\n",
        "        - Reserved memory: Total memory reserved by PyTorch\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Convert bytes to MB for better readability\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"GPU Memory: Allocated: {allocated:.2f}MB, Reserved: {reserved:.2f}MB\")\n",
        "\n",
        "# Initialize by checking current memory status\n",
        "get_memory_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgvmsDDzsLYC",
        "outputId": "1b400dba-ebe2-401b-b19e-7464d2322f8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device name: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: Allocated: 0.00MB, Reserved: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Configuration and Setup\n",
        "#################################\n",
        "This section defines the experimental configuration parameters and initializes\n",
        "the necessary directory structure for storing results.\n",
        "\"\"\"\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Configuration dataclass containing all hyperparameters and settings for model evaluation.\n",
        "\n",
        "    Attributes:\n",
        "        model_name (str): Name/path of the model to be evaluated\n",
        "        batch_size (int): Number of samples processed in each batch\n",
        "        learning_rate (float): Learning rate for model optimization\n",
        "        num_epochs (int): Number of training epochs\n",
        "        max_seq_length (int): Maximum sequence length for input tokenization\n",
        "        gradient_accumulation_steps (int): Number of steps to accumulate gradients\n",
        "        warmup_steps (Optional[int]): Number of warmup steps for learning rate scheduler\n",
        "        weight_decay (float): L2 regularization factor\n",
        "        eval_steps (int): Frequency of evaluation steps\n",
        "        save_steps (int): Frequency of model checkpoint saves\n",
        "        logging_steps (int): Frequency of logging training metrics\n",
        "    \"\"\"\n",
        "    model_name: str\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    num_epochs: int\n",
        "    max_seq_length: int\n",
        "    gradient_accumulation_steps: int\n",
        "    warmup_steps: Optional[int] = None\n",
        "    weight_decay: float = 0.01\n",
        "    eval_steps: int = 100\n",
        "    save_steps: int = 100\n",
        "    logging_steps: int = 10\n",
        "\n",
        "# Initialize configuration with DeepSeek model parameters\n",
        "config = ExperimentConfig(\n",
        "    model_name=\"deepseek-ai/deepseek-coder-6.7b-instruct\",  # Using DeepSeek's 6.7B instruction-tuned model\n",
        "    batch_size=1,                    # Small batch size due to model size\n",
        "    learning_rate=5e-5,             # Conservative learning rate for fine-tuning\n",
        "    num_epochs=3,                   # Number of training epochs\n",
        "    max_seq_length=512,            # Maximum sequence length for input processing\n",
        "    gradient_accumulation_steps=32, # Accumulate gradients to simulate larger batch size\n",
        "    warmup_steps=100               # Warmup steps for learning rate scheduler\n",
        ")\n",
        "\n",
        "# Set up results directory for storing evaluation outputs\n",
        "results_dir = Path(\"./results\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "print(\"Configuration and directories initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX4vwvGqs3-G",
        "outputId": "07f747a8-152d-474f-d40d-9151d7f9fee9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and directories initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Model Dependencies and Imports\n",
        "#################################\n",
        "This section installs required packages and imports essential libraries\n",
        "for working with transformer-based language models.\n",
        "\"\"\"\n",
        "\n",
        "# Install core dependencies for transformer model handling and evaluation\n",
        "!pip install transformers torch timeout-decorator\n",
        "\n",
        "# Import required libraries\n",
        "import torch  # PyTorch for deep learning operations\n",
        "from transformers import (\n",
        "    AutoTokenizer,         # For tokenization of input text\n",
        "    AutoModelForCausalLM   # For loading pre-trained causal language models\n",
        ")\n",
        "from typing import List, Dict  # Type hints for better code documentation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0TX5UXgwG1q",
        "outputId": "fde04a8f-bb53-4c63-85ae-7b6ebd54d9f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: timeout-decorator in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Model Loading and Code Generation\n",
        "#################################\n",
        "This section implements core functionality for loading language models\n",
        "and generating code from prompts with appropriate error handling and\n",
        "memory optimization.\n",
        "\"\"\"\n",
        "\n",
        "def load_model_and_tokenizer(config: ExperimentConfig) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "    \"\"\"\n",
        "    Loads and initializes the model and tokenizer with memory optimizations.\n",
        "\n",
        "    Args:\n",
        "        config (ExperimentConfig): Configuration object containing model parameters\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, tokenizer) initialized and ready for generation\n",
        "\n",
        "    Raises:\n",
        "        Exception: If model loading fails, with detailed error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clear memory before loading new model to prevent OOM errors\n",
        "        clear_memory()\n",
        "\n",
        "        print(f\"Loading {config.model_name}...\")\n",
        "\n",
        "        # Initialize tokenizer with remote code execution enabled\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=True  # Required for custom tokenizer implementations\n",
        "        )\n",
        "\n",
        "        # Load model with memory-efficient settings\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16,    # Use bfloat16 for memory efficiency\n",
        "            device_map=\"auto\",             # Optimize model placement across available devices\n",
        "            low_cpu_mem_usage=True         # Minimize CPU memory during loading\n",
        "        )\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "            model.gradient_checkpointing_enable()  # Trade compute for memory savings\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        get_memory_status()  # Display current memory usage\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generate_code(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 50\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates code using the loaded model with specified generation parameters.\n",
        "\n",
        "    Args:\n",
        "        model: The loaded language model\n",
        "        tokenizer: The model's tokenizer\n",
        "        prompt: Input prompt for code generation\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "        temperature: Sampling temperature (higher = more creative)\n",
        "        top_p: Nucleus sampling parameter\n",
        "        top_k: Top-k sampling parameter\n",
        "\n",
        "    Returns:\n",
        "        str: Generated code or empty string if generation fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Format prompt as chat message\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "        # Tokenize input with chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Generate code with specified parameters\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_new_tokens,  # Control generation length\n",
        "            do_sample=True,                 # Enable sampling-based generation\n",
        "            temperature=temperature,         # Control randomness\n",
        "            top_p=top_p,                    # Nucleus sampling threshold\n",
        "            top_k=top_k,                    # Top-k sampling parameter\n",
        "            num_return_sequences=1,         # Generate single sequence\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode and return only the generated portion (excluding prompt)\n",
        "        return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in code generation: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "# Initialize model and tokenizer using configuration\n",
        "model, tokenizer = load_model_and_tokenizer(config)\n",
        "\n",
        "# Test the generation pipeline with a simple prompt\n",
        "test_prompt = \"Write a quicksort algorithm in Python.\"\n",
        "generated_code = generate_code(model, tokenizer, test_prompt)\n",
        "print(\"\\nGenerated Code:\\n\", generated_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "ddda6f48631c47c3b8801fbc5806a71b",
            "48da9154f8c2415690fcd0cfce4efea6",
            "13d79001163742b4a7032fb96421d622",
            "6b274135602d4e488f1f5e80747ce0ed",
            "bd28af9b1ee84d6c8d43b1a766bfb7e5",
            "954c3bf683f94ddfa7dc35181c4165bf",
            "948919ac87be4196b958455eb2edb32e",
            "c0817a9a0ba2493da53441c71d2fcf3b",
            "0bd868ad102b4a4f8a36b382b4a74220",
            "a1d2def7e0224cf6990e9979da8b0815",
            "b357f7f8020440a4a228b04711f2b71d"
          ]
        },
        "id": "zNeAngZKtFzY",
        "outputId": "b773398e-5b61-4d21-aa7c-944474cde8a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading deepseek-ai/deepseek-coder-6.7b-instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddda6f48631c47c3b8801fbc5806a71b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "GPU Memory: Allocated: 12856.52MB, Reserved: 12858.00MB\n",
            "\n",
            "Generated Code:\n",
            " Sure, here is a basic implementation of the quicksort algorithm in Python:\n",
            "\n",
            "```python\n",
            "def quicksort(arr):\n",
            "    if len(arr) <= 1:\n",
            "        return arr\n",
            "    pivot = arr[len(arr) // 2]\n",
            "    left = [x for x in arr if x < pivot]\n",
            "    middle = [x for x in arr if x == pivot]\n",
            "    right = [x for x in arr if x > pivot]\n",
            "    return quicksort(left) + middle + quicksort(right)\n",
            "\n",
            "print(quicksort([3,6,8,10,1,2,1]))\n",
            "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
            "```\n",
            "\n",
            "This program works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Code Generation Management System\n",
        "#################################\n",
        "This module implements a robust code generation system with retry mechanisms,\n",
        "logging, and performance tracking capabilities.\n",
        "\"\"\"\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "class CodeGenerator:\n",
        "    \"\"\"\n",
        "    A class to manage code generation with retry logic and generation history tracking.\n",
        "\n",
        "    Attributes:\n",
        "        model: The language model for code generation\n",
        "        tokenizer: The model's tokenizer\n",
        "        generation_history (list): History of all generation attempts\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "        \"\"\"\n",
        "        Initialize the code generator with a model and tokenizer.\n",
        "\n",
        "        Args:\n",
        "            model: The language model to use for generation\n",
        "            tokenizer: The corresponding tokenizer\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.generation_history = []\n",
        "\n",
        "    def generate_with_retry(self, prompt: str, max_attempts: int = 3) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate code with automatic retry mechanism and comprehensive logging.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The input prompt for code generation\n",
        "            max_attempts (int): Maximum number of retry attempts\n",
        "\n",
        "        Returns:\n",
        "            Dict: Generation result containing:\n",
        "                - prompt: Original input prompt\n",
        "                - code: Generated code\n",
        "                - attempt: Attempt number\n",
        "                - generation_time: Time taken\n",
        "                - timestamp: Generation timestamp\n",
        "\n",
        "        Note:\n",
        "            Temperature increases with each retry attempt to encourage diversity\n",
        "        \"\"\"\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                # Track generation time\n",
        "                start_time = datetime.now()\n",
        "\n",
        "                # Generate code with adaptive temperature\n",
        "                generated_code = generate_code(\n",
        "                    self.model,\n",
        "                    self.tokenizer,\n",
        "                    prompt,\n",
        "                    temperature=0.8 if attempt > 0 else 0.6  # Higher temperature for retries\n",
        "                )\n",
        "\n",
        "                # Calculate generation duration\n",
        "                end_time = datetime.now()\n",
        "                generation_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "                # Create comprehensive result log\n",
        "                result = {\n",
        "                    \"prompt\": prompt,\n",
        "                    \"code\": generated_code,\n",
        "                    \"attempt\": attempt + 1,\n",
        "                    \"generation_time\": generation_time,\n",
        "                    \"timestamp\": end_time.isoformat()\n",
        "                }\n",
        "\n",
        "                # Update generation history\n",
        "                self.generation_history.append(result)\n",
        "\n",
        "                # Return successful generation\n",
        "                if generated_code:\n",
        "                    return result\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "\n",
        "        # Return error if all attempts fail\n",
        "        return {\"error\": \"All generation attempts failed\"}\n",
        "\n",
        "    def get_generation_stats(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate and return statistics about code generation performance.\n",
        "\n",
        "        Returns:\n",
        "            Dict containing:\n",
        "                - total_generations: Total number of generation attempts\n",
        "                - average_generation_time: Average time per generation\n",
        "                - successful_generations: Number of successful generations\n",
        "        \"\"\"\n",
        "        if not self.generation_history:\n",
        "            return {}\n",
        "\n",
        "        total_generations = len(self.generation_history)\n",
        "        avg_time = sum(g[\"generation_time\"] for g in self.generation_history) / total_generations\n",
        "\n",
        "        return {\n",
        "            \"total_generations\": total_generations,\n",
        "            \"average_generation_time\": avg_time,\n",
        "            \"successful_generations\": sum(1 for g in self.generation_history if \"code\" in g)\n",
        "        }\n",
        "\n",
        "# Initialize the code generation system\n",
        "code_generator = CodeGenerator(model, tokenizer)\n",
        "\n",
        "# Test the generation system with a sample prompt\n",
        "test_result = code_generator.generate_with_retry(\"Write a binary search function in Python.\")\n",
        "print(\"\\nGeneration Result:\", test_result)\n",
        "print(\"\\nGeneration Stats:\", code_generator.get_generation_stats())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyqA9-elthEA",
        "outputId": "fccaeaa2-3556-43e4-a2f4-c71837c68992"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation Result: {'prompt': 'Write a binary search function in Python.', 'code': 'Sure, here is a simple implementation of a binary search function in Python:\\n\\n```python\\ndef binary_search(arr, low, high, x):\\n \\n    if high >= low:\\n \\n        mid = (high + low) // 2\\n \\n        if arr[mid] == x:\\n            return mid\\n \\n        elif arr[mid] > x:\\n            return binary_search(arr, low, mid - 1, x)\\n \\n        else:\\n            return binary_search(arr, mid + 1, high, x)\\n \\n    else:\\n        return -1\\n \\n# Test array\\narr = [2, 3, 4, 10, 40]\\nx = 10\\n \\n# Function call\\nresult = binary_search(arr, 0, len(arr)-1, x)\\n \\nif result != -1:\\n    print(\"Element is present at index\", str(result))\\nelse:\\n    print(\"Element is not present in array\")\\n```\\n\\nIn this function, `arr` is the list we\\'re searching through, `low` and `high` are the indices of the first and last elements of the list, and `x` is the element we\\'re searching for. The function returns the index of the element if found, or `-1` if not found.\\n', 'attempt': 1, 'generation_time': 11.009614, 'timestamp': '2024-11-14T00:00:17.983244'}\n",
            "\n",
            "Generation Stats: {'total_generations': 1, 'average_generation_time': 11.009614, 'successful_generations': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# SemCoder Model Setup\n",
        "#################################\n",
        "This section handles the installation and setup of the SemCoder model,\n",
        "including Git LFS setup and repository cloning.\n",
        "\"\"\"\n",
        "\n",
        "# Clear GPU memory before new model setup\n",
        "clear_memory()  # Ensure clean memory state for new model\n",
        "\n",
        "# Install Git LFS and clone SemCoder repository\n",
        "print(\"Installing Git LFS and cloning SemCoder...\")\n",
        "!git lfs install  # Initialize Git Large File Storage for model weights\n",
        "\n",
        "# Clone SemCoder from HuggingFace repository\n",
        "# Note: Using /content/SemCoder path for Google Colab compatibility\n",
        "!git clone https://huggingface.co/semcoder/semcoder /content/SemCoder\n",
        "\n",
        "# Verify successful repository cloning\n",
        "import os\n",
        "if os.path.exists('/content/SemCoder'):\n",
        "    print(\"SemCoder repository cloned successfully!\")\n",
        "else:\n",
        "    raise RuntimeError(\"Failed to clone SemCoder repository\")  # Critical error if clone fails"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGcSGuTYu7R8",
        "outputId": "38f5621d-3cb4-4d9e-da9f-9194a3507ca0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Git LFS and cloning SemCoder...\n",
            "Git LFS initialized.\n",
            "fatal: destination path '/content/SemCoder' already exists and is not an empty directory.\n",
            "SemCoder repository cloned successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# SemCoder File Verification\n",
        "#################################\n",
        "This module verifies the integrity of the SemCoder installation by checking\n",
        "for all required model files in the safetensors format.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "def verify_semcoder_files() -> None:\n",
        "    \"\"\"\n",
        "    Verifies the presence of all required SemCoder model files.\n",
        "\n",
        "    Checks for:\n",
        "        - Configuration files (config.json, tokenizer.json)\n",
        "        - Model weight files in safetensors format\n",
        "        - Model index file\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If any required files are missing from the installation\n",
        "    \"\"\"\n",
        "    # Define required files for model functionality\n",
        "    required_files = [\n",
        "        'config.json',           # Model configuration\n",
        "        'tokenizer.json',        # Tokenizer configuration\n",
        "        'model.safetensors.index.json',  # Model weights index\n",
        "        # Sharded model weights in safetensors format\n",
        "        'model-00001-of-00003.safetensors',\n",
        "        'model-00002-of-00003.safetensors',\n",
        "        'model-00003-of-00003.safetensors'\n",
        "    ]\n",
        "    missing_files: List[str] = []\n",
        "\n",
        "    # Display current directory contents for debugging\n",
        "    print(\"SemCoder directory contents:\")\n",
        "    files = os.listdir('/content/SemCoder')\n",
        "    print(\"\\n\".join(files))\n",
        "\n",
        "    # Check for missing files\n",
        "    for file in required_files:\n",
        "        if file not in files:\n",
        "            missing_files.append(file)\n",
        "\n",
        "    # Handle verification results\n",
        "    if missing_files:\n",
        "        raise RuntimeError(f\"Missing required files: {', '.join(missing_files)}\")\n",
        "    else:\n",
        "        print(\"\\nAll required files present!\")\n",
        "        print(\"\\nModel files verification successful!\")\n",
        "\n",
        "# Execute verification\n",
        "verify_semcoder_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA2HhQiEvL4N",
        "outputId": "dd54efbe-89b8-4a58-94d6-3b4912069e5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SemCoder directory contents:\n",
            "trainer_state.json\n",
            "tokenizer.json\n",
            "special_tokens_map.json\n",
            "model.safetensors.index.json\n",
            "model-00002-of-00003.safetensors\n",
            "model-00001-of-00003.safetensors\n",
            "training_args.bin\n",
            "tokenizer_config.json\n",
            "model-00003-of-00003.safetensors\n",
            "README.md\n",
            ".git\n",
            "generation_config.json\n",
            "config.json\n",
            ".gitattributes\n",
            "\n",
            "All required files present!\n",
            "\n",
            "Model files verification successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# SemCoder Model Implementation\n",
        "#################################\n",
        "This module implements the SemCoder model class with memory-efficient loading\n",
        "and code generation capabilities.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import Optional\n",
        "\n",
        "class SemCoderModel:\n",
        "    \"\"\"\n",
        "    A class implementing the SemCoder model with optimized loading and generation.\n",
        "\n",
        "    Attributes:\n",
        "        model_path (str): Path to the local SemCoder model files\n",
        "        model: The loaded language model (initialized in load())\n",
        "        tokenizer: The model's tokenizer (initialized in load())\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"\n",
        "        Initialize SemCoder model instance.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the local model directory\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.model: Optional[AutoModelForCausalLM] = None\n",
        "        self.tokenizer: Optional[AutoTokenizer] = None\n",
        "\n",
        "    def load(self) -> None:\n",
        "        \"\"\"\n",
        "        Load the SemCoder model and tokenizer with memory optimizations.\n",
        "\n",
        "        Implements:\n",
        "            - Memory clearing before load\n",
        "            - bfloat16 precision for efficiency\n",
        "            - Automatic device mapping\n",
        "            - Gradient checkpointing\n",
        "\n",
        "        Raises:\n",
        "            Exception: If model loading fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Ensure clean memory state\n",
        "            clear_memory()\n",
        "\n",
        "            # Load tokenizer first\n",
        "            print(\"Loading SemCoder tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "\n",
        "            # Load model with optimizations\n",
        "            print(\"Loading SemCoder model...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_path,\n",
        "                torch_dtype=torch.bfloat16,    # Use bfloat16 for memory efficiency\n",
        "                device_map=\"auto\",             # Automatic device placement\n",
        "                low_cpu_mem_usage=True         # Minimize CPU memory usage\n",
        "            )\n",
        "\n",
        "            # Enable memory optimization\n",
        "            if hasattr(self.model, \"gradient_checkpointing_enable\"):\n",
        "                self.model.gradient_checkpointing_enable()\n",
        "\n",
        "            print(\"Successfully loaded SemCoder!\")\n",
        "            get_memory_status()  # Display memory usage\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading SemCoder: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_code(self, prompt: str, max_new_tokens: int = 512) -> str:\n",
        "        \"\"\"\n",
        "        Generate code using the loaded SemCoder model.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Input prompt for code generation\n",
        "            max_new_tokens (int): Maximum number of tokens to generate\n",
        "\n",
        "        Returns:\n",
        "            str: Generated code or empty string if generation fails\n",
        "\n",
        "        Note:\n",
        "            Uses sampling-based generation with temperature=0.7 and top_p=0.95\n",
        "            for balanced creativity and coherence\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Tokenize input with proper device placement\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            ).to(self.model.device)\n",
        "\n",
        "            # Generate with specified parameters\n",
        "            outputs = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,         # Enable sampling\n",
        "                temperature=0.7,        # Control randomness\n",
        "                top_p=0.95             # Nucleus sampling threshold\n",
        "            )\n",
        "\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating code: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "# Initialize and load SemCoder model\n",
        "semcoder = SemCoderModel(\"/content/SemCoder\")\n",
        "semcoder.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "0c18ddf333754617a8019b69d70e9692",
            "a49049d68854433d99ed756cbb96ffc1",
            "9b6c8fce53c84d9499660b8d5be6efea",
            "e53f2da5b0b447daaaa463c42ca7db70",
            "998c7ee23ff64b61881306553382e8ff",
            "42ecc832797c4c028102869be152ceeb",
            "4ca29893312e417099586ca3b0e6e7d7",
            "95fdfce28a714c6e82674ceb642f2a7c",
            "7b1c4fcf72f345d392bd498651cf5157",
            "0f794df291254f4c90b8b37e3f78a060",
            "62995ef68fbc4c4c8b6b24859554e3c0"
          ]
        },
        "id": "hQmSCrowCBba",
        "outputId": "5a44064c-2d7e-4912-8b52-735659ecf7ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SemCoder tokenizer...\n",
            "Loading SemCoder model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c18ddf333754617a8019b69d70e9692"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded SemCoder!\n",
            "GPU Memory: Allocated: 25721.17MB, Reserved: 25734.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# SemCoder Generation Testing\n",
        "#################################\n",
        "This module implements a test function to verify SemCoder's code generation\n",
        "capabilities using a Fibonacci sequence implementation as a test case.\n",
        "\"\"\"\n",
        "\n",
        "def test_semcoder_generation() -> None:\n",
        "    \"\"\"\n",
        "    Tests SemCoder's code generation capabilities with a standard programming task.\n",
        "\n",
        "    Test includes:\n",
        "        1. Code generation for Fibonacci sequence\n",
        "        2. Basic validation of generated code structure\n",
        "        3. Memory usage monitoring\n",
        "\n",
        "    The test uses the Fibonacci sequence as it requires:\n",
        "        - Function definition\n",
        "        - Loop or recursion\n",
        "        - Return statement\n",
        "        - Basic algorithm implementation\n",
        "\n",
        "    Prints:\n",
        "        - Input prompt\n",
        "        - Generated code\n",
        "        - Validation results\n",
        "        - Memory status\n",
        "    \"\"\"\n",
        "    # Define test prompt for Fibonacci sequence\n",
        "    prompt = \"Write a Python function to calculate the Fibonacci sequence.\"\n",
        "\n",
        "    print(\"Testing SemCoder with Fibonacci sequence prompt...\")\n",
        "    print(f\"Input prompt: {prompt}\")\n",
        "\n",
        "    try:\n",
        "        # Generate code using SemCoder\n",
        "        generated_code = semcoder.generate_code(prompt)\n",
        "\n",
        "        # Display generation results\n",
        "        print(\"\\nGenerated Code:\")\n",
        "        print(generated_code)\n",
        "\n",
        "        # Perform basic structural validation\n",
        "        validation_checks = {\n",
        "            \"function_definition\": \"def\" in generated_code,\n",
        "            \"return_statement\": \"return\" in generated_code\n",
        "        }\n",
        "\n",
        "        if all(validation_checks.values()):\n",
        "            print(\"\\nCode generation appears successful!\")\n",
        "            print(\"✓ Found function definition\")\n",
        "            print(\"✓ Found return statement\")\n",
        "        else:\n",
        "            print(\"\\nWarning: Generated code might be incomplete!\")\n",
        "            print(\"Missing elements:\")\n",
        "            for check, passed in validation_checks.items():\n",
        "                if not passed:\n",
        "                    print(f\"✗ Missing {check.replace('_', ' ')}\")\n",
        "\n",
        "        # Monitor memory usage after generation\n",
        "        print(\"\\nMemory status after generation:\")\n",
        "        get_memory_status()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in test generation: {str(e)}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "\n",
        "# Execute the test\n",
        "print(\"Initiating SemCoder generation test...\")\n",
        "test_semcoder_generation()"
      ],
      "metadata": {
        "id": "Il5vh8YxCD8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0c5a96-7887-4b8a-9420-46faa37525c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating SemCoder generation test...\n",
            "Testing SemCoder with Fibonacci sequence prompt...\n",
            "Input prompt: Write a Python function to calculate the Fibonacci sequence.\n",
            "\n",
            "Generated Code:\n",
            "Write a Python function to calculate the Fibonacci sequence.\n",
            "\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n <= 0:\n",
            "        return \"Please enter a positive integer.\"\n",
            "    elif n == 1:\n",
            "        return [0]\n",
            "    elif n == 2:\n",
            "        return [0, 1]\n",
            "    else:\n",
            "        fib_sequence = [0, 1]\n",
            "        for i in range(2, n):\n",
            "            fib_sequence.append(fib_sequence[i - 1] + fib_sequence[i - 2])\n",
            "        return fib_sequence\n",
            "\n",
            "# Test the function with n = 10\n",
            "n = 10\n",
            "result = fibonacci(n)\n",
            "print(result)\n",
            "```\n",
            "\n",
            "This solution defines a function `fibonacci(n)` that calculates the Fibonacci sequence up to the nth term. It handles cases where n is less than or equal to 0, n is 1, or n is 2, returning the appropriate sequence. For n greater than 2, it iterates to calculate the Fibonacci sequence up to the nth term. Finally, it tests the function with n = 10 and prints the result.\n",
            "\n",
            "Code generation appears successful!\n",
            "✓ Found function definition\n",
            "✓ Found return statement\n",
            "\n",
            "Memory status after generation:\n",
            "GPU Memory: Allocated: 25721.17MB, Reserved: 25946.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Evaluation Framework Setup\n",
        "#################################\n",
        "This section installs required packages for implementing the model\n",
        "evaluation framework, including dataset handling and progress tracking.\n",
        "\"\"\"\n",
        "\n",
        "# Install essential evaluation packages with version specifications\n",
        "!pip install --upgrade pip  # Ensure pip is up to date\n",
        "!pip install 'datasets>=3.1.0' 'tqdm>=4.66.0' 'fsspec==2024.10.0' --no-deps\n",
        "!pip install 'gcsfs>=2024.10.0'  # Install after fsspec to ensure compatibility\n",
        "\n",
        "\"\"\"\n",
        "Package Details:\n",
        "- datasets: HuggingFace's datasets library for efficient data handling\n",
        "           Used for loading and managing evaluation benchmarks\n",
        "\n",
        "- tqdm: Progress bar library for tracking long-running operations\n",
        "        Used to monitor evaluation progress across multiple samples\n",
        "\n",
        "- fsspec: Filesystem interface library\n",
        "         Required by datasets and gcsfs\n",
        "\n",
        "- gcsfs: Google Cloud Storage interface\n",
        "         Requires specific fsspec version\n",
        "\n",
        "Note:\n",
        "- Version specifications are used to avoid dependency conflicts\n",
        "- The --no-deps flag prevents unwanted dependency downgrades\n",
        "- Packages are installed in order to maintain compatibility\n",
        "\"\"\"\n",
        "\n",
        "# Verify installations\n",
        "import pkg_resources\n",
        "print(\"\\nInstalled versions:\")\n",
        "for package in ['datasets', 'tqdm', 'fsspec', 'gcsfs']:\n",
        "    try:\n",
        "        version = pkg_resources.get_distribution(package).version\n",
        "        print(f\"{package}: {version}\")\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        print(f\"{package}: Not found\")"
      ],
      "metadata": {
        "id": "eQnKrOm5x32X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ec6821-5052-419e-d683-813dc8c0efb9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: datasets>=3.1.0 in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting fsspec==2024.10.0\n",
            "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.9.0\n",
            "    Uninstalling fsspec-2024.9.0:\n",
            "      Successfully uninstalled fsspec-2024.9.0\n",
            "Successfully installed fsspec-2024.10.0\n",
            "Requirement already satisfied: gcsfs>=2024.10.0 in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (3.10.10)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (2024.10.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.10.0) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.10.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.10.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs>=2024.10.0) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.10.0) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.10.0) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.10.0) (2.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs>=2024.10.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs>=2024.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs>=2024.10.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs>=2024.10.0) (2024.8.30)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs>=2024.10.0) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs>=2024.10.0) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs>=2024.10.0) (1.25.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs>=2024.10.0) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (4.12.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs>=2024.10.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2024.10.0) (3.2.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs>=2024.10.0) (0.2.0)\n",
            "\n",
            "Installed versions:\n",
            "datasets: 3.1.0\n",
            "tqdm: 4.66.6\n",
            "fsspec: 2024.10.0\n",
            "gcsfs: 2024.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7b24142994f9>:35: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Test Execution Framework\n",
        "#################################\n",
        "This module implements a robust test execution system for evaluating\n",
        "generated code solutions against predefined test cases.\n",
        "\"\"\"\n",
        "\n",
        "# Import required libraries for code parsing and system operations\n",
        "from typing import List\n",
        "import ast\n",
        "import sys\n",
        "\n",
        "def run_tests(solution_code, test_code, namespace):\n",
        "    \"\"\"\n",
        "    Executes and validates test cases against a generated solution.\n",
        "\n",
        "    Args:\n",
        "        solution_code: The code solution to be tested\n",
        "        test_code: The test cases to run against the solution\n",
        "        namespace: The execution environment for running tests\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all tests pass, False otherwise\n",
        "    \"\"\"\n",
        "    # Clean up input code by removing quotes and whitespace\n",
        "    solution_code = solution_code.strip('\"\\'\\n ')\n",
        "    test_code = test_code.strip('\"\\'\\n ')\n",
        "\n",
        "    # Execute solution code in provided namespace\n",
        "    try:\n",
        "        exec(solution_code, namespace)\n",
        "    except:\n",
        "        print(f\"Error occurred in solution code: {str(e)}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        print(f\"Solution code: {solution_code}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Parse solution code to extract function name\n",
        "        tree = ast.parse(solution_code)\n",
        "        function_name = None\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                function_name = node.name\n",
        "                break\n",
        "\n",
        "        if not function_name:\n",
        "            raise ValueError(\"Could not find function definition in solution code\")\n",
        "\n",
        "        # Modify test code to collect results instead of using assertions\n",
        "        modified_test_code = test_code.replace(\"def check(candidate):\",\n",
        "            f\"def check(candidate):\\n    global test_results\\n    test_results = []\")\n",
        "\n",
        "        # Convert assertion statements to result collection\n",
        "        test_lines = [line for line in test_code.split('\\n') if line.strip().startswith('assert')]\n",
        "        for i, line in enumerate(test_lines):\n",
        "            modified_line = line.replace(\"assert \", \"test_results.append((\")\n",
        "            modified_line = f\"{modified_line}, {repr(line)}))\"\n",
        "            test_lines[i] = modified_line\n",
        "\n",
        "        # Construct complete test execution code\n",
        "        modified_test_code = \"\\n\".join([\n",
        "            \"test_results = []\",          # Initialize results list\n",
        "            modified_test_code,           # Modified test function\n",
        "            \"\\n\".join(test_lines),        # Modified assertions\n",
        "            f\"check({function_name})\"     # Execute tests\n",
        "        ])\n",
        "\n",
        "        # Execute modified test code\n",
        "        exec(modified_test_code, namespace)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred for executing modified test code: {str(e)}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        print(f\"Modified test code: {modified_test_code}\")\n",
        "        return False\n",
        "\n",
        "    # Process and display test results\n",
        "    test_results = namespace.get('test_results', [])\n",
        "    print(f\"\\nExecuting {len(test_results)} tests:\\n\")\n",
        "\n",
        "    # Track test results and display each test outcome\n",
        "    all_passed = True\n",
        "    for i, (result, test_code) in enumerate(test_results, 1):\n",
        "        if result:\n",
        "            print(f\"✓ Test {i} passed: {test_code}\")\n",
        "        else:\n",
        "            print(f\"✗ Test {i} failed: {test_code}\")\n",
        "            all_passed = False\n",
        "\n",
        "    # Display test summary\n",
        "    print(f\"\\nSummary: {sum(r[0] for r in test_results)}/{len(test_results)} tests passed\")\n",
        "    return all_passed\n",
        "\n",
        "# Example usage demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize test environment with required imports\n",
        "    setup_code = \"\"\"from typing import List, Dict, Optional, Any, TypeVar, Tuple\n",
        "import math\n",
        "import string\n",
        "import re\n",
        "\n",
        "M = TypeVar('M')\n",
        "\"\"\"\n",
        "    namespace = {}\n",
        "    exec(setup_code, namespace)\n",
        "\n",
        "    # Example solution implementation\n",
        "    solution_code = \"\"\"def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
        "    numbers.sort()\n",
        "    for i in range(1, len(numbers)):\n",
        "        if numbers[i] - numbers[i - 1] < threshold:\n",
        "            return True\n",
        "    return False\"\"\"\n",
        "\n",
        "    # Example test cases\n",
        "    test_code = '''METADATA = {\n",
        "        'author': 'jt',\n",
        "        'dataset': 'test'\n",
        "}\n",
        "\n",
        "def check(candidate):\n",
        "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
        "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
        "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
        "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
        "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
        "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
        "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False'''\n",
        "\n",
        "    # Execute test suite\n",
        "    run_tests(solution_code, test_code, namespace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqlYfo4Mwb_I",
        "outputId": "bd2f3c91-d65c-4ae5-b76d-cafdd8b29ace"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Executing 7 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
            "✓ Test 2 passed:     assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
            "✓ Test 3 passed:     assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
            "✓ Test 4 passed:     assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
            "✓ Test 5 passed:     assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
            "✓ Test 6 passed:     assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
            "✓ Test 7 passed:     assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
            "\n",
            "Summary: 7/7 tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Model Evaluation Framework\n",
        "#################################\n",
        "This module implements a comprehensive evaluation system for comparing\n",
        "code generation models using the HumanEval dataset.\n",
        "\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "from datasets import load_dataset\n",
        "from typing import Dict, List, Any, TypeVar\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    A class for evaluating code generation models on the HumanEval benchmark.\n",
        "\n",
        "    Attributes:\n",
        "        human_eval: Loaded HumanEval dataset\n",
        "        results: Dictionary storing evaluation results\n",
        "        debug: Boolean controlling debug output level\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize evaluator with HumanEval dataset and empty results\"\"\"\n",
        "        self.human_eval = load_dataset(\"openai_humaneval\")\n",
        "        self.results = {}\n",
        "        self.debug = True  # Control debug output\n",
        "\n",
        "    def format_prompt(self, prompt: str, model_type: str) -> str:\n",
        "        \"\"\"\n",
        "        Format input prompt according to model-specific requirements.\n",
        "\n",
        "        Args:\n",
        "            prompt: Original task prompt\n",
        "            model_type: Type of model (\"deepseek\" or \"semcoder\")\n",
        "\n",
        "        Returns:\n",
        "            Formatted prompt string\n",
        "        \"\"\"\n",
        "        # Format for DeepSeek model\n",
        "        if model_type == \"deepseek\":\n",
        "            return (\n",
        "                \"Write a Python function that solves the following task. \"\n",
        "                \"Provide ONLY the function implementation starting with 'def' and proper indentation. \"\n",
        "                \"The function should be properly indented with 4 spaces. \"\n",
        "                \"Do not include any explanations, comments, docstrings, type hints, or test code. \"\n",
        "                \"Do not include any print statements or assertions. \"\n",
        "                \"Only include the function definition and its implementation.\\n\\n\"\n",
        "                \"Example format:\\n\"\n",
        "                \"def example_function(param1, param2):\\n\"\n",
        "                \"    result = param1 + param2\\n\"\n",
        "                \"    return result\\n\\n\"\n",
        "                \"Your task:\\n\"\n",
        "                f\"{prompt}\"\n",
        "            )\n",
        "        # Format for SemCoder model\n",
        "        elif model_type == \"semcoder\":\n",
        "            return (\n",
        "                \"# Task: Implement the following Python function\\n\"\n",
        "                f\"{prompt}\\n\"\n",
        "                \"# Provide only the function implementation with proper indentation.\\n\"\n",
        "            )\n",
        "        return prompt\n",
        "\n",
        "    def clean_generated_code(self, code: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and normalize generated code.\n",
        "\n",
        "        Args:\n",
        "            code: Raw generated code\n",
        "\n",
        "        Returns:\n",
        "            Cleaned and formatted code string\n",
        "        \"\"\"\n",
        "        # Debug output of original code\n",
        "        if self.debug:\n",
        "            print(\"\\nOriginal generated code:\")\n",
        "            print(code)\n",
        "\n",
        "        # Normalize line endings and split into lines\n",
        "        code = code.replace('\\r\\n', '\\n')\n",
        "        lines = code.splitlines()\n",
        "\n",
        "        cleaned_lines = []\n",
        "        target_function_found = False\n",
        "        indent_level = 0\n",
        "        INDENT = \"    \"\n",
        "        has_seen_def = False\n",
        "\n",
        "        # Process each line\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            if not stripped: continue\n",
        "\n",
        "            function_def_found = stripped.startswith('def ')\n",
        "            if function_def_found:\n",
        "                if has_seen_def:\n",
        "                    cleaned_lines = []\n",
        "                    indent_level = 0\n",
        "                else:\n",
        "                    has_seen_def = True\n",
        "            target_function_found = has_seen_def\n",
        "\n",
        "            if not target_function_found: continue\n",
        "\n",
        "            if function_def_found:\n",
        "                # Clean function definition\n",
        "                function_def = stripped\n",
        "                # Remove return type hints\n",
        "                function_def = re.sub(r'\\s*->\\s*(?:List|Dict|Tuple|Optional|Set|Union|Any|float|int|str|bool)\\[?[^\\]]*\\]?\\s*:', ':', function_def)\n",
        "\n",
        "                # Clean parameter type hints\n",
        "                parts = function_def.split('(', 1)\n",
        "                if len(parts) == 2:\n",
        "                    func_name, params_part = parts\n",
        "                    params_and_rest = params_part.split(')', 1)\n",
        "                    if len(params_and_rest) == 2:\n",
        "                        params, rest = params_and_rest\n",
        "                        param_list = params.split(',')\n",
        "                        cleaned_params = []\n",
        "                        for param in param_list:\n",
        "                            cleaned_param = re.sub(r':\\s*(?:List|Dict|Tuple|Optional|Set|Union|Any|float|int|str|bool)\\[?[^\\]]*\\]?\\s*(?=[,)])?', '', param.strip())\n",
        "                            cleaned_params.append(cleaned_param)\n",
        "                        function_def = f\"{func_name}({', '.join(cleaned_params)}){rest}\"\n",
        "\n",
        "                # Normalize spacing\n",
        "                function_def = re.sub(r'\\s+:', ':', function_def)\n",
        "                function_def = re.sub(r'\\(\\s+', '(', function_def)\n",
        "                function_def = re.sub(r'\\s+\\)', ')', function_def)\n",
        "\n",
        "                cleaned_lines.append(function_def)\n",
        "                indent_level += 1\n",
        "                continue\n",
        "\n",
        "            # Filter out unwanted lines\n",
        "            if any(skip in stripped for skip in ['print(', 'assert', 'if __name__']):\n",
        "                continue\n",
        "\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "        # Join lines with Unix-style newlines\n",
        "        cleaned_code = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        if self.debug:\n",
        "            print(\"\\nCleaned code:\")\n",
        "            print(cleaned_code)\n",
        "            print(\"\\nCleaned code (repr):\")\n",
        "            print(repr(cleaned_code))\n",
        "\n",
        "        return cleaned_code if target_function_found else \"\"\n",
        "\n",
        "    def evaluate_single_solution(self, solution_code, test_cases, entry_point) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a single generated solution against its test cases.\n",
        "\n",
        "        Args:\n",
        "            solution_code: Generated solution to evaluate\n",
        "            test_cases: Test cases to run\n",
        "            entry_point: Name of the function to test\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        print(test_cases)\n",
        "\n",
        "        # Setup environment\n",
        "        setup_code = \"\"\"from typing import List, Dict, Optional, Any, TypeVar, Tuple\n",
        "import math\n",
        "import string\n",
        "import re\n",
        "\n",
        "M = TypeVar('M')\n",
        "\"\"\"\n",
        "        # Validate syntax\n",
        "        try:\n",
        "            compile(solution_code, '<string>', 'exec')\n",
        "        except SyntaxError as e:\n",
        "            if self.debug:\n",
        "                print(f\"Syntax error: {str(e)}\")\n",
        "                print(f\"Generated code:\\n{solution_code}\")\n",
        "            return {\n",
        "                \"pass@1\": 0,\n",
        "                \"pass@10\": 0,\n",
        "                \"pass@100\": 0,\n",
        "                \"syntax_validity\": 0,\n",
        "                \"execution_accuracy\": 0\n",
        "            }\n",
        "\n",
        "        # Execute tests\n",
        "        namespace = {}\n",
        "        try:\n",
        "            exec(setup_code, namespace)\n",
        "        except Exception as e:\n",
        "            if self.debug:\n",
        "                print(f\"Execution error for setup code: {str(e)}\")\n",
        "                print(f\"Setup code:\\n{setup_code}\")\n",
        "            execution_success = False\n",
        "\n",
        "        execution_success = run_tests(solution_code, test_cases, namespace)\n",
        "        return {\n",
        "            \"pass@1\": int(execution_success),\n",
        "            \"pass@10\": int(execution_success),\n",
        "            \"pass@100\": int(execution_success),\n",
        "            \"syntax_validity\": 1,\n",
        "            \"execution_accuracy\": int(execution_success)\n",
        "        }\n",
        "\n",
        "    def evaluate_model(self, model, tokenizer, model_type: str, num_samples: int = None):\n",
        "        \"\"\"\n",
        "        Evaluate model performance on HumanEval dataset.\n",
        "\n",
        "        Args:\n",
        "            model: The model to evaluate\n",
        "            tokenizer: Model's tokenizer\n",
        "            model_type: Type of model (\"deepseek\" or \"semcoder\")\n",
        "            num_samples: Number of samples to evaluate (None for all)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing aggregated evaluation results\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"pass@1\": 0,\n",
        "            \"pass@10\": 0,\n",
        "            \"pass@100\": 0,\n",
        "            \"syntax_validity\": 0,\n",
        "            \"execution_accuracy\": 0\n",
        "        }\n",
        "\n",
        "        total_samples = len(self.human_eval[\"test\"]) if num_samples is None else num_samples\n",
        "\n",
        "        # Process each task\n",
        "        for idx in tqdm(range(total_samples)):\n",
        "            task = self.human_eval[\"test\"][idx]\n",
        "            formatted_prompt = self.format_prompt(task[\"prompt\"], model_type)\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"\\n\\nProcessing task {idx + 1}/{total_samples}\")\n",
        "                print(\"Prompt:\")\n",
        "                print(formatted_prompt)\n",
        "\n",
        "            try:\n",
        "                # Generate code based on model type\n",
        "                if model_type == \"deepseek\":\n",
        "                    messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "                    inputs = tokenizer.apply_chat_template(\n",
        "                        messages,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True\n",
        "                    ).to(model.device)\n",
        "\n",
        "                    attention_mask = torch.ones_like(inputs)\n",
        "\n",
        "                    outputs = model.generate(\n",
        "                        inputs,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_new_tokens=512,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.95,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "                    generated_code = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "                else:  # semcoder\n",
        "                    inputs = tokenizer(\n",
        "                        formatted_prompt,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                        truncation=True,\n",
        "                        max_length=512\n",
        "                    ).to(model.device)\n",
        "\n",
        "                    outputs = model.generate(\n",
        "                        input_ids=inputs[\"input_ids\"],\n",
        "                        attention_mask=inputs[\"attention_mask\"],\n",
        "                        max_new_tokens=512,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.95,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "                    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                # Process and evaluate generated code\n",
        "                cleaned_code = self.clean_generated_code(generated_code)\n",
        "                if cleaned_code:\n",
        "                    evaluation = self.evaluate_single_solution(\n",
        "                        cleaned_code,\n",
        "                        task[\"test\"],\n",
        "                        task[\"entry_point\"]\n",
        "                    )\n",
        "\n",
        "                    if self.debug:\n",
        "                        print(\"\\nEvaluation results:\")\n",
        "                        for metric, value in evaluation.items():\n",
        "                            print(f\"{metric}: {value}\")\n",
        "\n",
        "                    # Update metrics\n",
        "                    for metric in results:\n",
        "                        results[metric] += evaluation[metric]\n",
        "\n",
        "            except Exception as e:\n",
        "                if self.debug:\n",
        "                    print(f\"Error processing sample {idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate final averages\n",
        "        for metric in results:\n",
        "            results[metric] /= total_samples\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize the evaluator\n",
        "evaluator = ModelEvaluator()"
      ],
      "metadata": {
        "id": "5qanOPRM0YGH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# DeepSeek Model Evaluation\n",
        "#################################\n",
        "This section evaluates the DeepSeek base model's performance on the\n",
        "HumanEval benchmark using the previously defined evaluation framework.\n",
        "\"\"\"\n",
        "\n",
        "# Begin DeepSeek model evaluation\n",
        "print(\"Evaluating DeepSeek base model...\")\n",
        "\n",
        "# Run evaluation with limited sample size for initial testing\n",
        "# num_samples=10 provides a quick assessment of model performance\n",
        "deepseek_results = evaluator.evaluate_model(\n",
        "    model=model,              # Previously loaded DeepSeek model\n",
        "    tokenizer=tokenizer,      # DeepSeek tokenizer\n",
        "    model_type=\"deepseek\",    # Specify model type for proper prompt formatting\n",
        "    num_samples=10           # Number of test cases to evaluate\n",
        ")\n",
        "\n",
        "# Display evaluation results\n",
        "print(\"\\nDeepSeek Base Results:\")\n",
        "print(json.dumps(deepseek_results, indent=2))  # Pretty print results in JSON format\n",
        "\n",
        "\"\"\"\n",
        "Results include:\n",
        "- pass@1: Single-attempt success rate\n",
        "- pass@10: Success rate within 10 attempts\n",
        "- pass@100: Success rate within 100 attempts\n",
        "- syntax_validity: Proportion of syntactically valid generations\n",
        "- execution_accuracy: Proportion of functionally correct solutions\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VNAxWjqO1g3H",
        "outputId": "6f24e0d7-89be-48de-a31f-0579f359ae82"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating DeepSeek base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Processing task 1/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:01<00:17,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    numbers.sort()\n",
            "    for i in range(1, len(numbers)):\n",
            "        if numbers[i] - numbers[i - 1] < threshold:\n",
            "            return True\n",
            "    return False\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 2/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:20<01:34, 11.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "\n",
            "    # Write your code here\n",
            "    # IMPORTANT: Don't include the following lines in your code\n",
            "    # assert False, \"Not implemented\"\n",
            "    # return None\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 3/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:33<01:25, 12.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    integer_part = int(number)\n",
            "    decimal_part = number - integer_part\n",
            "\n",
            "    return decimal_part\n",
            "\n",
            "\n",
            "def round_number(number: float, ndigits: int) -> float:\n",
            "    \"\"\" Given a positive floating point number and a number of decimal places,\n",
            "    it can be decomposed into the integer part and decimals.\n",
            "\n",
            "    The decimals are multiplied by 10 to the power of the number of decimal places,\n",
            "    then the rounded result is divided by 10 to the power of the number of decimal places.\n",
            "\n",
            "    Return the rounded number.\n",
            "    >>> round_number(3.5714, 2)\n",
            "    3.57\n",
            "    \"\"\"\n",
            "\n",
            "    factor = 10 ** ndigits\n",
            "    rounded_number = round(number * factor) / factor\n",
            "\n",
            "    return rounded_number\n",
            "\n",
            "\n",
            "def truncate_and_round(number: float, ndigits: int) -> float:\n",
            "    \"\"\" Given a positive floating point number and a number of decimal places,\n",
            "    it first trims off the integer part of the number and obtains the decimal part,\n",
            "    then it rounds the decimal part to the specified number of decimal places.\n",
            "\n",
            "    Return the rounded decimal part of the number.\n",
            "    >>> truncate_and_round(3.5714, 2)\n",
            "    0.57\n",
            "    \"\"\"\n",
            "\n",
            "    decimal_part = truncate_number(number)\n",
            "    rounded_decimal_part = round_number(decimal_part, ndigits)\n",
            "\n",
            "    return rounded_decimal_part\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "def truncate_and_round(number, ndigits):\n",
            "    \"\"\" Given a positive floating point number and a number of decimal places,\n",
            "    it first trims off the integer part of the number and obtains the decimal part,\n",
            "    then it rounds the decimal part to the specified number of decimal places.\n",
            "    Return the rounded decimal part of the number.\n",
            "    >>> truncate_and_round(3.5714, 2)\n",
            "    0.57\n",
            "    \"\"\"\n",
            "    decimal_part = truncate_number(number)\n",
            "    rounded_decimal_part = round_number(decimal_part, ndigits)\n",
            "    return rounded_decimal_part\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def truncate_and_round(number, ndigits):\\n    \"\"\" Given a positive floating point number and a number of decimal places,\\n    it first trims off the integer part of the number and obtains the decimal part,\\n    then it rounds the decimal part to the specified number of decimal places.\\n    Return the rounded decimal part of the number.\\n    >>> truncate_and_round(3.5714, 2)\\n    0.57\\n    \"\"\"\\n    decimal_part = truncate_number(number)\\n    rounded_decimal_part = round_number(decimal_part, ndigits)\\n    return rounded_decimal_part'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate(3.5) == 0.5\n",
            "    assert abs(candidate(1.33) - 0.33) < 1e-6\n",
            "    assert abs(candidate(123.456) - 0.456) < 1e-6\n",
            "\n",
            "Error occurred for executing modified test code: truncate_and_round() missing 1 required positional argument: 'ndigits'\n",
            "Error type: TypeError\n",
            "Modified test code: test_results = []\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    global test_results\n",
            "    test_results = []\n",
            "    assert candidate(3.5) == 0.5\n",
            "    assert abs(candidate(1.33) - 0.33) < 1e-6\n",
            "    assert abs(candidate(123.456) - 0.456) < 1e-6\n",
            "    test_results.append((candidate(3.5) == 0.5, '    assert candidate(3.5) == 0.5'))\n",
            "    test_results.append((abs(candidate(1.33) - 0.33) < 1e-6, '    assert abs(candidate(1.33) - 0.33) < 1e-6'))\n",
            "    test_results.append((abs(candidate(123.456) - 0.456) < 1e-6, '    assert abs(candidate(123.456) - 0.456) < 1e-6'))\n",
            "check(truncate_and_round)\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 0\n",
            "pass@10: 0\n",
            "pass@100: 0\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 0\n",
            "\n",
            "\n",
            "Processing task 4/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:34<00:48,  8.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    # Your code goes here\n",
            "    balance = 0\n",
            "    for operation in operations:\n",
            "        balance += operation\n",
            "        if balance < 0:\n",
            "            return True\n",
            "    return False\n",
            "\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 5/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:37<00:31,  6.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    # Calculate the mean\n",
            "    mean = sum(numbers) / len(numbers)\n",
            "\n",
            "    # Calculate the absolute differences from the mean\n",
            "    differences = [abs(num - mean) for num in numbers]\n",
            "\n",
            "    # Calculate the average of these absolute differences\n",
            "    mad = sum(differences) / len(differences)\n",
            "\n",
            "    return mad\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 6/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def intersperse(numbers: List[int], delimeter: int) -> List[int]:\n",
            "    \"\"\" Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n",
            "    >>> intersperse([], 4)\n",
            "    []\n",
            "    >>> intersperse([1, 2, 3], 4)\n",
            "    [1, 4, 2, 4, 3]\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:40<00:19,  4.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    result = []\n",
            "    for i in range(len(numbers)):\n",
            "        result.append(numbers[i])\n",
            "        if i < len(numbers) - 1:\n",
            "            result.append(delimeter)\n",
            "    return result\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 7/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def parse_nested_parens(paren_string: str) -> List[int]:\n",
            "    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
            "    For each of the group, output the deepest level of nesting of parentheses.\n",
            "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
            "\n",
            "    >>> parse_nested_parens('(()()) ((())) () ((())()())')\n",
            "    [2, 3, 1, 3]\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:43<00:13,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    result = []\n",
            "    for group in paren_string.split(' '):\n",
            "        count = 0\n",
            "        max_count = 0\n",
            "        for char in group:\n",
            "            if char == '(':\n",
            "                count += 1\n",
            "                if count > max_count:\n",
            "                    max_count = count\n",
            "            elif char == ')':\n",
            "                count -= 1\n",
            "        result.append(max_count)\n",
            "    return result\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 8/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
            "    >>> filter_by_substring([], 'a')\n",
            "    []\n",
            "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
            "    ['abc', 'bacd', 'array']\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:01<00:17,  8.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "\n",
            "def filter_by_length(strings: List[str], length: int) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that are of given length\n",
            "    >>> filter_by_length([], 5)\n",
            "    []\n",
            "    >>> filter_by_length(['abc', 'bacd', 'cde', 'array'], 4)\n",
            "    ['abcd', 'cde']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "def filter_by_vowels(strings: List[str], vowels: int) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given number of vowels\n",
            "    >>> filter_by_vowels([], 2)\n",
            "    []\n",
            "    >>> filter_by_vowels(['abc', 'bacd', 'cde', 'array'], 2)\n",
            "    ['abc', 'bacd', 'array']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "def filter_by_consonants(strings: List[str], consonants: int) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given number of consonants\n",
            "    >>> filter_by_consonants([], 3)\n",
            "    []\n",
            "    >>> filter_by_consonants(['abc', 'bacd', 'cde', 'array'], 3)\n",
            "    ['bacd', 'cde', 'array']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "def filter_by_special_chars(strings: List[str], special_chars: int) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given number of special characters\n",
            "    >>> filter_by_special_chars([], 3)\n",
            "    []\n",
            "    >>> filter_by_special_chars(['abc', 'bacd', 'cde#', 'array'], 2)\n",
            "    ['cde#', 'array']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "def filter_by_multiple_criteria(strings: List[str], substring: str = None, length: int = None,\n",
            "                                vowels: int = None, consonants: int = None, special_chars: int = None) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings by multiple criteria at once\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "def filter_by_multiple_criteria(strings: List[str], substring: str = None, length: int = None,\n",
            "                                vowels: int = None, consonants: int = None, special_chars: int = None) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings by multiple criteria at once\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def filter_by_multiple_criteria(strings: List[str], substring: str = None, length: int = None,\\n                                vowels: int = None, consonants: int = None, special_chars: int = None) -> List[str]:\\n    \"\"\" Filter an input list of strings by multiple criteria at once'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([], 'john') == []\n",
            "    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
            "    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\n",
            "    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
            "\n",
            "Syntax error: unterminated triple-quoted string literal (detected at line 3) (<string>, line 3)\n",
            "Generated code:\n",
            "def filter_by_multiple_criteria(strings: List[str], substring: str = None, length: int = None,\n",
            "                                vowels: int = None, consonants: int = None, special_chars: int = None) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings by multiple criteria at once\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 0\n",
            "pass@10: 0\n",
            "pass@100: 0\n",
            "syntax_validity: 0\n",
            "execution_accuracy: 0\n",
            "\n",
            "\n",
            "Processing task 9/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def sum_product(numbers: List[int]) -> Tuple[int, int]:\n",
            "    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\n",
            "    Empty sum should be equal to 0 and empty product should be equal to 1.\n",
            "    >>> sum_product([])\n",
            "    (0, 1)\n",
            "    >>> sum_product([1, 2, 3, 4])\n",
            "    (10, 24)\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:03<00:06,  6.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    # Your code here\n",
            "    sum = 0\n",
            "    product = 1\n",
            "    for num in numbers:\n",
            "        sum += num\n",
            "        product *= num\n",
            "    return (sum, product)\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "\n",
            "\n",
            "Cleaned code (repr):\n",
            "''\n",
            "\n",
            "\n",
            "Processing task 10/10\n",
            "Prompt:\n",
            "Write a Python function that solves the following task. Provide ONLY the function implementation starting with 'def' and proper indentation. The function should be properly indented with 4 spaces. Do not include any explanations, comments, docstrings, type hints, or test code. Do not include any print statements or assertions. Only include the function definition and its implementation.\n",
            "\n",
            "Example format:\n",
            "def example_function(param1, param2):\n",
            "    result = param1 + param2\n",
            "    return result\n",
            "\n",
            "Your task:\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def rolling_max(numbers: List[int]) -> List[int]:\n",
            "    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\n",
            "    in the sequence.\n",
            "    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 2, 3, 3, 3, 4, 4]\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/doctest.py\", line 1501, in run\n",
            "    sys.settrace(save_trace)\n",
            "\n",
            "100%|██████████| 10/10 [01:19<00:00,  7.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "    # Your code here\n",
            "    pass\n",
            "\n",
            "\n",
            "def rolling_min(numbers: List[int]) -> List[int]:\n",
            "    \"\"\" From a given list of integers, generate a list of rolling minimum element found until given moment\n",
            "    in the sequence.\n",
            "    >>> rolling_min([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 1, 1, 1, 1, 1, 1]\n",
            "    \"\"\"\n",
            "\n",
            "    # Your code here\n",
            "    pass\n",
            "\n",
            "\n",
            "def min_max_difference(numbers: List[int]) -> List[int]:\n",
            "    \"\"\" From a given list of integers, generate a list of differences between rolling minimum and rolling maximum \n",
            "    found until given moment in the sequence.\n",
            "    >>> min_max_difference([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 1, 2, 2, 3, 4, 4]\n",
            "    \"\"\"\n",
            "\n",
            "    # Your code here\n",
            "    pass\n",
            "\n",
            "\n",
            "def mean_and_difference(numbers: List[int]) -> Tuple[List[float], List[int]]:\n",
            "    \"\"\" From a given list of integers, generate a list of means and differences between rolling mean and rolling \n",
            "    maximum/minimum found until given moment in the sequence.\n",
            "    >>> mean_and_difference([1, 2, 3, 2, 3, 4, 2])\n",
            "    ([1.0, 1.5, 2.0, 2.0, 2.4, 2.6, 2.4], [0, 1, 1, 0, 1, 1, 0])\n",
            "    \"\"\"\n",
            "\n",
            "    # Your code here\n",
            "    pass\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "def mean_and_difference(numbers) -> Tuple[List[float], List[int]]:\n",
            "    \"\"\" From a given list of integers, generate a list of means and differences between rolling mean and rolling \n",
            "    maximum/minimum found until given moment in the sequence.\n",
            "    >>> mean_and_difference([1, 2, 3, 2, 3, 4, 2])\n",
            "    ([1.0, 1.5, 2.0, 2.0, 2.4, 2.6, 2.4], [0, 1, 1, 0, 1, 1, 0])\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    pass\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def mean_and_difference(numbers) -> Tuple[List[float], List[int]]:\\n    \"\"\" From a given list of integers, generate a list of means and differences between rolling mean and rolling \\n    maximum/minimum found until given moment in the sequence.\\n    >>> mean_and_difference([1, 2, 3, 2, 3, 4, 2])\\n    ([1.0, 1.5, 2.0, 2.0, 2.4, 2.6, 2.4], [0, 1, 1, 0, 1, 1, 0])\\n    \"\"\"\\n    # Your code here\\n    pass\\n    import doctest\\n    doctest.testmod()'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([]) == []\n",
            "    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\n",
            "    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\n",
            "    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "\n",
            "Error occurred for executing modified test code: \n",
            "Error type: AssertionError\n",
            "Modified test code: test_results = []\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    global test_results\n",
            "    test_results = []\n",
            "    assert candidate([]) == []\n",
            "    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\n",
            "    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\n",
            "    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "    test_results.append((candidate([]) == [], '    assert candidate([]) == []'))\n",
            "    test_results.append((candidate([1, 2, 3, 4]) == [1, 2, 3, 4], '    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]'))\n",
            "    test_results.append((candidate([4, 3, 2, 1]) == [4, 4, 4, 4], '    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]'))\n",
            "    test_results.append((candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100], '    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]'))\n",
            "check(mean_and_difference)\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 0\n",
            "pass@10: 0\n",
            "pass@100: 0\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 0\n",
            "\n",
            "DeepSeek Base Results:\n",
            "{\n",
            "  \"pass@1\": 0.0,\n",
            "  \"pass@10\": 0.0,\n",
            "  \"pass@100\": 0.0,\n",
            "  \"syntax_validity\": 0.2,\n",
            "  \"execution_accuracy\": 0.0\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nResults include:\\n- pass@1: Single-attempt success rate\\n- pass@10: Success rate within 10 attempts\\n- pass@100: Success rate within 100 attempts\\n- syntax_validity: Proportion of syntactically valid generations\\n- execution_accuracy: Proportion of functionally correct solutions\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# SemCoder Model Evaluation\n",
        "#################################\n",
        "This section evaluates the SemCoder model's performance on the same\n",
        "HumanEval benchmark for direct comparison with DeepSeek results.\n",
        "\"\"\"\n",
        "\n",
        "# Begin SemCoder evaluation\n",
        "print(\"Evaluating SemCoder...\")\n",
        "\n",
        "# Run evaluation using identical parameters as DeepSeek for fair comparison\n",
        "semcoder_results = evaluator.evaluate_model(\n",
        "    model=semcoder.model,        # Previously loaded SemCoder model\n",
        "    tokenizer=semcoder.tokenizer, # SemCoder tokenizer\n",
        "    model_type=\"semcoder\",       # Specify model type for appropriate prompt formatting\n",
        "    num_samples=10              # Match DeepSeek sample size for direct comparison\n",
        ")\n",
        "\n",
        "# Display evaluation results\n",
        "print(\"\\nSemCoder Results:\")\n",
        "print(json.dumps(semcoder_results, indent=2))  # Pretty print results in JSON format\n",
        "\n",
        "\"\"\"\n",
        "Note: Results use same metrics as DeepSeek evaluation:\n",
        "- pass@1: Single-attempt success rate\n",
        "- pass@10: Success rate within 10 attempts\n",
        "- pass@100: Success rate within 100 attempts\n",
        "- syntax_validity: Proportion of syntactically valid generations\n",
        "- execution_accuracy: Proportion of functionally correct solutions\n",
        "\n",
        "These results can be directly compared with DeepSeek results\n",
        "to assess relative model performance.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9jaOf2L11XBA",
        "outputId": "e05c8f9d-437b-415d-f555-661cddaaedb3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating SemCoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Processing task 1/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:03<00:33,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we can iterate through the list of numbers and check if the absolute difference between any two numbers is less than the given threshold.\n",
            "\n",
            "    for i in range(len(numbers)):\n",
            "        for j in range(i + 1, len(numbers)):\n",
            "            if abs(numbers[i] - numbers[j]) < threshold:\n",
            "                return True\n",
            "    return False\n",
            "\n",
            "# Test cases are provided in the docstring of the function.\n",
            "\n",
            "Cleaned code:\n",
            "def has_close_elements(numbers, threshold):\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we can iterate through the list of numbers and check if the absolute difference between any two numbers is less than the given threshold.\n",
            "    for i in range(len(numbers)):\n",
            "        for j in range(i + 1, len(numbers)):\n",
            "            if abs(numbers[i] - numbers[j]) < threshold:\n",
            "                return True\n",
            "    return False\n",
            "# Test cases are provided in the docstring of the function.\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def has_close_elements(numbers, threshold):\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n# To solve this problem, we can iterate through the list of numbers and check if the absolute difference between any two numbers is less than the given threshold.\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False\\n# Test cases are provided in the docstring of the function.'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
            "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
            "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
            "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
            "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
            "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
            "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
            "\n",
            "\n",
            "\n",
            "Executing 7 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
            "✓ Test 2 passed:     assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
            "✓ Test 3 passed:     assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
            "✓ Test 4 passed:     assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
            "✓ Test 5 passed:     assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
            "✓ Test 6 passed:     assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
            "✓ Test 7 passed:     assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
            "\n",
            "Summary: 7/7 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "\n",
            "Processing task 2/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:10<00:44,  5.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    result = []\n",
            "    current_group = ''\n",
            "    stack = []\n",
            "\n",
            "    for char in paren_string:\n",
            "        if char == '(':\n",
            "            if current_group:\n",
            "                result.append(current_group)\n",
            "                current_group = ''\n",
            "            stack.append('(')\n",
            "        elif char == ')':\n",
            "            if stack and stack[-1] == '(':\n",
            "                stack.pop()\n",
            "                if not stack:\n",
            "                    current_group += ')'\n",
            "            else:\n",
            "                current_group += ')'\n",
            "        elif current_group or stack:\n",
            "            current_group += char\n",
            "\n",
            "    if current_group:\n",
            "        result.append(current_group)\n",
            "\n",
            "    return result\n",
            "\n",
            "# Test the function with the provided example\n",
            "print(separate_paren_groups('( ) (( )) (( )( ))'))\n",
            "\n",
            "Cleaned code:\n",
            "def separate_paren_groups(paren_string):\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    result = []\n",
            "    current_group = ''\n",
            "    stack = []\n",
            "    for char in paren_string:\n",
            "        if char == '(':\n",
            "            if current_group:\n",
            "                result.append(current_group)\n",
            "                current_group = ''\n",
            "            stack.append('(')\n",
            "        elif char == ')':\n",
            "            if stack and stack[-1] == '(':\n",
            "                stack.pop()\n",
            "                if not stack:\n",
            "                    current_group += ')'\n",
            "            else:\n",
            "                current_group += ')'\n",
            "        elif current_group or stack:\n",
            "            current_group += char\n",
            "    if current_group:\n",
            "        result.append(current_group)\n",
            "    return result\n",
            "# Test the function with the provided example\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def separate_paren_groups(paren_string):\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n    result = []\\n    current_group = \\'\\'\\n    stack = []\\n    for char in paren_string:\\n        if char == \\'(\\':\\n            if current_group:\\n                result.append(current_group)\\n                current_group = \\'\\'\\n            stack.append(\\'(\\')\\n        elif char == \\')\\':\\n            if stack and stack[-1] == \\'(\\':\\n                stack.pop()\\n                if not stack:\\n                    current_group += \\')\\'\\n            else:\\n                current_group += \\')\\'\\n        elif current_group or stack:\\n            current_group += char\\n    if current_group:\\n        result.append(current_group)\\n    return result\\n# Test the function with the provided example'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate('(()()) ((())) () ((())()())') == [\n",
            "        '(()())', '((()))', '()', '((())()())'\n",
            "    ]\n",
            "    assert candidate('() (()) ((())) (((())))') == [\n",
            "        '()', '(())', '((()))', '(((())))'\n",
            "    ]\n",
            "    assert candidate('(()(())((())))') == [\n",
            "        '(()(())((())))'\n",
            "    ]\n",
            "    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
            "\n",
            "Error occurred for executing modified test code: closing parenthesis ')' does not match opening parenthesis '[' (<string>, line 21)\n",
            "Error type: SyntaxError\n",
            "Modified test code: test_results = []\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    global test_results\n",
            "    test_results = []\n",
            "    assert candidate('(()()) ((())) () ((())()())') == [\n",
            "        '(()())', '((()))', '()', '((())()())'\n",
            "    ]\n",
            "    assert candidate('() (()) ((())) (((())))') == [\n",
            "        '()', '(())', '((()))', '(((())))'\n",
            "    ]\n",
            "    assert candidate('(()(())((())))') == [\n",
            "        '(()(())((())))'\n",
            "    ]\n",
            "    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
            "    test_results.append((candidate('(()()) ((())) () ((())()())') == [, \"    assert candidate('(()()) ((())) () ((())()())') == [\"))\n",
            "    test_results.append((candidate('() (()) ((())) (((())))') == [, \"    assert candidate('() (()) ((())) (((())))') == [\"))\n",
            "    test_results.append((candidate('(()(())((())))') == [, \"    assert candidate('(()(())((())))') == [\"))\n",
            "    test_results.append((candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())'], \"    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\"))\n",
            "check(separate_paren_groups)\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 0\n",
            "pass@10: 0\n",
            "pass@100: 0\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 0\n",
            "\n",
            "\n",
            "Processing task 3/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:16<00:38,  5.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# The provided solution is incorrect as it does not handle the case where the number is negative.\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    if number < 0:\n",
            "        return 0  # Return 0 if the number is negative\n",
            "    return number % 1  # Calculate the decimal part of the number\n",
            "\n",
            "# Test the function with the given examples\n",
            "print(truncate_number(3.5))  # Output: 0.5\n",
            "print(truncate_number(-2.7))  # Output: 0\n",
            "print(truncate_number(10.999))  # Output: 0.999\n",
            "\n",
            "\n",
            "Cleaned code:\n",
            "def truncate_number(number):\n",
            "    if number < 0:\n",
            "        return 0  # Return 0 if the number is negative\n",
            "    return number % 1  # Calculate the decimal part of the number\n",
            "# Test the function with the given examples\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def truncate_number(number):\\n    if number < 0:\\n        return 0  # Return 0 if the number is negative\\n    return number % 1  # Calculate the decimal part of the number\\n# Test the function with the given examples'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate(3.5) == 0.5\n",
            "    assert abs(candidate(1.33) - 0.33) < 1e-6\n",
            "    assert abs(candidate(123.456) - 0.456) < 1e-6\n",
            "\n",
            "\n",
            "Executing 3 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate(3.5) == 0.5\n",
            "✓ Test 2 passed:     assert abs(candidate(1.33) - 0.33) < 1e-6\n",
            "✓ Test 3 passed:     assert abs(candidate(123.456) - 0.456) < 1e-6\n",
            "\n",
            "Summary: 3/3 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "\n",
            "Processing task 4/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:18<00:26,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    balance = 0\n",
            "    for operation in operations:\n",
            "        balance += operation\n",
            "        if balance < 0:\n",
            "            return True\n",
            "    return False\n",
            "\n",
            "# Run the doctests to validate the implementation.\n",
            "if __name__ == \"__main__\":\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "Cleaned code:\n",
            "def below_zero(operations):\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    balance = 0\n",
            "    for operation in operations:\n",
            "        balance += operation\n",
            "        if balance < 0:\n",
            "            return True\n",
            "    return False\n",
            "# Run the doctests to validate the implementation.\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def below_zero(operations):\\n    \"\"\" You\\'re given a list of deposit and withdrawal operations on a bank account that starts with\\n    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\\n    at that point function should return True. Otherwise it should return False.\\n    >>> below_zero([1, 2, 3])\\n    False\\n    >>> below_zero([1, 2, -4, 5])\\n    True\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n    balance = 0\\n    for operation in operations:\\n        balance += operation\\n        if balance < 0:\\n            return True\\n    return False\\n# Run the doctests to validate the implementation.\\n    import doctest\\n    doctest.testmod()'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([]) == False\n",
            "    assert candidate([1, 2, -3, 1, 2, -3]) == False\n",
            "    assert candidate([1, 2, -4, 5, 6]) == True\n",
            "    assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\n",
            "    assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\n",
            "    assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\n",
            "\n",
            "\n",
            "Executing 6 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate([]) == False\n",
            "✓ Test 2 passed:     assert candidate([1, 2, -3, 1, 2, -3]) == False\n",
            "✓ Test 3 passed:     assert candidate([1, 2, -4, 5, 6]) == True\n",
            "✓ Test 4 passed:     assert candidate([1, -1, 2, -2, 5, -5, 4, -4]) == False\n",
            "✓ Test 5 passed:     assert candidate([1, -1, 2, -2, 5, -5, 4, -5]) == True\n",
            "✓ Test 6 passed:     assert candidate([1, -2, 2, -2, 5, -5, 4, -4]) == True\n",
            "\n",
            "Summary: 6/6 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "\n",
            "Processing task 5/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:26<00:28,  5.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# The provided solution has the function definition and docstring, but not the implementation.\n",
            "# To solve this problem, we need to calculate the mean of the input list and then compute the Mean Absolute Deviation.\n",
            "\n",
            "    if not numbers:\n",
            "        return 0.0\n",
            "\n",
            "    mean_val = sum(numbers) / len(numbers)\n",
            "    mad_val = sum(abs(num - mean_val) for num in numbers) / len(numbers)\n",
            "\n",
            "    return mad_val\n",
            "\n",
            "# Test the function with the example input [1.0, 2.0, 3.0, 4.0]\n",
            "print(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]))\n",
            "# Output: 1.0\n",
            "# The Mean Absolute Deviation for the input list [1.0, 2.0, 3.0, 4.0] is 1.0\n",
            "\n",
            "Cleaned code:\n",
            "def mean_absolute_deviation(numbers):\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# The provided solution has the function definition and docstring, but not the implementation.\n",
            "# To solve this problem, we need to calculate the mean of the input list and then compute the Mean Absolute Deviation.\n",
            "    if not numbers:\n",
            "        return 0.0\n",
            "    mean_val = sum(numbers) / len(numbers)\n",
            "    mad_val = sum(abs(num - mean_val) for num in numbers) / len(numbers)\n",
            "    return mad_val\n",
            "# Test the function with the example input [1.0, 2.0, 3.0, 4.0]\n",
            "# Output: 1.0\n",
            "# The Mean Absolute Deviation for the input list [1.0, 2.0, 3.0, 4.0] is 1.0\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def mean_absolute_deviation(numbers):\\n    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\\n    around the mean of this dataset.\\n    Mean Absolute Deviation is the average absolute difference between each\\n    element and a centerpoint (mean in this case):\\n    MAD = average | x - x_mean |\\n    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\\n    1.0\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n# The provided solution has the function definition and docstring, but not the implementation.\\n# To solve this problem, we need to calculate the mean of the input list and then compute the Mean Absolute Deviation.\\n    if not numbers:\\n        return 0.0\\n    mean_val = sum(numbers) / len(numbers)\\n    mad_val = sum(abs(num - mean_val) for num in numbers) / len(numbers)\\n    return mad_val\\n# Test the function with the example input [1.0, 2.0, 3.0, 4.0]\\n# Output: 1.0\\n# The Mean Absolute Deviation for the input list [1.0, 2.0, 3.0, 4.0] is 1.0'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n",
            "    assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\n",
            "    assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n",
            "\n",
            "\n",
            "\n",
            "Executing 3 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert abs(candidate([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n",
            "✓ Test 2 passed:     assert abs(candidate([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\n",
            "✓ Test 3 passed:     assert abs(candidate([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n",
            "\n",
            "Summary: 3/3 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "\n",
            "Processing task 6/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def intersperse(numbers: List[int], delimeter: int) -> List[int]:\n",
            "    \"\"\" Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n",
            "    >>> intersperse([], 4)\n",
            "    []\n",
            "    >>> intersperse([1, 2, 3], 4)\n",
            "    [1, 4, 2, 4, 3]\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:30<00:20,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def intersperse(numbers: List[int], delimeter: int) -> List[int]:\n",
            "    \"\"\" Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n",
            "    >>> intersperse([], 4)\n",
            "    []\n",
            "    >>> intersperse([1, 2, 3], 4)\n",
            "    [1, 4, 2, 4, 3]\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    result = []\n",
            "    for i in range(len(numbers) - 1):\n",
            "        result.extend([numbers[i], delimeter])\n",
            "    result.append(numbers[-1])\n",
            "    return result\n",
            "\n",
            "\n",
            "# Test cases\n",
            "import doctest\n",
            "doctest.testmod()\n",
            "\n",
            "# The `intersperse` function takes a list of integers `numbers` and an integer `delimeter` as input. It inserts the `delimeter` between every two consecutive elements of the input list and returns the modified list.\n",
            "\n",
            "Cleaned code:\n",
            "def intersperse(numbers, delimeter):\n",
            "    \"\"\" Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n",
            "    >>> intersperse([], 4)\n",
            "    []\n",
            "    >>> intersperse([1, 2, 3], 4)\n",
            "    [1, 4, 2, 4, 3]\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    result = []\n",
            "    for i in range(len(numbers) - 1):\n",
            "        result.extend([numbers[i], delimeter])\n",
            "    result.append(numbers[-1])\n",
            "    return result\n",
            "# Test cases\n",
            "import doctest\n",
            "doctest.testmod()\n",
            "# The `intersperse` function takes a list of integers `numbers` and an integer `delimeter` as input. It inserts the `delimeter` between every two consecutive elements of the input list and returns the modified list.\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def intersperse(numbers, delimeter):\\n    \"\"\" Insert a number \\'delimeter\\' between every two consecutive elements of input list `numbers\\'\\n    >>> intersperse([], 4)\\n    []\\n    >>> intersperse([1, 2, 3], 4)\\n    [1, 4, 2, 4, 3]\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n    result = []\\n    for i in range(len(numbers) - 1):\\n        result.extend([numbers[i], delimeter])\\n    result.append(numbers[-1])\\n    return result\\n# Test cases\\nimport doctest\\ndoctest.testmod()\\n# The `intersperse` function takes a list of integers `numbers` and an integer `delimeter` as input. It inserts the `delimeter` between every two consecutive elements of the input list and returns the modified list.'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([], 7) == []\n",
            "    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\n",
            "    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n",
            "\n",
            "Error occurred for executing modified test code: list index out of range\n",
            "Error type: IndexError\n",
            "Modified test code: test_results = []\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    global test_results\n",
            "    test_results = []\n",
            "    assert candidate([], 7) == []\n",
            "    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\n",
            "    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n",
            "    test_results.append((candidate([], 7) == [], '    assert candidate([], 7) == []'))\n",
            "    test_results.append((candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2], '    assert candidate([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]'))\n",
            "    test_results.append((candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2], '    assert candidate([2, 2, 2], 2) == [2, 2, 2, 2, 2]'))\n",
            "check(intersperse)\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 0\n",
            "pass@10: 0\n",
            "pass@100: 0\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 0\n",
            "\n",
            "\n",
            "Processing task 7/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def parse_nested_parens(paren_string: str) -> List[int]:\n",
            "    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
            "    For each of the group, output the deepest level of nesting of parentheses.\n",
            "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
            "\n",
            "    >>> parse_nested_parens('(()()) ((())) () ((())()())')\n",
            "    [2, 3, 1, 3]\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:37<00:16,  5.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def parse_nested_parens(paren_string: str) -> List[int]:\n",
            "    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
            "    For each of the group, output the deepest level of nesting of parentheses.\n",
            "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
            "\n",
            "    >>> parse_nested_parens('(()()) ((())) () ((())()())')\n",
            "    [2, 3, 1, 3]\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    def max_nesting_level(paren_group: str) -> int:\n",
            "        max_level = 0\n",
            "        current_level = 0\n",
            "\n",
            "        for char in paren_group:\n",
            "            if char == '(':\n",
            "                current_level += 1\n",
            "                max_level = max(max_level, current_level)\n",
            "            elif char == ')':\n",
            "                current_level -= 1\n",
            "\n",
            "        return max_level\n",
            "\n",
            "    groups = paren_string.split()\n",
            "    return [max_nesting_level(group) for group in groups]\n",
            "\n",
            "# Run the function with the provided test case\n",
            "if __name__ == \"__main__\":\n",
            "    import doctest\n",
            "\n",
            "    doctest.testmod()\n",
            "\n",
            "Cleaned code:\n",
            "def max_nesting_level(paren_group):\n",
            "        max_level = 0\n",
            "        current_level = 0\n",
            "        for char in paren_group:\n",
            "            if char == '(':\n",
            "                current_level += 1\n",
            "                max_level = max(max_level, current_level)\n",
            "            elif char == ')':\n",
            "                current_level -= 1\n",
            "        return max_level\n",
            "    groups = paren_string.split()\n",
            "    return [max_nesting_level(group) for group in groups]\n",
            "# Run the function with the provided test case\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "Cleaned code (repr):\n",
            "\"def max_nesting_level(paren_group):\\n        max_level = 0\\n        current_level = 0\\n        for char in paren_group:\\n            if char == '(':\\n                current_level += 1\\n                max_level = max(max_level, current_level)\\n            elif char == ')':\\n                current_level -= 1\\n        return max_level\\n    groups = paren_string.split()\\n    return [max_nesting_level(group) for group in groups]\\n# Run the function with the provided test case\\n    import doctest\\n    doctest.testmod()\"\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n",
            "    assert candidate('() (()) ((())) (((())))') == [1, 2, 3, 4]\n",
            "    assert candidate('(()(())((())))') == [4]\n",
            "\n",
            "Syntax error: unindent does not match any outer indentation level (<string>, line 11)\n",
            "Generated code:\n",
            "def max_nesting_level(paren_group):\n",
            "        max_level = 0\n",
            "        current_level = 0\n",
            "        for char in paren_group:\n",
            "            if char == '(':\n",
            "                current_level += 1\n",
            "                max_level = max(max_level, current_level)\n",
            "            elif char == ')':\n",
            "                current_level -= 1\n",
            "        return max_level\n",
            "    groups = paren_string.split()\n",
            "    return [max_nesting_level(group) for group in groups]\n",
            "# Run the function with the provided test case\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 0\n",
            "pass@10: 0\n",
            "pass@100: 0\n",
            "syntax_validity: 0\n",
            "execution_accuracy: 0\n",
            "\n",
            "\n",
            "Processing task 8/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
            "    >>> filter_by_substring([], 'a')\n",
            "    []\n",
            "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
            "    ['abc', 'bacd', 'array']\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:39<00:08,  4.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
            "    >>> filter_by_substring([], 'a')\n",
            "    []\n",
            "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
            "    ['abc', 'bacd', 'array']\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    return [s for s in strings if substring in s]\n",
            "\n",
            "# Run the provided test cases to verify the correctness of the function.\n",
            "# The function filters the input list of strings based on whether the given substring is present in each string.\n",
            "\n",
            "Cleaned code:\n",
            "def filter_by_substring(strings, substring):\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
            "    >>> filter_by_substring([], 'a')\n",
            "    []\n",
            "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
            "    ['abc', 'bacd', 'array']\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    return [s for s in strings if substring in s]\n",
            "# Run the provided test cases to verify the correctness of the function.\n",
            "# The function filters the input list of strings based on whether the given substring is present in each string.\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def filter_by_substring(strings, substring):\\n    \"\"\" Filter an input list of strings only for ones that contain given substring\\n    >>> filter_by_substring([], \\'a\\')\\n    []\\n    >>> filter_by_substring([\\'abc\\', \\'bacd\\', \\'cde\\', \\'array\\'], \\'a\\')\\n    [\\'abc\\', \\'bacd\\', \\'array\\']\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n    return [s for s in strings if substring in s]\\n# Run the provided test cases to verify the correctness of the function.\\n# The function filters the input list of strings based on whether the given substring is present in each string.'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([], 'john') == []\n",
            "    assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
            "    assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\n",
            "    assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
            "\n",
            "\n",
            "Executing 4 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate([], 'john') == []\n",
            "✓ Test 2 passed:     assert candidate(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
            "✓ Test 3 passed:     assert candidate(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\n",
            "✓ Test 4 passed:     assert candidate(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
            "\n",
            "Summary: 4/4 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "\n",
            "Processing task 9/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def sum_product(numbers: List[int]) -> Tuple[int, int]:\n",
            "    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\n",
            "    Empty sum should be equal to 0 and empty product should be equal to 1.\n",
            "    >>> sum_product([])\n",
            "    (0, 1)\n",
            "    >>> sum_product([1, 2, 3, 4])\n",
            "    (10, 24)\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [00:43<00:04,  4.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def sum_product(numbers: List[int]) -> Tuple[int, int]:\n",
            "    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\n",
            "    Empty sum should be equal to 0 and empty product should be equal to 1.\n",
            "    >>> sum_product([])\n",
            "    (0, 1)\n",
            "    >>> sum_product([1, 2, 3, 4])\n",
            "    (10, 24)\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we will iterate through the list of numbers and calculate the sum and product of all the integers.\n",
            "\n",
            "    sum_result = 0\n",
            "    product_result = 1\n",
            "\n",
            "    for num in numbers:\n",
            "        sum_result += num\n",
            "        product_result *= num\n",
            "\n",
            "    return sum_result, product_result\n",
            "\n",
            "# Test the function with the provided test cases\n",
            "print(sum_product([]))  # Output: (0, 1)\n",
            "print(sum_product([1, 2, 3, 4]))  # Output: (10, 24)\n",
            "\n",
            "Cleaned code:\n",
            "def sum_product(numbers):\n",
            "    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\n",
            "    Empty sum should be equal to 0 and empty product should be equal to 1.\n",
            "    >>> sum_product([])\n",
            "    (0, 1)\n",
            "    >>> sum_product([1, 2, 3, 4])\n",
            "    (10, 24)\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we will iterate through the list of numbers and calculate the sum and product of all the integers.\n",
            "    sum_result = 0\n",
            "    product_result = 1\n",
            "    for num in numbers:\n",
            "        sum_result += num\n",
            "        product_result *= num\n",
            "    return sum_result, product_result\n",
            "# Test the function with the provided test cases\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def sum_product(numbers):\\n    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\\n    Empty sum should be equal to 0 and empty product should be equal to 1.\\n    >>> sum_product([])\\n    (0, 1)\\n    >>> sum_product([1, 2, 3, 4])\\n    (10, 24)\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n# To solve this problem, we will iterate through the list of numbers and calculate the sum and product of all the integers.\\n    sum_result = 0\\n    product_result = 1\\n    for num in numbers:\\n        sum_result += num\\n        product_result *= num\\n    return sum_result, product_result\\n# Test the function with the provided test cases'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([]) == (0, 1)\n",
            "    assert candidate([1, 1, 1]) == (3, 1)\n",
            "    assert candidate([100, 0]) == (100, 0)\n",
            "    assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\n",
            "    assert candidate([10]) == (10, 10)\n",
            "\n",
            "\n",
            "Executing 5 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate([]) == (0, 1)\n",
            "✓ Test 2 passed:     assert candidate([1, 1, 1]) == (3, 1)\n",
            "✓ Test 3 passed:     assert candidate([100, 0]) == (100, 0)\n",
            "✓ Test 4 passed:     assert candidate([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\n",
            "✓ Test 5 passed:     assert candidate([10]) == (10, 10)\n",
            "\n",
            "Summary: 5/5 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "\n",
            "Processing task 10/10\n",
            "Prompt:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def rolling_max(numbers: List[int]) -> List[int]:\n",
            "    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\n",
            "    in the sequence.\n",
            "    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 2, 3, 3, 3, 4, 4]\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:51<00:00,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original generated code:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def rolling_max(numbers: List[int]) -> List[int]:\n",
            "    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\n",
            "    in the sequence.\n",
            "    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 2, 3, 3, 3, 4, 4]\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we will iterate through the input list of numbers and keep track of the maximum element found so far.\n",
            "# At each step, we will compare the current number with the maximum element found so far and update the maximum accordingly.\n",
            "# We will store these maximum elements in a new list and return it as the final result.\n",
            "\n",
            "    rolling_max_list = []\n",
            "    max_element = float('-inf')\n",
            "\n",
            "    for num in numbers:\n",
            "        max_element = max(max_element, num)\n",
            "        rolling_max_list.append(max_element)\n",
            "\n",
            "    return rolling_max_list\n",
            "\n",
            "# Test the function with the provided example\n",
            "print(rolling_max([1, 2, 3, 2, 3, 4, 2]))  # Output: [1, 2, 3, 3, 3, 4, 4]\n",
            "\n",
            "# This solution efficiently generates the list of rolling maximum elements as required.\n",
            "\n",
            "Cleaned code:\n",
            "def rolling_max(numbers):\n",
            "    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\n",
            "    in the sequence.\n",
            "    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 2, 3, 3, 3, 4, 4]\n",
            "    \"\"\"\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we will iterate through the input list of numbers and keep track of the maximum element found so far.\n",
            "# At each step, we will compare the current number with the maximum element found so far and update the maximum accordingly.\n",
            "# We will store these maximum elements in a new list and return it as the final result.\n",
            "    rolling_max_list = []\n",
            "    max_element = float('-inf')\n",
            "    for num in numbers:\n",
            "        max_element = max(max_element, num)\n",
            "        rolling_max_list.append(max_element)\n",
            "    return rolling_max_list\n",
            "# Test the function with the provided example\n",
            "# This solution efficiently generates the list of rolling maximum elements as required.\n",
            "\n",
            "Cleaned code (repr):\n",
            "'def rolling_max(numbers):\\n    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\\n    in the sequence.\\n    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\\n    [1, 2, 3, 3, 3, 4, 4]\\n    \"\"\"\\n# Provide only the function implementation with proper indentation.\\n# To solve this problem, we will iterate through the input list of numbers and keep track of the maximum element found so far.\\n# At each step, we will compare the current number with the maximum element found so far and update the maximum accordingly.\\n# We will store these maximum elements in a new list and return it as the final result.\\n    rolling_max_list = []\\n    max_element = float(\\'-inf\\')\\n    for num in numbers:\\n        max_element = max(max_element, num)\\n        rolling_max_list.append(max_element)\\n    return rolling_max_list\\n# Test the function with the provided example\\n# This solution efficiently generates the list of rolling maximum elements as required.'\n",
            "\n",
            "\n",
            "METADATA = {\n",
            "    'author': 'jt',\n",
            "    'dataset': 'test'\n",
            "}\n",
            "\n",
            "\n",
            "def check(candidate):\n",
            "    assert candidate([]) == []\n",
            "    assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\n",
            "    assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\n",
            "    assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "\n",
            "\n",
            "Executing 4 tests:\n",
            "\n",
            "✓ Test 1 passed:     assert candidate([]) == []\n",
            "✓ Test 2 passed:     assert candidate([1, 2, 3, 4]) == [1, 2, 3, 4]\n",
            "✓ Test 3 passed:     assert candidate([4, 3, 2, 1]) == [4, 4, 4, 4]\n",
            "✓ Test 4 passed:     assert candidate([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "\n",
            "Summary: 4/4 tests passed\n",
            "\n",
            "Evaluation results:\n",
            "pass@1: 1\n",
            "pass@10: 1\n",
            "pass@100: 1\n",
            "syntax_validity: 1\n",
            "execution_accuracy: 1\n",
            "\n",
            "SemCoder Results:\n",
            "{\n",
            "  \"pass@1\": 0.7,\n",
            "  \"pass@10\": 0.7,\n",
            "  \"pass@100\": 0.7,\n",
            "  \"syntax_validity\": 0.9,\n",
            "  \"execution_accuracy\": 0.7\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNote: Results use same metrics as DeepSeek evaluation:\\n- pass@1: Single-attempt success rate\\n- pass@10: Success rate within 10 attempts\\n- pass@100: Success rate within 100 attempts\\n- syntax_validity: Proportion of syntactically valid generations\\n- execution_accuracy: Proportion of functionally correct solutions\\n\\nThese results can be directly compared with DeepSeek results\\nto assess relative model performance.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Extended Evaluation Setup\n",
        "#################################\n",
        "This section prepares for comprehensive model evaluation using both\n",
        "HumanEval and HumanEval+ benchmarks.\n",
        "\"\"\"\n",
        "\n",
        "# Install HuggingFace datasets library for benchmark access\n",
        "!pip install datasets  # Required for loading HumanEval and HumanEval+ datasets\n",
        "\n",
        "\"\"\"\n",
        "Package Details:\n",
        "- datasets: HuggingFace's datasets library\n",
        "           Provides access to:\n",
        "           - Original HumanEval benchmark (164 programming tasks)\n",
        "           - HumanEval+ extended benchmark\n",
        "\n",
        "Note: HumanEval+ extends the original benchmark with:\n",
        "      - Additional test cases\n",
        "      - Edge cases\n",
        "      - Complex inputs\n",
        "      - Performance considerations\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-gv6fe7bCFpB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "751b17ec-c33a-43fd-d08d-87e029309b08",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2024.9.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPackage Details:\\n- datasets: HuggingFace's datasets library\\n           Provides access to:\\n           - Original HumanEval benchmark (164 programming tasks)\\n           - HumanEval+ extended benchmark\\n           \\nNote: HumanEval+ extends the original benchmark with:\\n      - Additional test cases\\n      - Edge cases\\n      - Complex inputs\\n      - Performance considerations\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Benchmark Dataset Loading and Testing\n",
        "#################################\n",
        "This section loads the HumanEval dataset and performs an initial\n",
        "test generation using the first benchmark problem.\n",
        "\"\"\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "def generate_code_with_semcoder(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate code using the SemCoder model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The programming task prompt\n",
        "\n",
        "    Returns:\n",
        "        str: Generated code solution\n",
        "    \"\"\"\n",
        "    # Format prompt for SemCoder\n",
        "    formatted_prompt = (\n",
        "        \"# Task: Implement the following Python function\\n\"\n",
        "        f\"{prompt}\\n\"\n",
        "        \"# Provide only the function implementation with proper indentation.\\n\"\n",
        "    )\n",
        "\n",
        "    # Generate code using previously loaded SemCoder model\n",
        "    return semcoder.generate_code(formatted_prompt)\n",
        "\n",
        "# Load the complete HumanEval benchmark\n",
        "human_eval = load_dataset(\"openai_humaneval\")  # Contains 164 Python programming tasks\n",
        "\n",
        "# Extract first task for initial testing\n",
        "task = human_eval[\"test\"][0]  # Index 0 contains first benchmark problem\n",
        "prompt = task[\"prompt\"]       # Extract problem description\n",
        "\n",
        "# Display task details for verification\n",
        "print(\"HumanEval Prompt:\\n\", prompt)  # Show problem description\n",
        "print(\"Expected Solution:\\n\", task[\"canonical_solution\"])  # Show reference solution\n",
        "\n",
        "# Test code generation with SemCoder\n",
        "generated_code = generate_code_with_semcoder(prompt)  # Generate solution using SemCoder\n",
        "print(\"Generated Code:\\n\", generated_code)  # Display generated solution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "nSqmkMvmI_Kh",
        "outputId": "cf5e54c8-b885-4e56-a735-308abf0454f2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanEval Prompt:\n",
            " from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "Expected Solution:\n",
            "     for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = abs(elem - elem2)\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "Generated Code:\n",
            " # Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# To solve this problem, we can iterate through the list of numbers and compare the absolute difference between each pair of numbers. If any pair has a difference less than the threshold, we return True; otherwise, we return False.\n",
            "\n",
            "    for i in range(len(numbers)):\n",
            "        for j in range(i + 1, len(numbers)):\n",
            "            if abs(numbers[i] - numbers[j]) < threshold:\n",
            "                return True\n",
            "    return False\n",
            "\n",
            "# This solution efficiently checks all pairs of numbers in the list and returns True if any pair is closer than the threshold, and False otherwise.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNote: This initial test:\\n1. Verifies dataset loading\\n2. Confirms prompt extraction\\n3. Tests code generation pipeline\\n4. Allows comparison between:\\n   - Problem description\\n   - Expected solution\\n   - Generated solution\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Solution Comparison\n",
        "#################################\n",
        "This section performs a basic comparison between the generated\n",
        "solution and the canonical solution from the benchmark.\n",
        "\"\"\"\n",
        "\n",
        "# Compare generated code with canonical solution\n",
        "# Note: Strips whitespace for more accurate comparison\n",
        "if generated_code.strip() == task[\"canonical_solution\"].strip():\n",
        "    print(\"The generated code matches the expected solution!\")\n",
        "else:\n",
        "    print(\"The generated code does not match the expected solution.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "CZGFBENkJFH3",
        "outputId": "9f6a425b-5ec9-4896-a722-ad68aa8ca4a4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated code does not match the expected solution.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNote: This is a strict comparison that:\\n1. Only checks for exact matches\\n2. Ignores whitespace differences\\n3. Does not account for:\\n   - Functionally equivalent but differently written solutions\\n   - Alternative algorithmic approaches\\n   - Different variable names\\n   - Different formatting styles\\n   \\nFor more comprehensive evaluation, functional testing\\n(running test cases) provides better insight into solution\\ncorrectness.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Doctest Validation\n",
        "#################################\n",
        "This section executes the generated code and runs its doctests\n",
        "to verify functionality against the provided examples.\n",
        "\"\"\"\n",
        "\n",
        "# First attempt to execute the generated code\n",
        "try:\n",
        "    exec(generated_code)  # Load the generated function into namespace\n",
        "except Exception as e:\n",
        "    print(f\"Error in executing generated code: {e}\")\n",
        "    print(\"Generated code that failed:\")\n",
        "    print(generated_code)\n",
        "\n",
        "# If code execution succeeded, run doctests\n",
        "try:\n",
        "    import doctest\n",
        "    doctest.testmod()  # Run all doctests in the current namespace\n",
        "except Exception as e:\n",
        "    print(f\"Error running doctests: {e}\")\n",
        "    print(\"Doctest execution failed. This might indicate:\")\n",
        "    print(\"- Syntax errors in the docstring examples\")\n",
        "    print(\"- Mismatched output formatting\")\n",
        "    print(\"- Function behavior different from examples\")"
      ],
      "metadata": {
        "id": "eGMGYKRSKMTL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Custom Test Suite Execution\n",
        "#################################\n",
        "This section implements and runs a custom test suite to validate\n",
        "the generated function against specific test cases.\n",
        "\"\"\"\n",
        "\n",
        "# Define comprehensive test cases\n",
        "test_cases = [\n",
        "    ([1.0, 2.0, 3.0], 0.5, False),          # Basic case with no close elements\n",
        "    ([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3, True)  # Case with close elements\n",
        "]\n",
        "\n",
        "def run_tests(func):\n",
        "    \"\"\"\n",
        "    Execute test cases against the generated function.\n",
        "\n",
        "    Args:\n",
        "        func: The generated function to test\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any test case fails\n",
        "\n",
        "    Note: Each test case contains:\n",
        "        - Input list of numbers\n",
        "        - Threshold value\n",
        "        - Expected boolean result\n",
        "    \"\"\"\n",
        "    for numbers, threshold, expected in test_cases:\n",
        "        result = func(numbers, threshold)\n",
        "        assert result == expected, f\"Test failed: {numbers}, {threshold} -> {result}\"\n",
        "\n",
        "# Execute tests on generated function\n",
        "try:\n",
        "    # Load the generated function into current namespace\n",
        "    exec(generated_code)\n",
        "\n",
        "    # Run test suite against the loaded function\n",
        "    run_tests(has_close_elements)\n",
        "    print(\"All tests passed successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test failed: {e}\")\n",
        "    print(\"\\nDetails:\")\n",
        "    print(f\"- Error type: {type(e).__name__}\")\n",
        "    print(f\"- Generated code being tested:\")\n",
        "    print(generated_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXMbtVbuKO1s",
        "outputId": "129c859b-b450-4e87-fa13-8d708155466b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Multi-Task Evaluation Loop\n",
        "#################################\n",
        "This section implements a loop to test SemCoder's performance\n",
        "across multiple HumanEval tasks, providing a broader assessment\n",
        "of model capabilities.\n",
        "\"\"\"\n",
        "\n",
        "# Evaluate first 5 tasks from HumanEval\n",
        "for i in range(5):  # Limited sample for initial testing\n",
        "    # Extract task details\n",
        "    task = human_eval[\"test\"][i]\n",
        "    prompt = task[\"prompt\"]\n",
        "\n",
        "    # Display task information\n",
        "    print(f\"\\nTask {i + 1} Prompt:\\n{prompt}\")\n",
        "\n",
        "    # Generate solution using SemCoder\n",
        "    generated_code = generate_code_with_semcoder(prompt)\n",
        "    print(\"Generated Code:\\n\", generated_code)\n",
        "\n",
        "    # Test generated solution\n",
        "    try:\n",
        "        # Load generated function into namespace\n",
        "        exec(generated_code)\n",
        "\n",
        "        # Execute test cases\n",
        "        # Note: Currently configured for has_close_elements\n",
        "        # TODO: Modify test runner for each specific function\n",
        "        run_tests(has_close_elements)\n",
        "\n",
        "        print(f\"Task {i + 1}: All tests passed successfully!\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Task {i + 1}: Test failed - {e}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        print(\"Generated code that failed:\")\n",
        "        print(generated_code)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WshSVIlDKZ_-",
        "outputId": "e0b6fbc8-4da3-45b6-97a3-1881749cda5d",
        "collapsed": true
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 1 Prompt:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            " # Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    sorted_numbers = sorted(numbers)\n",
            "    for i in range(len(sorted_numbers) - 1):\n",
            "        if sorted_numbers[i + 1] - sorted_numbers[i] < threshold:\n",
            "            return True\n",
            "    return False\n",
            "\n",
            "# Run doctest to validate the function\n",
            "if __name__ == \"__main__\":\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "\n",
            "# Task: Ensure that the function works correctly with the provided test cases.\n",
            "Task 1: All tests passed successfully!\n",
            "\n",
            "\n",
            "Task 2 Prompt:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            " # Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    groups = []\n",
            "    start_index = None\n",
            "    open_count = 0\n",
            "\n",
            "    for i, char in enumerate(paren_string):\n",
            "        if char == '(':\n",
            "            if open_count == 0:\n",
            "                start_index = i\n",
            "            open_count += 1\n",
            "        elif char == ')':\n",
            "            open_count -= 1\n",
            "            if open_count == 0:\n",
            "                groups.append(paren_string[start_index:i + 1])\n",
            "\n",
            "    return groups\n",
            "\n",
            "# Run the provided test cases to verify the correctness of the function\n",
            "import doctest\n",
            "doctest.testmod()\n",
            "\n",
            "**********************************************************************\n",
            "File \"__main__\", line 10, in __main__.separate_paren_groups\n",
            "Failed example:\n",
            "    separate_paren_groups('( ) (( )) (( )( ))')\n",
            "Expected:\n",
            "    ['()', '(())', '(()())']\n",
            "Got:\n",
            "    ['( )', '(( ))', '(( )( ))']\n",
            "**********************************************************************\n",
            "1 items had failures:\n",
            "   1 of   1 in __main__.separate_paren_groups\n",
            "***Test Failed*** 1 failures.\n",
            "Task 2: All tests passed successfully!\n",
            "\n",
            "\n",
            "Task 3 Prompt:\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            " # Task: Implement the following Python function\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# The provided solution is incorrect as it does not handle the case when the number is negative.\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "\n",
            "    if number < 0:\n",
            "        raise ValueError(\"Input number must be positive\")\n",
            "\n",
            "    return number % 1\n",
            "Task 3: All tests passed successfully!\n",
            "\n",
            "\n",
            "Task 4 Prompt:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            " # Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "    balance = 0\n",
            "    for operation in operations:\n",
            "        balance += operation\n",
            "        if balance < 0:\n",
            "            return True\n",
            "    return False\n",
            "\n",
            "# Run the doctest to check the function\n",
            "if __name__ == \"__main__\":\n",
            "    import doctest\n",
            "    doctest.testmod()\n",
            "**********************************************************************\n",
            "File \"__main__\", line 10, in __main__.separate_paren_groups\n",
            "Failed example:\n",
            "    separate_paren_groups('( ) (( )) (( )( ))')\n",
            "Expected:\n",
            "    ['()', '(())', '(()())']\n",
            "Got:\n",
            "    ['( )', '(( ))', '(( )( ))']\n",
            "**********************************************************************\n",
            "1 items had failures:\n",
            "   1 of   1 in __main__.separate_paren_groups\n",
            "***Test Failed*** 1 failures.\n",
            "Task 4: All tests passed successfully!\n",
            "\n",
            "\n",
            "Task 5 Prompt:\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "Generated Code:\n",
            " # Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# The test cases are provided within the docstring.\n",
            "\n",
            "```python\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "    if not numbers:\n",
            "        return 0.0\n",
            "\n",
            "    mean_val = sum(numbers) / len(numbers)\n",
            "    abs_deviations = [abs(num - mean_val) for num in numbers]\n",
            "    mad_val = sum(abs_deviations) / len(numbers)\n",
            "\n",
            "    return mad_val\n",
            "\n",
            "# Test the function with the provided test case\n",
            "print(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]))  # Output: 1.0\n",
            "```\n",
            "Task 5: Test failed - invalid syntax (<string>, line 18)\n",
            "Error type: SyntaxError\n",
            "Generated code that failed:\n",
            "# Task: Implement the following Python function\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "# Provide only the function implementation with proper indentation.\n",
            "# The test cases are provided within the docstring.\n",
            "\n",
            "```python\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "\n",
            "    if not numbers:\n",
            "        return 0.0\n",
            "\n",
            "    mean_val = sum(numbers) / len(numbers)\n",
            "    abs_deviations = [abs(num - mean_val) for num in numbers]\n",
            "    mad_val = sum(abs_deviations) / len(numbers)\n",
            "\n",
            "    return mad_val\n",
            "\n",
            "# Test the function with the provided test case\n",
            "print(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]))  # Output: 1.0\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Advanced Code Evaluation System\n",
        "#################################\n",
        "This module implements a comprehensive evaluation system for code\n",
        "generation models, including syntax validation, test execution,\n",
        "and performance metrics calculation.\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import timeout_decorator\n",
        "\n",
        "class CodeEvaluator:\n",
        "    \"\"\"\n",
        "    A class for evaluating code generation model performance.\n",
        "\n",
        "    Attributes:\n",
        "        dataset: HumanEval or similar benchmark dataset\n",
        "        metrics: Dictionary tracking various performance metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset=\"openai_humaneval\"):\n",
        "        \"\"\"\n",
        "        Initialize evaluator with specified dataset and metrics.\n",
        "\n",
        "        Args:\n",
        "            dataset: Name of the evaluation dataset\n",
        "        \"\"\"\n",
        "        self.dataset = load_dataset(dataset)\n",
        "        self.metrics = {\n",
        "            \"pass@1\": 0.0,      # Single-attempt success rate\n",
        "            \"pass@10\": 0.0,     # Success within 10 attempts\n",
        "            \"pass@100\": 0.0,    # Success within 100 attempts\n",
        "            \"syntax_validity\": 0.0,  # Syntactic correctness\n",
        "            \"execution_accuracy\": 0.0  # Functional correctness\n",
        "        }\n",
        "\n",
        "    @timeout_decorator.timeout(5)  # Prevent infinite loops/hanging\n",
        "    def execute_test_case(self, code: str, test_case: str) -> bool:\n",
        "        \"\"\"\n",
        "        Execute a single test case with timeout protection.\n",
        "\n",
        "        Args:\n",
        "            code: Generated code to test\n",
        "            test_case: Test case to execute\n",
        "\n",
        "        Returns:\n",
        "            bool: True if test passes, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            namespace = {}\n",
        "            exec(code, namespace)\n",
        "            exec(test_case, namespace)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            return False\n",
        "\n",
        "    def check_syntax(self, code: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verify syntactic correctness of generated code.\n",
        "\n",
        "        Args:\n",
        "            code: Code to check\n",
        "\n",
        "        Returns:\n",
        "            bool: True if syntax is valid\n",
        "        \"\"\"\n",
        "        try:\n",
        "            compile(code, '<string>', 'exec')\n",
        "            return True\n",
        "        except SyntaxError:\n",
        "            return False\n",
        "\n",
        "    def evaluate_single_solution(self,\n",
        "                               task_id: int,\n",
        "                               generated_code: str,\n",
        "                               num_samples: int = 1) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate a single generated solution comprehensively.\n",
        "\n",
        "        Args:\n",
        "            task_id: Index of the task\n",
        "            generated_code: Generated solution to evaluate\n",
        "            num_samples: Number of evaluation samples\n",
        "\n",
        "        Returns:\n",
        "            Dict containing evaluation results\n",
        "        \"\"\"\n",
        "        task = self.dataset[\"test\"][task_id]\n",
        "\n",
        "        # Verify syntax first\n",
        "        syntax_valid = self.check_syntax(generated_code)\n",
        "\n",
        "        # Execute test cases if syntax is valid\n",
        "        if syntax_valid:\n",
        "            test_cases = task[\"test_cases\"]\n",
        "            # Use thread pool for parallel test execution\n",
        "            with ThreadPoolExecutor() as executor:\n",
        "                results = list(executor.map(\n",
        "                    lambda tc: self.execute_test_case(generated_code, tc),\n",
        "                    test_cases\n",
        "                ))\n",
        "                print(\"Results\")\n",
        "                print(results)\n",
        "            execution_success = all(results)\n",
        "        else:\n",
        "            execution_success = False\n",
        "\n",
        "        return {\n",
        "            \"syntax_valid\": syntax_valid,\n",
        "            \"execution_success\": execution_success\n",
        "        }\n",
        "\n",
        "    def evaluate_model(self, model, tokenizer, n_tasks: int = None):\n",
        "        \"\"\"\n",
        "        Evaluate model performance across multiple tasks.\n",
        "\n",
        "        Args:\n",
        "            model: The model to evaluate\n",
        "            tokenizer: Model's tokenizer\n",
        "            n_tasks: Number of tasks to evaluate (None for all)\n",
        "\n",
        "        Returns:\n",
        "            Dict containing aggregated metrics\n",
        "        \"\"\"\n",
        "        if n_tasks is None:\n",
        "            n_tasks = len(self.dataset[\"test\"])\n",
        "\n",
        "        results = []\n",
        "        for i in range(n_tasks):\n",
        "            task = self.dataset[\"test\"][i]\n",
        "            prompt = task[\"prompt\"]\n",
        "\n",
        "            # Generate solution\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                max_new_tokens=512,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.8\n",
        "            )\n",
        "            generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Evaluate solution\n",
        "            result = self.evaluate_single_solution(i, generated_code)\n",
        "            results.append(result)\n",
        "\n",
        "        # Calculate aggregate metrics\n",
        "        self.metrics[\"syntax_validity\"] = np.mean([r[\"syntax_valid\"] for r in results])\n",
        "        self.metrics[\"execution_accuracy\"] = np.mean([r[\"execution_success\"] for r in results])\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "# Initialize the evaluation system\n",
        "evaluator = CodeEvaluator()\n",
        "\n",
        "def evaluate_stage(model, tokenizer, stage_name: str):\n",
        "    \"\"\"\n",
        "    Evaluate and log results for a specific evaluation stage.\n",
        "\n",
        "    Args:\n",
        "        model: Model to evaluate\n",
        "        tokenizer: Model's tokenizer\n",
        "        stage_name: Name of evaluation stage\n",
        "\n",
        "    Returns:\n",
        "        Dict containing evaluation metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating {stage_name}...\")\n",
        "    metrics = evaluator.evaluate_model(model, tokenizer)\n",
        "\n",
        "    print(f\"\\nResults for {stage_name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "HZZey8V4rdUe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Supervised Fine-Tuning Setup\n",
        "#################################\n",
        "This section prepares the environment for implementing supervised\n",
        "fine-tuning (SFT) on our code generation models.\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages for fine-tuning\n",
        "!pip install transformers datasets  # Core libraries for model training"
      ],
      "metadata": {
        "id": "_cbaZaL-KsO5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "collapsed": true,
        "outputId": "ee145a1c-f55e-47bd-8d11-6fbf230d64dc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPackage Details:\\n- transformers: Hugging Face's transformers library\\n               Provides:\\n               - Model fine-tuning capabilities\\n               - Training utilities\\n               - Optimization tools\\n               \\n- datasets: Data handling library\\n           Used for:\\n           - Loading training data\\n           - Data preprocessing\\n           - Batch preparation\\n\\nNote: SFT Process Overview:\\n1. Prepare training data\\n2. Configure training parameters\\n3. Implement training loop\\n4. Evaluate fine-tuned model\\n5. Compare with base model performance\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Training Data Preparation\n",
        "#################################\n",
        "This section loads and examines the CodeSearchNet dataset,\n",
        "which will be used for supervised fine-tuning of our models.\n",
        "\"\"\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load CodeSearchNet dataset, focusing on Python code\n",
        "# This dataset contains real-world Python code examples with documentation\n",
        "codesearchnet = load_dataset(\n",
        "    \"code_search_net\",  # Dataset name\n",
        "    \"python\"           # Language subset\n",
        ")\n",
        "\n",
        "# Display dataset structure and statistics\n",
        "print(codesearchnet)  # Shows splits, sizes, and features"
      ],
      "metadata": {
        "id": "mI66t5TyLFui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "19c4ec4df401439195571d6ad430f03f",
            "4829247ce0054f94be3a782141aaa8b0",
            "225f678733c3429f9e40730a51831a2e",
            "81ac4addca0c4f23b8fa0460a07c068d",
            "2e458380c66446a78dee0c31b4653ad4",
            "2859dc3a4a7a4ea78f03d93e480c7e37",
            "0d20ece86a0547e3ba9cb1b0f558407c",
            "d11c727d75b144799a08ce360320332c",
            "fa6c0df8534d4144b5510a6b2e68f68a",
            "fc188a91b5454acea7d1f97cf67f87ba",
            "0d3a0129ced94a49b78b5df51437dc12",
            "4e7ab72766b649df976c582fd563369e",
            "bdcc5c7eef5a4edeb961b0c66b8c4050",
            "327a4ec4945b4eda9c2c8dc53814d222",
            "fd188418cb114ddc9130cee9e020ee19",
            "b346a2ec23ac45b1a6c47d6594a4cb1b",
            "2bb13e27ec52475aaa48fae75de184a8",
            "f9a0a8293faf458d850ae08f92b75d5e",
            "b542683d3f1e4f24b2de613a82109338",
            "a83418a1adb1434e88659c57e3633872",
            "96f53d4516d04776a40c4b81761a407f",
            "5b5588c263104e62804bba259ca71812",
            "6f1052040f9e445f9c01acead63a0968",
            "a8bcf8b0b76e454ba8ebcc48d0b24060",
            "a614f8c210a04230ad4cdb63b1136e66",
            "cf9b1fadeca04c6ab6ea61971547780f",
            "3771a2d20b5841fc942760906afe7243",
            "bc531772bfac4d228aa1e3a9c8781391",
            "50e7c0f638434e7390c6a9b23e66889b",
            "dd1e8671de6044679051882f6a7b8089",
            "0e659049f8d94e6d9e0f51f4cd2b4339",
            "409532dfe8e44c278c984639b4a62840",
            "795b1c5c1d3d475cae7e29c19958886d",
            "f708ae6037034eb2a884ad771d2564c1",
            "fb4ed2e8a9fb45389d2e05416c186f19",
            "74e686bde64f44daa0b8c9f38516913d",
            "aac120b89276498b91d54a015eb47cc6",
            "268407b6f4e04b7f91017145c723b970",
            "2c3e152b11494c429d3b54ad9cb6ce62",
            "6c4297a411db453aaf1f1d856e69c5eb",
            "0bf10fc1eec84b63b48344b46a670fa6",
            "71a3f23173ee44a09d176f8dbffd941d",
            "52215d0f51744af2b08d78a8b9fe089f",
            "30995a1a000c480b9ead286256d411b0",
            "7e7cf688001747d6a8d743e607073904",
            "3335971477734f1888cd731856615d6e",
            "ac9b1143dccd4bd0b759ae3244a4f354",
            "7d941719d4104ec1886da2fb32e934ef",
            "5e7ab2beec2242e7ae2a2543cc693673",
            "f63d30dabd2d4c21866804edaa96bc1f",
            "8ece531304844e6691f056f6b0743f24",
            "8a47b146c10b4fa6b8fc4a68fad3961d",
            "32fadb5a2e62443e95d0fd6b282f2ea1",
            "8131d47077dc417493a336ad138a476f",
            "3d0bc65796d4481887c984e300d107b7",
            "7037d967a823447990856387519db5ff",
            "82a405e046bf40a3887c07bb2e7ad640",
            "b2020366bb64410aacbde9fb3ee5c5bb",
            "abd7424b736b438d8f513616b30a7423",
            "90e7bb42dd9f49e085179dcc769d3ce8",
            "613e8244d92d4052b5e5540e2080b95e",
            "4865fc99c4fd4e019d8d5acc425cda60",
            "c3dedd76209349ed99ab2dcf7633c24b",
            "5418e3ad23f04aef88ff145cd1794428",
            "973ff844f1584e01aef9ecaab1ce5c38",
            "074f0e69b19642708c8c4e9f2f293310"
          ]
        },
        "outputId": "31b6f760-17fe-4ff8-b54e-d62eb12c7988"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19c4ec4df401439195571d6ad430f03f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "code_search_net.py:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e7ab72766b649df976c582fd563369e"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "python.zip:   0%|          | 0.00/941M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f1052040f9e445f9c01acead63a0968"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f708ae6037034eb2a884ad771d2564c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e7cf688001747d6a8d743e607073904"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7037d967a823447990856387519db5ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
            "        num_rows: 412178\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
            "        num_rows: 22176\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
            "        num_rows: 23107\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCodeSearchNet Dataset Details:\\n- Contains Python code from open-source repositories\\n- Includes:\\n  * Function implementations\\n  * Docstrings\\n  * Method names\\n  * Repository metadata\\n  \\nDataset Structure:\\n- Training split\\n- Validation split\\n- Test split\\n\\nFeatures typically include:\\n- repository_name\\n- func_name\\n- whole_func_string\\n- language\\n- func_code_string\\n- docstring\\n- sha\\n- url\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Dataset Sample Inspection\n",
        "#################################\n",
        "This section examines a single sample from the CodeSearchNet dataset\n",
        "to understand its structure and content format.\n",
        "\"\"\"\n",
        "\n",
        "# Extract first training sample for inspection\n",
        "sample = codesearchnet[\"train\"][0]  # Index 0 contains first training example\n",
        "\n",
        "# Display sample contents with clear formatting\n",
        "print(f\"Code:\\n{sample['func_code_string']}\")  # Function implementation\n",
        "print(f\"Documentation:\\n{sample['func_documentation_string']}\")  # Associated documentation"
      ],
      "metadata": {
        "id": "tmczCBGILjUh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "7819e8e5-744c-4753-f158-274d1dba1d43"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code:\n",
            "def update(self, field_dict, where_clause=None):\n",
            "        '''\n",
            "        update db entry\n",
            "\n",
            "        :param field_dict: dictionary of fields and values\n",
            "        :param where_clause: where clause for the update\n",
            "        '''\n",
            "        query = '''\n",
            "        UPDATE %s SET %s\n",
            "        ''' % (\n",
            "            self._name,\n",
            "            ','.join('%s=:%s' % (k, k) for k in field_dict)\n",
            "        )\n",
            "        if where_clause:\n",
            "            query += ' WHERE %s' % (where_clause)\n",
            "        self._cursor.execute(query, field_dict)\n",
            "        self._connection.commit()\n",
            "Documentation:\n",
            "update db entry\n",
            "\n",
            "        :param field_dict: dictionary of fields and values\n",
            "        :param where_clause: where clause for the update\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nSample Analysis:\\n- func_code_string: Contains the actual Python function implementation\\n- func_documentation_string: Contains the function's documentation\\n                           (docstrings, comments, etc.)\\n\\nThis inspection helps verify:\\n1. Data format and structure\\n2. Code-documentation alignment\\n3. Content quality\\n4. Potential preprocessing needs\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Dataset Tokenization\n",
        "#################################\n",
        "This section implements the tokenization process for preparing\n",
        "the CodeSearchNet dataset for model training.\n",
        "\"\"\"\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize and format examples for training.\n",
        "\n",
        "    Args:\n",
        "        examples: Dictionary containing batched dataset examples\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing tokenized inputs and labels\n",
        "\n",
        "    Process:\n",
        "    1. Extract documentation and code\n",
        "    2. Tokenize both with padding\n",
        "    3. Process labels for loss calculation\n",
        "    4. Combine into model inputs\n",
        "    \"\"\"\n",
        "    # Extract paired examples\n",
        "    inputs = examples[\"func_documentation_string\"]   # Documentation as input\n",
        "    targets = examples[\"func_code_string\"]          # Code as target\n",
        "\n",
        "    # Tokenize documentation (inputs)\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,        # Maximum sequence length\n",
        "        truncation=True,       # Truncate if needed\n",
        "        padding=\"max_length\"   # Pad to max_length\n",
        "    )\n",
        "\n",
        "    # Tokenize code (targets/labels)\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).input_ids\n",
        "\n",
        "    # Process labels for training\n",
        "    # Replace padding tokens with -100 to ignore in loss computation\n",
        "    labels = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in seq]\n",
        "        for seq in labels\n",
        "    ]\n",
        "\n",
        "    # Combine inputs and labels\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization to entire dataset\n",
        "tokenized_datasets = codesearchnet.map(\n",
        "    tokenize_function,\n",
        "    batched=True  # Process examples in batches for efficiency\n",
        ")\n",
        "\n",
        "# Verify tokenization results\n",
        "print(tokenized_datasets[\"train\"][0])  # Display first tokenized example"
      ],
      "metadata": {
        "id": "ic6qDDtXLkh6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187,
          "referenced_widgets": [
            "06be9c451ac84dec8d2d8ed6e3d2765d",
            "9690c03015d34ed59c5731b58be59e87",
            "e6c9f80643a84fa58307aebff3e30949",
            "73965977423846f593f3860d6455ee96",
            "b195978fc2674ad3aa2afd1f39d8090a",
            "2a84ccc6c7484660b25e5cec55b7b382",
            "d81c806343664e5bb23ab813dba46ba8",
            "11ecf5f4e43e4fb991c2502248bfac37",
            "d1a0ff599b45444e807e679f4b8176af",
            "8fb52b6173fb47d3bd62089e39add48f",
            "925aeaa1efd245b7a991ae647adcbd63",
            "80f1be4bc0f440989fa54834cc23f3dc",
            "625df984e0b342308383ee977d2c5e66",
            "1c5ec9de135949cfab536192422012de",
            "6917dfcd9eab4f3db206af965fac41ac",
            "530bced6d7104f23a59129a32e89d998",
            "be1cc3bd469741aeb7a45482c428f1b5",
            "be25139eaf2440048a252fb8eb988698",
            "1f679fd4144848769e668ada81f2806f",
            "d0509adf1a9e4271af1cc56bb8ff9079",
            "c60c41fa730644b79db2c6ba98c87b3f",
            "160d874001744345b64310d5606c2010",
            "161aac2bf91947f183444437c208bcca",
            "5c8195ccde864bb98c386d7a69a77b41",
            "92c65a97f66a4a76b3fdb4fda333f6a1",
            "6372dda30b714621a5d287f2c2bc6699",
            "63844507105248fc9ea144f3b1d57d15",
            "3680ec570f444fa4bf54abe70e0155a2",
            "5890611817124b1aaa4ed4028ce33457",
            "1ffe736baa774469941a80be377ccbd2",
            "849c92405fd6482e895d9d72b13eedd0",
            "210f6abacea54e318353039d46eb31f0",
            "6cd2a04b6a1d4123bf462eb371a822bc"
          ]
        },
        "outputId": "1176e08e-8ef9-4a5b-9a65-9e6f828676ee"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/412178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06be9c451ac84dec8d2d8ed6e3d2765d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/22176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80f1be4bc0f440989fa54834cc23f3dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/23107 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "161aac2bf91947f183444437c208bcca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'repository_name': 'cisco-sas/kitty', 'func_path_in_repository': 'kitty/data/data_manager.py', 'func_name': 'Table.update', 'whole_func_string': \"def update(self, field_dict, where_clause=None):\\n        '''\\n        update db entry\\n\\n        :param field_dict: dictionary of fields and values\\n        :param where_clause: where clause for the update\\n        '''\\n        query = '''\\n        UPDATE %s SET %s\\n        ''' % (\\n            self._name,\\n            ','.join('%s=:%s' % (k, k) for k in field_dict)\\n        )\\n        if where_clause:\\n            query += ' WHERE %s' % (where_clause)\\n        self._cursor.execute(query, field_dict)\\n        self._connection.commit()\", 'language': 'python', 'func_code_string': \"def update(self, field_dict, where_clause=None):\\n        '''\\n        update db entry\\n\\n        :param field_dict: dictionary of fields and values\\n        :param where_clause: where clause for the update\\n        '''\\n        query = '''\\n        UPDATE %s SET %s\\n        ''' % (\\n            self._name,\\n            ','.join('%s=:%s' % (k, k) for k in field_dict)\\n        )\\n        if where_clause:\\n            query += ' WHERE %s' % (where_clause)\\n        self._cursor.execute(query, field_dict)\\n        self._connection.commit()\", 'func_code_tokens': ['def', 'update', '(', 'self', ',', 'field_dict', ',', 'where_clause', '=', 'None', ')', ':', 'query', '=', \"'''\\n        UPDATE %s SET %s\\n        '''\", '%', '(', 'self', '.', '_name', ',', \"','\", '.', 'join', '(', \"'%s=:%s'\", '%', '(', 'k', ',', 'k', ')', 'for', 'k', 'in', 'field_dict', ')', ')', 'if', 'where_clause', ':', 'query', '+=', \"' WHERE %s'\", '%', '(', 'where_clause', ')', 'self', '.', '_cursor', '.', 'execute', '(', 'query', ',', 'field_dict', ')', 'self', '.', '_connection', '.', 'commit', '(', ')'], 'func_documentation_string': 'update db entry\\n\\n        :param field_dict: dictionary of fields and values\\n        :param where_clause: where clause for the update', 'func_documentation_tokens': ['update', 'db', 'entry'], 'split_name': 'train', 'func_code_url': 'https://github.com/cisco-sas/kitty/blob/cb0760989dcdfe079e43ac574d872d0b18953a32/kitty/data/data_manager.py#L331-L347', 'input_ids': [32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32014, 32013, 7819, 10499, 6048, 185, 185, 436, 1191, 2280, 2010, 62, 13057, 25, 15825, 280, 5622, 285, 3029, 185, 436, 1191, 2280, 1064, 62, 535, 1029, 25, 1064, 12363, 327, 254, 3967], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32013, 1551, 3967, 7, 1180, 11, 2010, 62, 13057, 11, 1064, 62, 535, 1029, 28, 11161, 1772, 185, 436, 31269, 185, 436, 3967, 10499, 6048, 185, 185, 436, 1191, 2280, 2010, 62, 13057, 25, 15825, 280, 5622, 285, 3029, 185, 436, 1191, 2280, 1064, 62, 535, 1029, 25, 1064, 12363, 327, 254, 3967, 185, 436, 31269, 185, 436, 5151, 405, 31269, 185, 436, 27683, 3018, 82, 16634, 3018, 82, 185, 436, 31269, 3018, 334, 185, 655, 1781, 2480, 1523, 11, 185, 655, 651, 5261, 13, 11582, 1497, 4, 82, 28, 25, 4, 82, 6, 3018, 334, 74, 11, 528, 8, 327, 528, 279, 2010, 62, 13057, 8, 185, 436, 2189, 185, 436, 562, 1064, 62, 535, 1029, 25, 185, 655, 5151, 8528, 651, 11294, 3018, 82, 6, 3018, 334, 2234, 62, 535, 1029, 8, 185, 436, 1781, 2480, 21769, 13, 15238, 7, 5112, 11, 2010, 62, 13057, 8, 185, 436, 1781, 2480, 14051, 13, 18501, 822]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNote: Tokenization process includes:\\n1. Input processing:\\n   - Documentation text → token IDs\\n   - Padding to fixed length\\n   - Truncation of long sequences\\n\\n2. Label processing:\\n   - Code text → token IDs\\n   - Padding token replacement\\n   - Loss masking setup\\n\\n3. Verification:\\n   - Input token structure\\n   - Label formatting\\n   - Padding handling\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Training Setup Configuration\n",
        "#################################\n",
        "This section configures the training environment, including model\n",
        "initialization, tokenizer setup, and training parameters.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "# Initialize model for fine-tuning\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/SemCoder\",          # Local model path\n",
        "    torch_dtype=torch.bfloat16    # Use bfloat16 for memory efficiency\n",
        ").cuda()                          # Move to GPU\n",
        "\n",
        "# Setup tokenizer from local files\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/SemCoder\")\n",
        "\n",
        "# Initialize data collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Configure training parameters\n",
        "training_args = TrainingArguments(\n",
        "    # Basic training configuration\n",
        "    output_dir=\"./results\",          # Directory for saving outputs\n",
        "    eval_strategy=\"epoch\",           # Evaluate after each epoch\n",
        "    learning_rate=5e-5,             # Conservative learning rate\n",
        "\n",
        "    # Batch size configuration\n",
        "    per_device_train_batch_size=1,  # Small batch size due to memory constraints\n",
        "    per_device_eval_batch_size=1,   # Matching evaluation batch size\n",
        "    gradient_accumulation_steps=32,  # Accumulate gradients to simulate larger batch\n",
        "\n",
        "    # Training duration\n",
        "    num_train_epochs=3,             # Number of training epochs\n",
        "\n",
        "    # Optimization parameters\n",
        "    weight_decay=0.01,              # L2 regularization\n",
        "    fp16=True,                      # Enable mixed precision training\n",
        "\n",
        "    # Logging and saving configuration\n",
        "    logging_dir=\"./logs\",           # Directory for logs\n",
        "    logging_steps=10,               # Log every 10 steps\n",
        "    save_total_limit=2,             # Keep only last 2 checkpoints\n",
        "    report_to=\"none\",               # Disable external logging\n",
        ")\n",
        "\n",
        "# Initialize the training framework\n",
        "trainer = Trainer(\n",
        "    model=model,                           # Fine-tuning model\n",
        "    args=training_args,                    # Training configuration\n",
        "    train_dataset=tokenized_datasets[\"train\"],        # Training data\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],    # Validation data\n",
        "    data_collator=data_collator,                     # Padding utility\n",
        ")"
      ],
      "metadata": {
        "id": "78XUz-s6LnVn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c3990650d728481e91f1cc89379c4e5d",
            "ba766732890e42e8a6e84f77e8ea4f8a",
            "a040e4796c054310ae515405a6914cba",
            "71dda3747cd74c7c8892f0992d02b199",
            "81e32a50b8e6494cb2fc7c25231885f5",
            "38d304c29f504e6787f890df409d3098",
            "aa9eba186636494cad4b3cd1a6758465",
            "f5e12a82c57349e1b5b739892bcd69dd",
            "9af42b6d50e548828ab12b8fa3a6d965",
            "fb6ed04808954e0f9ca77dd3a88737be",
            "bade62dc64c14dcbb93d276eb07824dd"
          ]
        },
        "outputId": "85806d26-5d8c-4fa2-e005-8eb115400853"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3990650d728481e91f1cc89379c4e5d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Model Fine-Tuning - Minimal Memory\n",
        "#################################\n",
        "This section attempts fine-tuning with absolute minimal\n",
        "memory footprint and explicit PyTorch memory settings.\n",
        "\"\"\"\n",
        "\n",
        "# Set PyTorch memory management settings\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32'\n",
        "\n",
        "# Clear all GPU memory\n",
        "clear_memory()\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure training with absolute minimal settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "\n",
        "    # Absolute minimal batch and data settings\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=512,    # Extreme accumulation\n",
        "    max_grad_norm=0.3,                 # More aggressive gradient clipping\n",
        "\n",
        "    # Maximum memory optimization\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=0,\n",
        "    group_by_length=True,\n",
        "    remove_unused_columns=True,\n",
        "\n",
        "    # Minimal training loop\n",
        "    max_steps=10,                      # Just try 10 steps initially\n",
        "    eval_strategy=\"no\",                # No evaluation\n",
        "    logging_steps=5,                   # Minimal logging\n",
        "    save_total_limit=1,               # Keep only one checkpoint\n",
        "    save_steps=10,                    # Save at the end only\n",
        "    report_to=\"none\",                 # No reporting\n",
        "\n",
        "    # Conservative hyperparameters\n",
        "    learning_rate=1e-5,               # Reduced learning rate\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=2,                   # Minimal warmup\n",
        "\n",
        "    # Additional memory optimizations\n",
        "    optim=\"adamw_torch_fused\",        # Use fused optimizer\n",
        "    ddp_find_unused_parameters=False,  # Disable unused parameter detection\n",
        ")\n",
        "\n",
        "# Create minimal dataset\n",
        "tiny_train_dataset = tokenized_datasets[\"train\"].select(range(20))  # Just 20 examples\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tiny_train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting minimal fine-tuning test...\")\n",
        "print(\"Using only 20 examples and 10 steps...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3Ci7EvGG8xx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Model Saving\n",
        "#################################\n",
        "This section saves the fine-tuned model and tokenizer\n",
        "for future use and evaluation.\n",
        "\"\"\"\n",
        "\n",
        "# Save complete model artifacts\n",
        "trainer.save_model(\"./semcoder-sft\")          # Save model weights and configuration\n",
        "tokenizer.save_pretrained(\"./semcoder-sft\")   # Save associated tokenizer\n",
        "\n",
        "print(\"Fine-tuned model saved!\")"
      ],
      "metadata": {
        "id": "5L5OlGFyLsQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#################################\n",
        "# Initial Model Testing\n",
        "#################################\n",
        "This section performs an initial test of the fine-tuned model\n",
        "using a simple programming task.\n",
        "\"\"\"\n",
        "\n",
        "# Load the fine-tuned model for testing\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./semcoder-sft\"   # Path to saved model\n",
        ").cuda()              # Move to GPU\n",
        "\n",
        "# Define test case\n",
        "test_prompt = \"Write a Python function to find the maximum element in a list.\"\n",
        "\n",
        "# Prepare input for generation\n",
        "inputs = tokenizer(\n",
        "    test_prompt,\n",
        "    return_tensors=\"pt\"   # Convert to PyTorch tensor\n",
        ").to(fine_tuned_model.device)\n",
        "\n",
        "# Generate code using fine-tuned model\n",
        "outputs = fine_tuned_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_new_tokens=100    # Limit generation length\n",
        ")\n",
        "\n",
        "# Convert generated tokens to readable code\n",
        "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Display results\n",
        "print(\"Generated Code:\\n\", generated_code)"
      ],
      "metadata": {
        "id": "GHzJMpbMLt4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:  Implement CodeDPO with SemCoder**"
      ],
      "metadata": {
        "id": "7QXirlZ2Z3Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMwglqcUhHOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:  Implement SemCoder for SWE Bench Verified**"
      ],
      "metadata": {
        "id": "1C2DYFlNygbj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNIIfGrh5KxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}