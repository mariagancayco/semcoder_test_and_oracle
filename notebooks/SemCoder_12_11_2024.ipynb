{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "JDG6m5xoW2b1",
        "KzOoIRgm0ndG",
        "YaormCswUMDk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Generative Models for Code** -- Final Project<br><br>\n",
        "**Maria Gancayco (mig2131@columbia.edu)**<br>\n",
        "**Stephen Wright (svw2112@columbia.edu)**<br>\n",
        "*Due:* Wednesday, 12 Dec 2024 at 11:59pm ET"
      ],
      "metadata": {
        "id": "0YBqgsFTo1AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Setup"
      ],
      "metadata": {
        "id": "HJZ1QzRd0sii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: Environment and Memory Management\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# Check and display GPU availability for transparency\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
        "\n",
        "# Memory management utilities\n",
        "def clear_memory() -> None:\n",
        "    \"\"\"\n",
        "    Clears GPU memory cache and performs garbage collection.\n",
        "\n",
        "    This function is crucial for maintaining optimal memory usage during model evaluation,\n",
        "    especially when loading and comparing multiple large language models.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()  # Clear CUDA cache\n",
        "    gc.collect()  # Trigger Python garbage collection\n",
        "\n",
        "def get_memory_status() -> None:\n",
        "    \"\"\"\n",
        "    Displays current GPU memory usage statistics.\n",
        "\n",
        "    Reports both allocated and reserved memory in megabytes (MB).\n",
        "    This helps monitor memory consumption during model operations.\n",
        "\n",
        "    Note:\n",
        "        - Allocated memory: Actually used GPU memory\n",
        "        - Reserved memory: Total memory reserved by PyTorch\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        # Convert bytes to MB for better readability\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"GPU Memory: Allocated: {allocated:.2f}MB, Reserved: {reserved:.2f}MB\")\n",
        "clear_memory()\n",
        "# Initialize by checking current memory status\n",
        "get_memory_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgvmsDDzsLYC",
        "outputId": "a69f8fc6-e00c-4de0-c430-584eb2f93665",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU device name: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: Allocated: 0.00MB, Reserved: 0.00MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration and Setup\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Configuration dataclass containing all hyperparameters and settings for model evaluation.\n",
        "\n",
        "    Attributes:\n",
        "        model_name (str): Name/path of the model to be evaluated\n",
        "        batch_size (int): Number of samples processed in each batch\n",
        "        learning_rate (float): Learning rate for model optimization\n",
        "        num_epochs (int): Number of training epochs\n",
        "        max_seq_length (int): Maximum sequence length for input tokenization\n",
        "        gradient_accumulation_steps (int): Number of steps to accumulate gradients\n",
        "        warmup_steps (Optional[int]): Number of warmup steps for learning rate scheduler\n",
        "        weight_decay (float): L2 regularization factor\n",
        "        eval_steps (int): Frequency of evaluation steps\n",
        "        save_steps (int): Frequency of model checkpoint saves\n",
        "        logging_steps (int): Frequency of logging training metrics\n",
        "    \"\"\"\n",
        "    model_name: str\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    num_epochs: int\n",
        "    max_seq_length: int\n",
        "    gradient_accumulation_steps: int\n",
        "    warmup_steps: Optional[int] = None\n",
        "    weight_decay: float = 0.01\n",
        "    eval_steps: int = 100\n",
        "    save_steps: int = 100\n",
        "    logging_steps: int = 10\n",
        "\n",
        "# Set up results directory for storing evaluation outputs\n",
        "results_dir = Path(\"./results\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
        "\n",
        "print(\"Configuration and directories initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX4vwvGqs3-G",
        "outputId": "3256586f-1fcf-4ab3-abb9-9219338c6d89",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and directories initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = ExperimentConfig(\n",
        "    model_name=\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n",
        "    batch_size=1,                    # Small batch size due to model size\n",
        "    learning_rate=5e-5,             # Conservative learning rate for fine-tuning\n",
        "    num_epochs=3,                   # Number of training epochs\n",
        "    max_seq_length=512,            # Maximum sequence length for input processing\n",
        "    gradient_accumulation_steps=32, # Accumulate gradients to simulate larger batch size\n",
        "    warmup_steps=100               # Warmup steps for learning rate scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "BlGsbQiSJt4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Dependencies and Imports\n",
        "\n",
        "# Install core dependencies for transformer model handling and evaluation\n",
        "!pip install transformers torch timeout-decorator\n",
        "\n",
        "# Import required libraries\n",
        "import torch  # PyTorch for deep learning operations\n",
        "from transformers import (\n",
        "    AutoTokenizer,         # For tokenization of input text\n",
        "    AutoModelForCausalLM   # For loading pre-trained causal language models\n",
        ")\n",
        "from typing import List, Dict  # Type hints for better code documentation\n",
        "import timeout_decorator\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Y0TX5UXgwG1q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"openai_humaneval\")"
      ],
      "metadata": {
        "id": "APCk2qR2dedO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Loading and Code Generation\n",
        "\n",
        "def load_model_and_tokenizer(config: ExperimentConfig) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "\n",
        "    try:\n",
        "        # Clear memory before loading new model to prevent OOM errors\n",
        "        clear_memory()\n",
        "\n",
        "        print(f\"Loading {config.model_name}...\")\n",
        "\n",
        "        # Initialize tokenizer with remote code execution enabled\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=True  # Required for custom tokenizer implementations\n",
        "        )\n",
        "\n",
        "        # Load model with memory-efficient settings\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            config.model_name,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.bfloat16,    # Use bfloat16 for memory efficiency\n",
        "            device_map=\"auto\",             # Optimize model placement across available devices\n",
        "            low_cpu_mem_usage=True         # Minimize CPU memory during loading\n",
        "        )\n",
        "\n",
        "        # Enable gradient checkpointing if available\n",
        "        if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "            model.gradient_checkpointing_enable()  # Trade compute for memory savings\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        get_memory_status()  # Display current memory usage\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generate_code(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 50\n",
        ") -> str:\n",
        "\n",
        "    try:\n",
        "        # Format prompt as chat message\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        print(\"Generating inputs...\")\n",
        "        # Tokenize input with chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "        print(\"Generating outputs...\")\n",
        "        # Generate code with specified parameters\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=max_new_tokens,  # Control generation length\n",
        "            do_sample=True,                 # Enable sampling-based generation\n",
        "            temperature=temperature,         # Control randomness\n",
        "            top_p=top_p,                    # Nucleus sampling threshold\n",
        "            top_k=top_k,                    # Top-k sampling parameter\n",
        "            num_return_sequences=1,         # Generate single sequence\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode and return only the generated portion (excluding prompt)\n",
        "        return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in code generation: {str(e)}\")\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "zNeAngZKtFzY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and tokenizer using configuration\n",
        "deepseek_7b_model, deepseek_7b_tokenizer = load_model_and_tokenizer(config)"
      ],
      "metadata": {
        "id": "F66bJYxMJHn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HumanEval+ Test Case Generation"
      ],
      "metadata": {
        "id": "nUHM4NGUFknM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def generate_humaneval_plus_tests(model_type, deepseek_model=None, deepseek_tokenizer=None, num_total_tests=100):\n",
        "    dataset = load_dataset(\"openai_humaneval\")\n",
        "    results = []\n",
        "    total_tests_generated = 0\n",
        "    with open(f'{model_type}_test_case_generation_results.txt', 'w') as f:\n",
        "      for i in range(len(dataset['test'])):\n",
        "          if total_tests_generated >= num_total_tests:\n",
        "              break\n",
        "\n",
        "          problem = dataset['test'][i]\n",
        "          prompt = problem['prompt']\n",
        "          solution = problem['canonical_solution']\n",
        "          entry_point = problem['entry_point']\n",
        "          test_code = problem['test']\n",
        "\n",
        "          # Extract working test cases\n",
        "          check_match = re.search(r'def check\\(candidate\\):\\s*(.*?)(?=\\n\\n|$)', test_code, re.DOTALL)\n",
        "          test_cases = re.findall(r'assert.*?(?=\\n|$)', check_match.group(1) if check_match else '')\n",
        "\n",
        "          test_prompt = f\"\"\"\n",
        "Please provide executable test cases for this function:\n",
        "{prompt}\n",
        "\n",
        "Working test examples:\n",
        "{test_cases}\n",
        "\n",
        "Include these types of tests:\n",
        "1. Performance test:\n",
        "def test_{entry_point}_perf():\n",
        "    {test_cases[0].replace('candidate', entry_point)}\n",
        "\n",
        "2. Edge case test:\n",
        "def test_{entry_point}_edge():\n",
        "    {test_cases[-1].replace('candidate', entry_point)}\n",
        "\n",
        "3. Error test:\n",
        "def test_{entry_point}_error():\n",
        "    with pytest.raises(TypeError):\n",
        "        {entry_point}(None)\n",
        "\n",
        "Only provide executable test cases. No placeholders.\"\"\"\n",
        "\n",
        "          try:\n",
        "              generated_tests, cleaned_tests = None, None\n",
        "              if model_type == \"semcoder\":\n",
        "                generated_tests = semcoder.generate_code(test_prompt)\n",
        "                cleaned_tests = evaluator.clean_generated_code(generated_tests)\n",
        "              elif \"deepseek\"in model_type:\n",
        "                generated_tests = generate_code(deepseek_model, deepseek_tokenizer, test_prompt, max_new_tokens=4096)\n",
        "                cleaned_tests = clean_deepseek_generated_code(generated_tests)\n",
        "              elif model_type == \"gpt-4\":\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                        {\"role\": \"user\", \"content\": test_prompt}\n",
        "                    ],\n",
        "                    max_tokens=4096,\n",
        "                    temperature=0.8,\n",
        "                    top_p=0.95\n",
        "                )\n",
        "                generated_tests = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "                cleaned_tests = clean_deepseek_generated_code(generated_tests) #TODO:- Please rename this since we're also using for OpenAI\n",
        "              if cleaned_tests:\n",
        "                  num_tests = len(re.findall(r'def test_', cleaned_tests))\n",
        "                  total_tests_generated += num_tests\n",
        "\n",
        "                  result = {\n",
        "                      'problem_id': i,\n",
        "                      'entry_point': entry_point,\n",
        "                      'tests': cleaned_tests,\n",
        "                      'num_tests': num_tests\n",
        "                  }\n",
        "                  results.append(result)\n",
        "\n",
        "                  print(f\"Generated {num_tests} enhanced tests\")\n",
        "                  print(f\"Total tests so far: {total_tests_generated}/{num_total_tests}\")\n",
        "                  print(\"\\nTest prompt:\")\n",
        "                  print(test_prompt)\n",
        "                  print(\"\\nGenerated tests:\")\n",
        "                  print(generated_tests)\n",
        "                  print(\"\\nCleaned tests:\")\n",
        "                  print(cleaned_tests)\n",
        "\n",
        "                  f.write(f\"Generated {num_tests} enhanced tests\\n\")\n",
        "                  f.write(f\"Total tests so far: {total_tests_generated}/{num_total_tests}\")\n",
        "                  f.write(\"\\nGenerated tests:\\n\")\n",
        "                  f.write(cleaned_tests + \"\\n\")\n",
        "              else:\n",
        "                  print(\"No valid tests generated\")\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"Error generating tests: {str(e)}\")\n",
        "              continue\n",
        "\n",
        "    return results, total_tests_generated"
      ],
      "metadata": {
        "id": "ok47e3T_IVNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate HumanEval+ tests\n",
        "print(\"Generating HumanEval+ test cases...\")\n",
        "plus_results, total_plus_tests = generate_humaneval_plus_tests(\"deepseek_7b\", deepseek_model=deepseek_7b_model, deepseek_tokenizer=deepseek_7b_tokenizer, num_total_tests=100)"
      ],
      "metadata": {
        "id": "ocMFO0cQK7gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Utilities"
      ],
      "metadata": {
        "id": "6bwKickq6PzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_test_suites(content: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract test suites from the content and format them with function calls.\n",
        "    Handles both standalone assert statements and function definitions.\n",
        "    Returns a list of formatted test suite strings.\n",
        "    \"\"\"\n",
        "    # Split content into test suite blocks\n",
        "    test_blocks = re.split(r'Generated \\d+ enhanced tests\\nTotal tests so far: \\d+/\\d+\\n+Generated tests:', content)\n",
        "\n",
        "    # Remove empty blocks\n",
        "    test_blocks = [block.strip() for block in test_blocks if block.strip()]\n",
        "\n",
        "    formatted_suites = []\n",
        "    for block in test_blocks:\n",
        "        if \"unittest.TestCase\" in block:\n",
        "          print(\"FORMATTED TEST SUITE:\")\n",
        "          print(block)\n",
        "          formatted_suites.append(block)\n",
        "          continue\n",
        "\n",
        "\n",
        "        print(\"ORIGINAL TEST SUITE:\")\n",
        "        print(block)\n",
        "        suite_parts = []\n",
        "\n",
        "        # First, collect any imports at the start of the block\n",
        "        import_statements = re.findall(r'^import [^\\n]+', block, re.MULTILINE)\n",
        "\n",
        "        # Extract function-based tests\n",
        "        test_functions = re.finditer(r'def (test_\\w+)\\(\\):\\n((?:[ ]{4}.*\\n?)+)', block)\n",
        "\n",
        "        # Extract standalone assert statements (not within functions)\n",
        "        # Looking for asserts that are at the start of a line and not indented\n",
        "        standalone_asserts = re.finditer(r'^assert [^\\n]+$', block, re.MULTILINE)\n",
        "\n",
        "        # Extract standalone pytest.raises statements\n",
        "        standalone_raises = re.finditer(r'^with pytest\\.raises\\([^\\)]+\\):\\n[ ]{4}[^\\n]+\\n', block, re.MULTILINE)\n",
        "\n",
        "        # Add imports if they exist\n",
        "        if import_statements:\n",
        "            suite_parts.extend(import_statements)\n",
        "            suite_parts.append(\"\")  # Add blank line after imports\n",
        "\n",
        "        # Add standalone asserts\n",
        "        for match in standalone_asserts:\n",
        "            suite_parts.append(match.group(0))\n",
        "\n",
        "        # Add standalone pytest.raises\n",
        "        for match in standalone_raises:\n",
        "            suite_parts.append(match.group(0).rstrip())\n",
        "\n",
        "        # Add function-based tests\n",
        "        for match in test_functions:\n",
        "            func_name = match.group(1)\n",
        "            func_body = match.group(2).rstrip()\n",
        "            formatted_func = f\"def {func_name}():\\n{func_body}\\n{func_name}()\"\n",
        "            suite_parts.append(formatted_func)\n",
        "\n",
        "        if suite_parts:\n",
        "            formatted_suite = \"\\n\".join(suite_parts)\n",
        "            print(\"FORMATTED TEST SUITE:\")\n",
        "            print(formatted_suite)\n",
        "            print(\"-\" * 50)\n",
        "            formatted_suites.append(formatted_suite)\n",
        "\n",
        "    return formatted_suites\n",
        "\n",
        "def process_file_path(file_path: str) -> list[str]:\n",
        "    \"\"\"Process a file by path and return list of formatted test suite strings.\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "    return extract_test_suites(content)\n",
        "\n",
        "def process_file_content(content: str) -> list[str]:\n",
        "    \"\"\"Process file content directly and return list of formatted test suite strings.\"\"\"\n",
        "    return extract_test_suites(content)"
      ],
      "metadata": {
        "id": "WBeBJle2E4Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "def execute_test_case(code: str, test_case: str) -> bool:\n",
        "    try:\n",
        "        namespace = {}\n",
        "        # Execute the function code\n",
        "        exec(code, namespace)\n",
        "        # Execute the test case\n",
        "        exec(\"import pytest\", namespace)\n",
        "        exec(test_case, namespace)\n",
        "        return True\n",
        "    except pytest.raises.Exception:\n",
        "        # This catches when pytest.raises() fails (i.e., expected exception wasn't raised)\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        # Catch any other exceptions\n",
        "        return False\n",
        "\n",
        "def check_syntax(code: str) -> bool:\n",
        "        try:\n",
        "            compile(code, '<string>', 'exec')\n",
        "            return True\n",
        "        except SyntaxError:\n",
        "            return False\n",
        "@timeout_decorator.timeout(5)\n",
        "def evaluate_single_test_suite(solution: str,\n",
        "                               generated_tests: str) -> Dict:\n",
        "        syntax_valid = check_syntax(solution + \"\\n\" + generated_tests)\n",
        "\n",
        "        # Execute test cases if syntax is valid\n",
        "        if syntax_valid:\n",
        "            # TODO:- consider using thread pool for parallel test execution\n",
        "            execution_success = execute_test_case(solution, generated_tests)\n",
        "        else:\n",
        "            execution_success = False\n",
        "\n",
        "        return {\n",
        "            \"syntax_valid\": syntax_valid,\n",
        "            \"execution_success\": execution_success\n",
        "        }\n",
        "def evaluate_test_suite(model_type, dataset, n_tasks, test_suites):\n",
        "  solutions = dataset['test'][\"canonical_solution\"]\n",
        "  metrics = {\"pass@1\": 0.0,      # Single-attempt success rate\n",
        "            \"pass@10\": 0.0,     # Success within 10 attempts\n",
        "            \"pass@100\": 0.0,    # Success within 100 attempts\n",
        "            \"syntax_validity\": 0.0,  # Syntactic correctness\n",
        "            \"execution_accuracy\": 0.0  # Functional correctness\n",
        "  }\n",
        "  results = []\n",
        "  with open(f'{model_type}_test_case_generation_accuracy_results.txt', 'w') as f:\n",
        "          for i in range(n_tasks):\n",
        "              solution = solutions[i]\n",
        "              full_solution = dataset['test'][\"prompt\"][i] + solution\n",
        "              cleaned_tests = test_suites[i]\n",
        "              result = evaluate_single_test_suite(full_solution, cleaned_tests)\n",
        "\n",
        "              f.write(f\"PROBLEM {i}:\\n\")\n",
        "              print(f\"PROBLEM {i}:\\n\")\n",
        "              f.write(\"CANONICAL SOLUTION:\\n\")\n",
        "              print(\"CANONICAL SOLUTION:\\n\")\n",
        "              f.write(full_solution + \"\\n\")\n",
        "              print(full_solution + \"\\n\")\n",
        "              f.write(\"CLEANED TESTS:\\n\")\n",
        "              print(\"CLEANED TESTS:\\n\")\n",
        "              f.write(cleaned_tests + \"\\n\")\n",
        "              print(cleaned_tests)\n",
        "              f.write(\"RESULT:\\n\" + str(result) + \"\\n\")\n",
        "              print(\"RESULT:\\n\" + str(result))\n",
        "\n",
        "              results.append(result)\n",
        "\n",
        "          metrics[\"syntax_validity\"] = np.mean([r[\"syntax_valid\"] for r in results])\n",
        "          metrics[\"execution_accuracy\"] = np.mean([r[\"execution_success\"] for r in results])\n",
        "          f.write(str(metrics))\n",
        "\n",
        "def clean_deepseek_generated_code(code: str) -> str:\n",
        "        \"\"\"Clean up generated code to extract only the functions.\"\"\"\n",
        "        lines = code.split('\\n')\n",
        "        cleaned_lines = []\n",
        "        found_start = False\n",
        "        found_test_func_call = False\n",
        "        for line in lines:\n",
        "            if line.startswith('```python'):\n",
        "                found_start = True\n",
        "            elif line.startswith('```'):\n",
        "                if found_test_func_call: break\n",
        "                else: found_start = False\n",
        "            elif found_start:\n",
        "                if line.startswith('test_') and line.endswith('()'):\n",
        "                    found_test_func_call = True\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(cleaned_lines).strip()"
      ],
      "metadata": {
        "id": "qgB4E7XDH7xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_7b_extracted_test_suites = process_file_path(\"/content/deepseek_7b_test_case_generation_results.txt\")"
      ],
      "metadata": {
        "id": "roFIR_sMOHri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_test_suite(\"deepseek_7b\", dataset, len(deepseek_7b_extracted_test_suites), deepseek_7b_extracted_test_suites)"
      ],
      "metadata": {
        "id": "Pdj8aDaZdVsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SemCoder Simple Prompt Results"
      ],
      "metadata": {
        "id": "JDG6m5xoW2b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_generated_code(code: str) -> str:\n",
        "    \"\"\"Clean up generated code to extract only the functions.\"\"\"\n",
        "    lines = code.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    in_function = False\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('def '):\n",
        "            in_function = True\n",
        "            cleaned_lines.append(line)\n",
        "        elif in_function and (line.startswith('    ') or not line.strip()):\n",
        "            cleaned_lines.append(line)\n",
        "        elif in_function and line.strip() and not line.startswith('    '):\n",
        "            in_function = False\n",
        "            cleaned_lines.append('')\n",
        "\n",
        "    return '\\n'.join(cleaned_lines).strip()"
      ],
      "metadata": {
        "id": "F5j6feh4XGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSeek Simple Prompt Results"
      ],
      "metadata": {
        "id": "KzOoIRgm0ndG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "import timeout_decorator\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "OMHcCCL4bwbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "            \"pass@1\": 0.0,      # Single-attempt success rate\n",
        "            \"pass@10\": 0.0,     # Success within 10 attempts\n",
        "            \"pass@100\": 0.0,    # Success within 100 attempts\n",
        "            \"syntax_validity\": 0.0,  # Syntactic correctness\n",
        "            \"execution_accuracy\": 0.0  # Functional correctness\n",
        "}\n",
        "# TODO:- rename to accomodate that we're also using this for GPT-4\n",
        "\n",
        "def evaluate_model(model, dataset, model_type, tokenizer, n_tasks: int = None):\n",
        "        solutions = dataset['test'][\"canonical_solution\"]\n",
        "        if n_tasks is None:\n",
        "            n_tasks = len(solutions)\n",
        "\n",
        "        results = []\n",
        "        with open(f'{model_type}_test_case_generation_results.txt', 'w') as f:\n",
        "          for i in range(n_tasks):\n",
        "              solution = solutions[i]\n",
        "              full_solution = dataset['test'][\"prompt\"][i] + solution\n",
        "\n",
        "              prompt = f\"\"\"\n",
        "              Please provide and execute a set of test cases for the following function:\n",
        "              {full_solution}\n",
        "\n",
        "              Please do not include natural language or anything that cannot be compiled/executed.\n",
        "              Please only provided the test cases and their immediate execution.\n",
        "\n",
        "              Example:\n",
        "              def test_hello_with_name():\n",
        "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
        "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
        "              test_hello_with_name()\n",
        "\n",
        "              def test_hello_without_name():\n",
        "                  assert hello(None) == \"Hello, world\"\n",
        "                  assert hello(\"\") == \"Hello, world\"\n",
        "              test_hello_without_name()\n",
        "              \"\"\"\n",
        "              generated_tests = \"\"\n",
        "              if model_type == \"deepseek\":\n",
        "                  generated_tests = generate_code(\n",
        "                      model,\n",
        "                      tokenizer,\n",
        "                      prompt,\n",
        "                      max_new_tokens=4096\n",
        "                  )\n",
        "              elif model_type == \"semcoder\":\n",
        "                  generated_tests = model.generate_code(prompt, max_new_tokens=4096)\n",
        "\n",
        "              cleaned_tests = clean_deepseek_generated_code(generated_tests) if model_type == \"deepseek\" else \"\" #no-op for now\n",
        "              result = evaluate_single_test_suite(full_solution, cleaned_tests)\n",
        "\n",
        "              f.write(f\"PROBLEM {i}:\\n\")\n",
        "              print(f\"PROBLEM {i}:\\n\")\n",
        "              f.write(\"CANONICAL SOLUTION:\\n\")\n",
        "              print(\"CANONICAL SOLUTION:\\n\")\n",
        "              f.write(full_solution + \"\\n\")\n",
        "              print(full_solution + \"\\n\")\n",
        "              f.write(\"GENERATED TESTS:\\n\")\n",
        "              print(\"GENERATED TESTS:\\n\")\n",
        "              f.write(generated_tests + \"\\n\")\n",
        "              print(generated_tests)\n",
        "              f.write(\"CLEANED TESTS:\\n\")\n",
        "              print(\"CLEANED TESTS:\\n\")\n",
        "              f.write(cleaned_tests + \"\\n\")\n",
        "              print(cleaned_tests)\n",
        "              f.write(\"RESULT:\\n\" + str(result) + \"\\n\")\n",
        "              print(\"RESULT:\\n\" + str(result))\n",
        "\n",
        "              results.append(result)\n",
        "\n",
        "          # Calculate aggregate metrics\n",
        "          metrics[\"syntax_validity\"] = np.mean([r[\"syntax_valid\"] for r in results])\n",
        "          metrics[\"execution_accuracy\"] = np.mean([r[\"execution_success\"] for r in results])\n",
        "          f.write(str(metrics))\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "fAzhBR72brWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = TestCaseEvaluator()"
      ],
      "metadata": {
        "id": "yvhHTDR-kRwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluator.evaluate_model(model, \"deepseek\", tokenizer, 100)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "from google.colab import files\n",
        "files.download('deepseek_test_case_generation_results.txt')"
      ],
      "metadata": {
        "id": "pZgl0946phra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardized SemCoder Results"
      ],
      "metadata": {
        "id": "YaormCswUMDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluator.evaluate_model(semcoder, \"semcoder\", tokenizer, 100)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "from google.colab import files\n",
        "files.download('semcoder_test_case_generation_results.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G5u-CaLrUTpL",
        "outputId": "018d09ad-148e-4edf-923e-7e10f4397ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 0:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = abs(elem - elem2)\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "\n",
            "GENERATED TESTS:\n",
            "\n",
            "\n",
            "              Please provide and execute a set of test cases for the following function:\n",
            "              from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = abs(elem - elem2)\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "\n",
            "              Please do not include natural language or anything that cannot be compiled/executed.\n",
            "              Please only provided the test cases and their immediate execution.\n",
            "\n",
            "              Example:\n",
            "              def test_hello_with_name():\n",
            "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
            "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
            "              test_hello_with_name()\n",
            "\n",
            "              def test_hello_without_name():\n",
            "                  assert hello(None) == \"Hello, world\"\n",
            "                  assert hello(\"\") == \"Hello, world\"\n",
            "              test_hello_without_name()\n",
            "              \n",
            "CLEANED TESTS:\n",
            "\n",
            "\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 1:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "    result = []\n",
            "    current_string = []\n",
            "    current_depth = 0\n",
            "\n",
            "    for c in paren_string:\n",
            "        if c == '(':\n",
            "            current_depth += 1\n",
            "            current_string.append(c)\n",
            "        elif c == ')':\n",
            "            current_depth -= 1\n",
            "            current_string.append(c)\n",
            "\n",
            "            if current_depth == 0:\n",
            "                result.append(''.join(current_string))\n",
            "                current_string.clear()\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "GENERATED TESTS:\n",
            "\n",
            "\n",
            "              Please provide and execute a set of test cases for the following function:\n",
            "              from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "    result = []\n",
            "    current_string = []\n",
            "    current_depth = 0\n",
            "\n",
            "    for c in paren_string:\n",
            "        if c == '(':\n",
            "            current_depth += 1\n",
            "            current_string.append(c)\n",
            "        elif c == ')':\n",
            "            current_depth -= 1\n",
            "            current_string.append(c)\n",
            "\n",
            "            if current_depth == 0:\n",
            "                result.append(''.join(current_string))\n",
            "                current_string.clear()\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "              Please do not include natural language or anything that cannot be compiled/executed.\n",
            "              Please only provided the test cases and their immediate execution.\n",
            "\n",
            "              Example:\n",
            "              def test_hello_with_name():\n",
            "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
            "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
            "              test_hello_with_name()\n",
            "\n",
            "              def test_hello_without_name():\n",
            "                  assert hello(None) == \"Hello, world\"\n",
            "                  assert hello(\"\") == \"Hello, world\"\n",
            "              test_hello_without_name()\n",
            "              \n",
            "CLEANED TESTS:\n",
            "\n",
            "\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 2:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "    return number % 1.0\n",
            "\n",
            "\n",
            "GENERATED TESTS:\n",
            "\n",
            "\n",
            "              Please provide and execute a set of test cases for the following function:\n",
            "              \n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "    return number % 1.0\n",
            "\n",
            "\n",
            "              Please do not include natural language or anything that cannot be compiled/executed.\n",
            "              Please only provided the test cases and their immediate execution.\n",
            "\n",
            "              Example:\n",
            "              def test_hello_with_name():\n",
            "                  assert hello(\"Alice\") == \"Hello, Alice\"\n",
            "                  assert hello(\"Bob\") == \"Hello, Bob\"\n",
            "              test_hello_with_name()\n",
            "\n",
            "              def test_hello_without_name():\n",
            "                  assert hello(None) == \"Hello, world\"\n",
            "                  assert hello(\"\") == \"Hello, world\"\n",
            "              test_hello_without_name()\n",
            "              \n",
            "CLEANED TESTS:\n",
            "\n",
            "\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-7ff308e44fc3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemcoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"semcoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{metric}: {value:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'semcoder_test_case_generation_results.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-125da5b34cdb>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model, model_type, tokenizer, n_tasks)\u001b[0m\n\u001b[1;32m    103\u001b[0m                   )\n\u001b[1;32m    104\u001b[0m               \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"semcoder\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                   \u001b[0mgenerated_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m               \u001b[0mcleaned_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_generated_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_tests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a45ccccdd31e>\u001b[0m in \u001b[0;36mgenerate_code\u001b[0;34m(self, prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Generate with specified parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             outputs = self.model.generate(\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 )\n\u001b[1;32m    944\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    356\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Code Coverage Assessment"
      ],
      "metadata": {
        "id": "m12pf6IXmQmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install pytest pytest-cov coverage\n",
        "from google.colab import files  # Colab-specific import"
      ],
      "metadata": {
        "id": "tk8dFxQ-vTE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "import subprocess\n",
        "import statistics\n",
        "from typing import Dict, List, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "from google.colab import files  # Colab-specific import"
      ],
      "metadata": {
        "id": "BTyDl0LyC8eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_aggregate_metrics(results, target_score_name) -> Dict:\n",
        "    if not results:\n",
        "        return {'error': 'No valid results to analyze'}\n",
        "\n",
        "    score_values = [r[target_score_name] for r in results if target_score_name in r]\n",
        "\n",
        "    if not score_values:\n",
        "        return {'error': 'No valid score values found'}\n",
        "\n",
        "    return {\n",
        "        f'mean_{target_score_name}': statistics.mean(score_values),\n",
        "        f'median_{target_score_name}': statistics.median(score_values),\n",
        "        f'min_{target_score_name}': min(score_values),\n",
        "        f'max_{target_score_name}': max(score_values),\n",
        "        f'std_dev': statistics.stdev(score_values) if len(score_values) > 1 else 0,\n",
        "        'total_entries_analyzed': len(score_values)\n",
        "    }"
      ],
      "metadata": {
        "id": "WDF3ZnCLBdLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCoverageAnalyzer:\n",
        "    def __init__(self, input_file: str = \"\", output_dir: str = \"/content/coverage_results\"):\n",
        "        \"\"\"Initialize the analyzer with input file path and output directory.\"\"\"\n",
        "        self.input_file = input_file\n",
        "        self.output_dir = output_dir\n",
        "        self.coverage_results = []\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def create_test_files(self, solution: str, tests: str, temp_dir: str) -> Tuple[str, str]:\n",
        "        \"\"\"Create temporary Python files for the solution and tests.\"\"\"\n",
        "        # Create solution file\n",
        "        solution_file = Path(temp_dir) / \"solution.py\"\n",
        "        with open(solution_file, 'w') as f:\n",
        "            f.write(solution)\n",
        "\n",
        "        # Create test file with proper imports for Colab\n",
        "        test_file = Path(temp_dir) / \"test_solution.py\"\n",
        "        with open(test_file, 'w') as f:\n",
        "            f.write(\"import sys\\n\")\n",
        "            f.write(f\"sys.path.append('{temp_dir}')\\n\")\n",
        "            f.write(\"from solution import *\\n\")\n",
        "            f.write(tests)\n",
        "\n",
        "        return str(solution_file), str(test_file)\n",
        "\n",
        "    def get_coverage_data():\n",
        "        with open('coverage.json') as f:\n",
        "            coverage_data = json.load(f)\n",
        "            for file_path, file_data in coverage_data['files'].items():\n",
        "                  if 'solution.py' in file_path:\n",
        "                     return {\n",
        "                        'line_coverage': file_data['summary']['percent_covered'],\n",
        "                        'total_lines': file_data['summary']['num_statements'],\n",
        "                        'covered_lines': file_data['summary']['covered_lines'],\n",
        "                        'missing_lines': file_data['summary']['missing_lines']\n",
        "                     }\n",
        "    def run_coverage_analysis(self, solution_file: str, test_file: str, temp_dir: str) -> Dict:\n",
        "        \"\"\"Run pytest with coverage and return results.\"\"\"\n",
        "        try:\n",
        "            orig_dir = os.getcwd()\n",
        "            os.chdir(temp_dir)\n",
        "\n",
        "            # Run pytest with coverage using python -m to ensure proper module resolution\n",
        "            cmd = ['python3', '-m', 'pytest', '--cov=solution',\n",
        "                '--cov-report=json', 'test_solution.py', '-v']\n",
        "\n",
        "            env = os.environ.copy()\n",
        "            env['PYTHONPATH'] = temp_dir  # Ensure proper module resolution\n",
        "\n",
        "            result = subprocess.run(cmd, capture_output=True, text=True, env=env)\n",
        "            if os.path.exists('coverage.json'): return get_coverage_data()\n",
        "            return {'error': 'No coverage data generated'}\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Command output: {e.output}\")\n",
        "            return {'error': f'pytest failed: {str(e)}'}\n",
        "        except Exception as e:\n",
        "            print(f\"Exception details: {str(e)}\")\n",
        "            return {'error': f'Analysis failed: {str(e)}'}\n",
        "        finally:\n",
        "            os.chdir(orig_dir)\n",
        "\n",
        "    def analyze_all_entries(self) -> Dict:\n",
        "        with open(self.input_file, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        entries = content.split('CANONICAL SOLUTION:')[1:]  # Skip first empty split\n",
        "\n",
        "        for i, entry in enumerate(entries):\n",
        "            try:\n",
        "                # Add back the header since we split on it\n",
        "                entry = 'CANONICAL SOLUTION:' + entry\n",
        "\n",
        "                with tempfile.TemporaryDirectory() as temp_dir:\n",
        "                    solution, tests = extract_sections(entry)\n",
        "                    if not tests.strip():\n",
        "                        continue\n",
        "\n",
        "                    solution_file, test_file = self.create_test_files(solution, tests, temp_dir)\n",
        "\n",
        "                    result = self.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "                    print(result)\n",
        "                    if 'line_coverage' in result:\n",
        "                        self.coverage_results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing entry {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return calculate_aggregate_metrics(self.coverage_results, \"line_coverage\")"
      ],
      "metadata": {
        "id": "B8o_-_5imP1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coverage_analyzer = TestCoverageAnalyzer()\n",
        "deep_seek_coverage_results = []\n",
        "for index, test_suite in enumerate(extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    solution_file, test_file = coverage_analyzer.create_test_files(solution, test_suite, temp_dir)\n",
        "    result = coverage_analyzer.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "    if 'line_coverage' in result:\n",
        "      deep_seek_coverage_results.append(result)\n",
        "print(calculate_aggregate_metrics(deep_seek_coverage_results, \"line_coverage\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "332LRk76v3tX",
        "outputId": "616255b0-e6f0-4dd1-c546-4ebf6837880f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_line_coverage': 97.34693877551021, 'median_line_coverage': 100.0, 'min_line_coverage': 21.428571428571427, 'max_line_coverage': 100.0, 'std_dev': 13.428673596709753, 'total_entries_analyzed': 35}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_coverage_results = []\n",
        "for index, test_suite in enumerate(semcoder_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    solution_file, test_file = coverage_analyzer.create_test_files(solution, test_suite, temp_dir)\n",
        "    result = coverage_analyzer.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "    if 'line_coverage' in result:\n",
        "      semcoder_coverage_results.append(result)\n",
        "print(calculate_aggregate_metrics(semcoder_coverage_results, \"line_coverage\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_5bWgW1UYN",
        "outputId": "8bce61a6-94de-4de7-ff40-a3fc97cc00a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_line_coverage': 96.75324675324676, 'median_line_coverage': 100.0, 'min_line_coverage': 21.428571428571427, 'max_line_coverage': 100.0, 'std_dev': 14.40695307294402, 'total_entries_analyzed': 33}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measuring Novelty and Diversity"
      ],
      "metadata": {
        "id": "J2jYSvx_8ZPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring with LLM as Judge"
      ],
      "metadata": {
        "id": "hCC-fAzp8feE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "wnszaEDm_-Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "import json\n",
        "from google.colab import userdata\n",
        "def analyze_novelty_with_claude(source_function: str, generated_tests: str, original_tests: str = None) -> dict:\n",
        "    \"\"\"Use Claude API to analyze test novelty.\"\"\"\n",
        "\n",
        "    anthropic = Anthropic(api_key=userdata.get('ANTHROPIC_API_KEY'))\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "As an expert test engineer, analyze the semantic novelty and diversity of the generated test cases for the given function. Consider the function's purpose, edge cases, and expected behaviors.\n",
        "\n",
        "Source Function:\n",
        "\n",
        "{source_function}\n",
        "\n",
        "\n",
        "Generated Test Suite:\n",
        "\n",
        "{generated_tests}\n",
        "\n",
        "Original Test Suite:\n",
        "\n",
        "{original_tests}\n",
        "\n",
        "Please analyze:\n",
        "1. How well do the tests cover different aspects of the function's behavior?\n",
        "2. What novel testing scenarios are introduced?\n",
        "3. Are there important edge cases or boundary conditions tested?\n",
        "4. How diverse are the test inputs and scenarios?\n",
        "5. Are the tests relevant to the function's purpose?\n",
        "\n",
        "Provide your analysis in the following JSON format:\n",
        "{{\n",
        "    \"novelty_score\": <float between 0.0 and 1.0>,\n",
        "    \"novel_aspects\": [<list of strings describing novel aspects>],\n",
        "    \"unique_scenarios\": [<list of strings describing unique test scenarios>],\n",
        "    \"coverage_assessment\": <string describing overall test coverage>,\n",
        "    \"recommendations\": [<list of strings with suggested additional test cases>]\n",
        "}}\n",
        "Do not provide any other additonal text other than the JSON in order to facilitate\n",
        "text processing.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    message = anthropic.messages.create(\n",
        "        model=\"claude-3-sonnet-20240229\",\n",
        "        max_tokens=4096,\n",
        "        temperature=0,  # Use 0 for consistent analysis\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Parse the response as JSON\n",
        "        analysis = json.loads(message.content[0].text)\n",
        "        return analysis\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Failed to parse Claude's response as JSON\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ZCBXGynx8R20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_seek_novelty_results = []\n",
        "for index, test_suite in enumerate(extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  original_tests = dataset['test'][\"test\"][index]\n",
        "  result = analyze_novelty_with_claude(solution, test_suite, original_tests)\n",
        "  print(result)\n",
        "  deep_seek_novelty_results.append(result)\n",
        "print(calculate_aggregate_metrics(deep_seek_novelty_results, \"novelty_score\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKhY4GiQ31oq",
        "outputId": "2cb612dd-beaa-405e-b7d2-ba47aef27ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance edge case'], 'unique_scenarios': ['Passing None as input', 'Large list with close elements'], 'coverage_assessment': 'The generated test suite covers some important aspects like error handling and performance edge cases, but lacks comprehensive coverage of boundary conditions and diverse input scenarios.', 'recommendations': ['Test with empty list', 'Test with list containing duplicate values', 'Test with list containing negative numbers', 'Test with threshold values at or near 0', 'Test with large threshold values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (passing None as input)', 'Tests for performance (large input string)'], 'unique_scenarios': ['Empty string input', 'Nested parentheses within a group', 'Consecutive groups with no spaces', 'Single group with no spaces'], 'coverage_assessment': 'The tests cover a good range of scenarios, including edge cases and error handling. However, there are still some gaps in testing for more complex nested structures and extreme input sizes.', 'recommendations': ['Test cases with deeply nested parentheses groups', 'Test cases with very large input strings to stress performance', 'Test cases with invalid input (e.g., unbalanced parentheses)', 'Test cases with empty input string']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Testing for TypeError when passing None', 'Testing with a large number input'], 'unique_scenarios': ['Passing None to trigger TypeError', 'Testing with a large number input (123.456)'], 'coverage_assessment': 'The generated test suite covers some important aspects like testing for the expected output, testing for edge cases with large numbers, and testing for error handling when passing an invalid input (None). However, it does not cover negative number inputs or zero inputs.', 'recommendations': ['Test with negative number inputs', 'Test with zero input', 'Test with very large and very small number inputs to check for precision issues']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (empty list)'], 'unique_scenarios': ['Empty list', 'List with alternating positive and negative values', 'List with consecutive negative values causing balance to go below zero', 'Passing None as input (error case)'], 'coverage_assessment': 'The generated test suite covers some important aspects like edge cases (empty list), error handling, and scenarios where the balance goes below zero. However, it lacks tests for other edge cases like large positive/negative values, and scenarios where the balance goes below zero and then recovers.', 'recommendations': ['Test cases with large positive and negative values to check for potential integer overflow', 'Test cases where the balance goes below zero and then recovers to positive', 'Test cases with mixed positive, negative, and zero values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (using abs() to check precision)'], 'unique_scenarios': ['Empty list', 'None input', 'Precision checking'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and precision checking, but lack tests for edge cases like lists with negative numbers, zero values, or duplicate values. The original tests provide better coverage of different input scenarios.', 'recommendations': ['Test with lists containing negative numbers', 'Test with lists containing zero values', 'Test with lists containing duplicate values', 'Test with very large or very small numbers to check for overflow/underflow']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for TypeError when passing None as input', 'Tests for edge case of empty list'], 'unique_scenarios': ['Passing None as input', 'Empty list as input', 'List with repeated elements'], 'coverage_assessment': 'The tests cover basic functionality, edge cases, and type errors, but lack comprehensive coverage of boundary conditions and complex input scenarios.', 'recommendations': ['Test with negative delimiter values', 'Test with large lists and extreme values', 'Test with lists containing None or other non-integer values', 'Test with different data types for the delimiter parameter']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for edge case with unbalanced parentheses', 'Tests for error handling with invalid input (None)'], 'unique_scenarios': ['Nested parentheses with varying depths', 'Single group of nested parentheses', 'Empty string input', 'Invalid input (None)'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases and error handling. However, they do not test for other types of invalid inputs (e.g., non-string inputs) or empty groups.', 'recommendations': ['Test with non-string inputs (e.g., integers, lists)', \"Test with empty groups (e.g., '() ()')\", \"Test with mixed valid and invalid groups (e.g., '() (()() ()')\"]}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (large input lists)'], 'unique_scenarios': ['Passing None as input', 'Large input lists with varying substring occurrences'], 'coverage_assessment': 'The tests cover basic functionality, edge cases (empty list), and some performance scenarios, but lack tests for other edge cases like non-string inputs or None substring.', 'recommendations': ['Test with non-string inputs (e.g., integers, lists) to ensure proper error handling', 'Test with None as substring', 'Test with strings containing special characters or whitespace', 'Test with very large input lists to further stress performance']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for empty list input', 'Tests for single element list input', 'Tests for list with duplicate elements'], 'unique_scenarios': ['Empty list', 'List with single element', 'List with duplicate elements', 'List with zero element', 'List with positive and negative elements'], 'coverage_assessment': 'The test suite covers most of the basic scenarios, including edge cases like empty lists and lists with a single element. However, it lacks tests for negative numbers and larger input sizes.', 'recommendations': ['Add tests for lists containing negative numbers', 'Add tests for larger input sizes to check for performance and edge cases', 'Consider adding tests for different data types (e.g., floats, strings) to ensure robustness']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with large input lists', 'Tests for error handling with invalid input types', 'Tests for edge cases with all equal elements', 'Tests for edge cases with negative numbers'], 'unique_scenarios': ['Large input list with 1 million elements', 'Input list with all elements equal to 1', 'Input list with negative numbers', 'Input list with invalid types (None, string, list)'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including performance, edge cases, and error handling. It provides good coverage of the function's behavior.\", 'recommendations': ['Test with an empty list to ensure correct handling of edge case', 'Test with a list containing a mix of positive and negative numbers', 'Test with a list containing duplicate elements']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for performance with large input string', 'Tests for error handling with invalid input (None)', 'Separate test cases for edge cases'], 'unique_scenarios': ['Empty string input', 'Single character input', 'Palindrome input', 'Non-palindrome input', 'Large input string', 'Invalid input (None)'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases, performance, and error handling. However, it lacks tests for other types of invalid inputs (e.g., non-string inputs) and more complex palindrome cases.', 'recommendations': ['Add tests for non-string inputs (e.g., integers, lists, etc.)', 'Add tests for palindromes with mixed cases and special characters', 'Add tests for palindromes with leading/trailing whitespace']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Performance test case', 'Error handling test case'], 'unique_scenarios': ['Large input strings', 'Non-string inputs', 'Empty string inputs'], 'coverage_assessment': 'The generated test suite covers some important aspects like performance, error handling, and edge cases like empty strings. However, it lacks tests for other edge cases like strings of different lengths and more diverse input combinations.', 'recommendations': ['Test cases with strings of different lengths', 'Test cases with more diverse input combinations', 'Test cases for corner cases like extremely large inputs or inputs with non-binary characters']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests performance by measuring execution time', 'Tests error handling by passing invalid input (None)'], 'unique_scenarios': ['Empty list input', 'List with strings of different lengths', 'List with strings of same length', 'Performance test', 'Error handling test'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases like empty lists and strings of different lengths. The performance and error handling tests add additional coverage for non-functional requirements.', 'recommendations': ['Test with very large input lists to check for performance degradation', 'Test with non-string input types to further validate error handling', 'Test with strings containing Unicode characters or special symbols']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling with invalid inputs (None, strings)', 'Tests for edge cases with 0 and 1 as inputs'], 'unique_scenarios': ['Testing with None inputs', 'Testing with string inputs', 'Testing with 0 and 1 as inputs'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including typical cases, edge cases, and error handling. However, some additional tests for larger input values and negative numbers could further improve coverage.', 'recommendations': ['Add tests with larger input values to ensure correctness for larger numbers', 'Add tests with negative input values to ensure correct handling of negative numbers', 'Consider adding tests with floating-point inputs to ensure correct handling of non-integer inputs']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing for performance with an empty string', 'Testing for error handling with invalid input (None)'], 'unique_scenarios': ['Empty string', 'String with repeated characters', 'Invalid input (None)'], 'coverage_assessment': 'The generated test suite covers some important aspects like empty string, repeated characters, and invalid input handling. However, it lacks tests for other edge cases like very long strings or strings with special characters.', 'recommendations': ['Test with very long strings to check for performance and edge cases', 'Test with strings containing special characters or non-ASCII characters', 'Test with different data types as input (e.g., integers, lists) to ensure proper error handling']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests a large input value (10000)', 'Tests additional edge cases (1, 10)'], 'unique_scenarios': ['Large input value (10000)', 'Small input values (1, 10)', 'Zero input value (0)'], 'coverage_assessment': 'The generated test suite covers a good range of input values, including edge cases like 0, 1, and 10, as well as a large input value of 10000. However, it does not test negative input values or other potential edge cases.', 'recommendations': [\"Test negative input values to ensure the function handles them correctly (e.g. assert string_sequence(-1) == '')\", 'Test very large input values beyond 10000 to ensure the function can handle extremely large inputs without running out of memory or taking too long to execute', 'Test input values that are close to the maximum integer value supported by the system to ensure the function handles potential integer overflow correctly']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Performance testing', 'Error handling'], 'unique_scenarios': ['Empty string', 'String with repeated characters', 'Handling None input'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases like empty strings and strings with repeated characters. They also introduce performance testing and error handling, which are important aspects not covered in the original test suite.', 'recommendations': ['Test with non-string inputs (e.g., integers, lists)', 'Test with strings containing non-alphabetic characters (e.g., punctuation, whitespace)', 'Test with very large strings to check performance']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for empty input'], 'unique_scenarios': ['Empty input', 'TypeError for invalid input', 'Edge case with mixed note types'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and edge cases, but lack comprehensive coverage of different input scenarios and boundary conditions.', 'recommendations': ['Test cases with very long input strings to check for performance and edge cases', 'Test cases with invalid note types or unexpected characters', 'Test cases with mixed note types in different orders and combinations', 'Test cases with boundary conditions like all whole notes, all half notes, all quarter notes']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for edge cases like empty strings and substrings equal to the original string', 'Tests for error handling with invalid inputs (None)', 'Tests for performance with longer strings and substrings'], 'unique_scenarios': ['Empty string and substring', 'Substring not found in string', 'Substring found multiple times with overlap', 'Substring equal to original string', 'Invalid inputs (None)', 'Long strings and substrings'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases, error handling, and performance. However, some additional tests for boundary conditions and more diverse inputs could further improve coverage.', 'recommendations': ['Test with strings containing non-ASCII characters', 'Test with very long strings and substrings (stress testing)', 'Test with substrings at the beginning and end of the string', 'Test with substrings that partially overlap with the string']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling (TypeError for None input)', 'Tests for performance (empty string input)'], 'unique_scenarios': ['Testing with None input to check for error handling', 'Testing with empty string input to check for performance', 'Testing with duplicate numbers in the input string'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases like empty strings and error handling for invalid inputs. However, it lacks tests for some boundary conditions and corner cases.', 'recommendations': [\"Test with inputs containing invalid strings (e.g., 'ten', 'eleven') to ensure proper error handling\", 'Test with inputs containing duplicate numbers to ensure correct sorting behavior', 'Test with inputs containing leading/trailing spaces or extra spaces between numbers', \"Test with inputs containing mixed cases (e.g., 'One', 'TWO') to ensure case-insensitive behavior\"]}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (perf)'], 'unique_scenarios': ['Passing None as input', 'Testing with duplicate values in the list'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and performance, but lack comprehensive coverage of edge cases and boundary conditions.', 'recommendations': ['Test with empty list input', 'Test with list containing only one element', 'Test with list containing negative numbers', 'Test with list containing non-numeric values', 'Test with very large or very small numbers to check for precision issues']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests with negative numbers', 'Tests with duplicate numbers'], 'unique_scenarios': ['Rescaling a list with all distinct numbers', 'Rescaling a list with duplicate numbers', 'Rescaling a list with negative numbers'], 'coverage_assessment': 'The tests cover the basic functionality of the rescale_to_unit function, including rescaling lists with distinct numbers and lists with duplicate numbers. However, there are no tests for edge cases such as empty lists or lists with only one element.', 'recommendations': ['Add tests for empty lists and lists with only one element', 'Add tests with floating-point numbers with different precisions', 'Add tests with very large or very small numbers to check for potential overflow or underflow issues']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Performance testing', 'Error handling for invalid input (None)'], 'unique_scenarios': ['Empty list input', 'List with non-integer values', 'List with repeated integer values', 'Invalid input (None)'], 'coverage_assessment': 'The tests cover basic functionality, edge cases, and error handling, but lack comprehensive coverage of boundary conditions and complex input scenarios.', 'recommendations': ['Test with large input lists to assess performance', 'Test with lists containing different data types (e.g., nested lists, tuples, sets)', 'Test with edge cases like negative integers, zero, and maximum/minimum integer values', 'Test with input lists containing both integers and non-integers in different orders']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Performance testing', 'Error handling for invalid input'], 'unique_scenarios': ['Empty string', 'Long string', 'Non-string input'], 'coverage_assessment': 'The generated tests cover some important aspects like empty strings, long strings, and invalid inputs. However, they lack tests for other edge cases like strings with special characters or Unicode strings.', 'recommendations': ['Test strings with special characters (e.g., punctuation, whitespace)', 'Test Unicode strings', 'Test strings with different character encodings']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for performance', 'Tests for edge cases', 'Tests for error handling'], 'unique_scenarios': ['Testing with None input', 'Testing with prime number input', 'Testing with large input'], 'coverage_assessment': \"The generated tests cover some important aspects like edge cases, error handling, and performance, but lack comprehensive coverage of the function's behavior.\", 'recommendations': ['Test with negative inputs', 'Test with non-integer inputs', 'Test with inputs that have multiple divisors', 'Test with inputs that have no divisors other than 1 and itself']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for edge cases like 0, 1, and negative numbers', 'Tests for large input values', 'Tests for invalid input types like None and strings'], 'unique_scenarios': ['Testing with randomly generated large input values', 'Testing with invalid input types', 'Testing with negative input values'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases, large inputs, and invalid inputs. However, it lacks tests for some specific scenarios like prime numbers and numbers with repeated factors.', 'recommendations': ['Add tests for prime numbers as input', 'Add tests for numbers with repeated factors (e.g., 12 = 2 * 2 * 3)', 'Add tests for very large input values (e.g., larger than 10^5)']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (empty list)'], 'unique_scenarios': ['Empty list', 'List with duplicates', 'List without duplicates', 'Invalid input (None)'], 'coverage_assessment': 'The tests cover basic functionality, edge cases (empty list), and error handling (invalid input). However, they lack tests for larger inputs and more complex scenarios.', 'recommendations': ['Test with larger input lists', 'Test with lists containing negative numbers', 'Test with lists containing duplicate values at the beginning, middle, and end', 'Test with lists containing consecutive duplicate values']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Testing for TypeError when passing None as input', 'Testing for empty string input'], 'unique_scenarios': ['Passing None as input to test for TypeError', 'Passing empty string as input', 'Passing a sentence with mixed cases as input', 'Passing a string with special characters as input'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases like empty strings and error handling for invalid inputs. It also tests different types of inputs like sentences and strings with special characters.', 'recommendations': ['Test with non-string inputs like integers or lists to ensure proper error handling', 'Test with Unicode characters or non-ASCII strings', 'Test with extremely long strings to check for performance issues']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for large input', 'Tests for strings with spaces and special characters'], 'unique_scenarios': ['Empty list input', 'Single string input', 'Multiple string input', 'Large input with repeated strings', 'Strings with spaces', 'Strings with special characters'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases like empty and single-element lists, as well as larger inputs and inputs with special characters. However, some additional edge cases could be tested.', 'recommendations': ['Test with lists containing None or other non-string values', 'Test with nested lists or other non-list iterables', 'Test with Unicode strings or strings with different encodings']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Performance testing with a large input list', 'Testing for TypeError when input is None', 'Testing with a mix of uppercase and lowercase strings'], 'unique_scenarios': ['Empty input list', 'Large input list for performance testing', 'Input list with mixed casing', 'Input is None (invalid input)'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases like empty lists, large inputs for performance testing, and invalid inputs. However, some additional cases like testing with different prefix lengths or testing with non-string inputs could further improve coverage.', 'recommendations': ['Test with prefixes of different lengths (e.g., empty prefix, single character prefix, long prefix)', 'Test with non-string inputs (e.g., integers, lists, None) for the prefix parameter', 'Test with non-string inputs (e.g., integers, lists, None) for the strings parameter']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for performance', 'Tests for error handling'], 'unique_scenarios': ['Empty list input', 'Invalid input type (None)'], 'coverage_assessment': 'The generated test suite covers some important aspects like edge cases (empty list) and error handling (invalid input type), but lacks diversity in positive test cases and does not cover boundary conditions.', 'recommendations': ['Test cases with lists containing only positive numbers', 'Test cases with lists containing only negative numbers', 'Test cases with lists containing both positive and negative numbers, including 0', 'Test cases with large lists to check performance', 'Test cases with boundary values like maximum and minimum integers']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for negative numbers', 'Tests for 0 as input', 'Tests for products of prime and non-prime numbers', 'Tests for products of two prime numbers'], 'unique_scenarios': ['Testing negative numbers', 'Testing 0 as input', 'Testing products of prime and non-prime numbers', 'Testing products of two prime numbers'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases like negative numbers and 0, as well as more complex cases involving products of prime and non-prime numbers. However, it lacks tests for larger prime numbers and does not cover all possible edge cases.', 'recommendations': ['Add tests for larger prime numbers (e.g., numbers with more than 10 digits)', 'Test cases for numbers close to the maximum integer value', 'Test cases for numbers close to the minimum integer value', 'Test cases for prime numbers with special properties (e.g., Mersenne primes, twin primes)']}\n",
            "{'novelty_score': 0.4, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for edge case with polynomial coefficients'], 'unique_scenarios': ['Polynomial with all coefficients non-zero', 'Polynomial with zero coefficients'], 'coverage_assessment': \"The generated test suite covers some important aspects of the function's behavior, such as testing for edge cases and error handling. However, it lacks diversity in test inputs and scenarios, and does not thoroughly test the function's purpose and expected behavior.\", 'recommendations': ['Test cases with polynomials of varying degrees and coefficient combinations', 'Test cases with polynomials having multiple roots or no roots', 'Test cases with large and small coefficient values to check for numerical stability', 'Test cases with non-integer coefficients', 'Test cases with empty or invalid input lists']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (comparing against itself)'], 'unique_scenarios': ['Passing None as input to test error handling', 'Passing the same list to test performance'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and performance, but lack comprehensive coverage of edge cases and boundary conditions.', 'recommendations': ['Test with empty lists', 'Test with lists containing duplicate values', 'Test with very large lists to check for performance issues', 'Test with lists containing non-integer values']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for empty list input', 'Tests for single element list input', 'Tests for list with all elements being the same', 'Tests for invalid input (None instead of list)'], 'unique_scenarios': ['Empty list input', 'Single element list input', 'List with all elements being the same', 'Invalid input (None instead of list)', 'Typical use case with duplicate and unique elements'], 'coverage_assessment': \"The generated test suite covers a good range of scenarios, including typical use cases, edge cases, and error handling. It tests the function's behavior with empty lists, single-element lists, lists with all elements being the same, and invalid inputs.\", 'recommendations': ['Test with lists containing different data types (e.g., strings, floats, mixed types)', 'Test with very large lists to check performance', 'Test with nested lists or other non-list iterables']}\n",
            "{'mean_novelty_score': 0.66, 'median_novelty_score': 0.7, 'min_novelty_score': 0.4, 'max_novelty_score': 0.8, 'std_dev': 0.08116794499134279, 'total_entries_analyzed': 35}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_aggregate_metrics(deep_seek_novelty_results[:34], \"novelty_score\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y3so-aA8DGJ",
        "outputId": "67cf5d25-3f89-4658-b489-11a6bc78fad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mean_novelty_score': 0.6558823529411765, 'median_novelty_score': 0.7, 'min_novelty_score': 0.4, 'max_novelty_score': 0.8, 'std_dev': 0.07859052479933758, 'total_entries_analyzed': 34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_novelty_results = []\n",
        "for index, test_suite in enumerate(semcoder_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  original_tests = dataset['test'][\"test\"][index]\n",
        "  result = analyze_novelty_with_claude(solution, test_suite, original_tests)\n",
        "  semcoder_novelty_results.append(result)\n",
        "  print(result)\n",
        "print(calculate_aggregate_metrics(semcoder_novelty_results, \"novelty_score\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrlQnQ5D35nt",
        "outputId": "9662cde4-0002-4c8c-edb4-e2362252d716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance edge case'], 'unique_scenarios': ['Passing None as input', 'Large list with close elements'], 'coverage_assessment': 'The generated test suite covers some important aspects like error handling and performance edge cases, but lacks comprehensive coverage of boundary conditions and diverse input scenarios.', 'recommendations': ['Test with empty list', 'Test with list containing duplicate elements', 'Test with list containing negative numbers', 'Test with threshold values at or near 0', 'Test with large threshold values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (passing None as input)', 'Tests for performance (large input string)'], 'unique_scenarios': ['Empty string input', 'Nested parentheses', 'Single group of parentheses', 'Multiple groups of parentheses', 'Unbalanced parentheses (not tested)'], 'coverage_assessment': 'The tests cover a good range of scenarios, including edge cases like empty strings and single groups of parentheses. However, they do not test for unbalanced parentheses, which is an important edge case.', 'recommendations': [\"Add tests for unbalanced parentheses (e.g., '(())', '()()', '(((())')\", 'Add tests for strings with non-parenthesis characters', 'Add tests for very large input strings to further test performance']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Performance testing', 'Error handling for invalid input'], 'unique_scenarios': ['Testing with a large number', 'Testing with None input'], 'coverage_assessment': 'The generated tests cover some important aspects like edge cases, error handling, and performance, but lack tests for negative numbers and zero.', 'recommendations': ['Test with negative numbers', 'Test with zero input', 'Test with very large and very small numbers to check for precision issues']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (empty list)'], 'unique_scenarios': ['Empty list', 'List with alternating positive and negative values', 'List with consecutive negative values causing balance to go below zero', 'Passing None as input (error case)'], 'coverage_assessment': 'The generated test suite covers some important aspects like edge cases (empty list), error handling, and scenarios where the balance goes below zero. However, it lacks tests for other edge cases like large positive/negative values, and scenarios where the balance goes below zero and then recovers.', 'recommendations': ['Test cases with large positive and negative values to check for potential integer overflow', 'Test cases where the balance goes below zero and then recovers to positive', 'Test cases with mixed positive, negative, and zero values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (using abs() to check precision)'], 'unique_scenarios': ['Empty list', 'None input', 'Precision checking'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and precision checking, but lack tests for edge cases like lists with negative numbers, zero values, or duplicate values. The original tests provide better coverage of different input scenarios.', 'recommendations': ['Test with lists containing negative numbers', 'Test with lists containing zero values', 'Test with lists containing duplicate values', 'Test with very large or very small numbers to check for overflow/underflow']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for TypeError when passing None as input', 'Tests for edge case with all elements being the same as the delimiter'], 'unique_scenarios': ['Passing None as input', 'All elements being the same as the delimiter'], 'coverage_assessment': 'The generated tests cover some important edge cases and error scenarios, but lack diversity in input data and do not test for other potential edge cases or boundary conditions.', 'recommendations': ['Test with empty lists and non-integer inputs', 'Test with negative delimiters', 'Test with large lists and extreme values', 'Test with different data types for the delimiter']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for edge case with unbalanced parentheses', 'Tests for error handling with invalid input (None)'], 'unique_scenarios': ['Nested parentheses with varying depths', 'Single group of nested parentheses', 'Empty string input', 'Invalid input (None)'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases and error handling. However, they do not test for other types of invalid inputs (e.g., non-string inputs) or empty groups.', 'recommendations': ['Test with non-string inputs (e.g., integers, lists)', \"Test with empty groups (e.g., '() ()')\", \"Test with mixed valid and invalid groups (e.g., '() (()() ()')\"]}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing for TypeError when passing None as input', 'Testing for empty input list'], 'unique_scenarios': ['Passing None as input', 'Empty input list', 'Input list with substring present', 'Input list with substring not present'], 'coverage_assessment': 'The generated test suite covers some important aspects like handling empty input, substring presence/absence, and type errors. However, it lacks tests for edge cases like substring at the start/end of strings, case sensitivity, and handling non-string inputs.', 'recommendations': ['Test case with substring at the start of a string', 'Test case with substring at the end of a string', 'Test case with mixed case strings', 'Test case with non-string inputs in the list']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for TypeError when passing None as input', 'Tests for performance with large input'], 'unique_scenarios': ['Passing None as input', 'Testing with large input for performance'], 'coverage_assessment': 'The generated tests cover some important aspects like empty input, single element input, and type error handling. However, they lack tests for negative numbers, zero values, and more diverse input scenarios.', 'recommendations': ['Test with negative numbers', 'Test with zero values', 'Test with mixed positive and negative numbers', 'Test with duplicate values', 'Test with large input sizes for edge cases']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing for performance with an empty list', 'Testing for error handling with invalid input (None)'], 'unique_scenarios': ['Empty list', 'List with decreasing values', 'List with repeated values', 'List with large values', 'Invalid input (None)'], 'coverage_assessment': 'The generated tests cover some important aspects like empty lists, decreasing values, repeated values, and large values. However, they do not cover some edge cases like negative numbers or lists with a single element.', 'recommendations': ['Test with lists containing negative numbers', 'Test with lists containing a single element', 'Test with lists containing duplicate values', 'Test with extremely large or small integer values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Performance testing', 'Error handling'], 'unique_scenarios': ['Empty string', 'Non-string input', 'Palindrome with even length', 'Palindrome with odd length', 'Non-palindrome string'], 'coverage_assessment': 'The generated tests cover some important aspects like edge cases, error handling, and performance, but lack comprehensive coverage of different input scenarios and boundary conditions.', 'recommendations': ['Test cases with strings containing special characters or whitespace', 'Test cases with very long strings to check for performance edge cases', 'Test cases with Unicode strings', 'Test cases with strings containing repeated characters']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Performance test case', 'Error handling test case'], 'unique_scenarios': ['Testing with longer input strings', 'Testing with None input'], 'coverage_assessment': 'The generated test suite covers some important aspects like edge cases, error handling, and performance, but lacks diversity in input scenarios and does not cover all boundary conditions.', 'recommendations': ['Test cases with empty strings as input', 'Test cases with strings of different lengths', 'Test cases with non-binary characters in input strings']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (empty list)'], 'unique_scenarios': ['Handling None input', 'Testing with large strings', 'Raising TypeError'], 'coverage_assessment': 'The generated tests cover some important aspects like empty input, large inputs, and error handling. However, they lack tests for some edge cases like lists with duplicate strings of the same length.', 'recommendations': ['Test case with list containing duplicate strings of the same length', 'Test case with list containing only one string', 'Test case with list containing strings of varying lengths but no clear longest string']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (perf)'], 'unique_scenarios': ['Testing with None input', 'Testing with large input values'], 'coverage_assessment': 'The generated tests cover some additional aspects like error handling and performance, but lack comprehensive coverage of edge cases and boundary conditions.', 'recommendations': ['Test with negative input values', 'Test with zero input values', 'Test with input values that have a common factor other than 1', 'Test with input values that are very large or very small']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing for performance with an empty string', 'Testing for error handling with invalid input (None)'], 'unique_scenarios': ['Empty string', 'String with repeated characters', 'Invalid input (None)'], 'coverage_assessment': 'The generated test suite covers some important aspects like empty string, repeated characters, and invalid input handling. However, it lacks tests for other edge cases like very long strings or strings with special characters.', 'recommendations': ['Test with very long strings to check for performance and edge cases', 'Test with strings containing special characters or non-ASCII characters', 'Test with different data types as input (e.g., integers, lists) to ensure proper error handling']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for performance/scalability with a larger input', 'Tests for error handling with an invalid input type'], 'unique_scenarios': ['Testing with a large input value (10)', 'Testing with an invalid input type (None)'], 'coverage_assessment': 'The generated tests cover some important aspects like edge cases, error handling, and performance, but they lack tests for negative inputs and other boundary conditions.', 'recommendations': ['Test with negative input values', 'Test with non-integer input values', 'Test with very large input values to check for potential integer overflow']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for performance (empty string)', 'Tests for error handling (passing None)'], 'unique_scenarios': ['Empty string', 'String with repeated characters', 'Passing None as input'], 'coverage_assessment': \"The generated tests cover some important aspects like empty strings, repeated characters, and error handling. However, they lack tests for different character cases (uppercase, lowercase, mixed) and do not test the function's behavior with non-string inputs.\", 'recommendations': ['Test with strings containing only uppercase characters', 'Test with strings containing only lowercase characters', 'Test with strings containing a mix of uppercase and lowercase characters', 'Test with non-string inputs like integers or lists']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for empty input'], 'unique_scenarios': ['Empty input', 'TypeError for invalid input', 'Edge case with mixed note types'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and edge cases, but lack comprehensive coverage of different input scenarios and boundary conditions.', 'recommendations': ['Test cases with very long input strings to check for performance and edge cases', 'Test cases with invalid note types or unexpected characters', 'Test cases with mixed note types in different orders and combinations', 'Test cases with boundary conditions like all whole notes, all half notes, all quarter notes']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing with None input', 'Testing for TypeError exception'], 'unique_scenarios': ['Empty string with non-empty substring', 'Substring appearing multiple times', 'Substring not appearing in string', 'Substring appearing once'], 'coverage_assessment': 'The tests cover some important aspects like empty strings, overlapping substrings, and substrings not appearing in the string. However, they lack tests for edge cases like very long strings or substrings, and do not cover cases where the substring is longer than the string.', 'recommendations': ['Test with very long strings and substrings', 'Test with substring longer than the string', 'Test with non-string inputs like numbers or lists', 'Test with Unicode strings containing non-ASCII characters']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for performance (empty string input)', 'Tests for error handling (passing None as input)'], 'unique_scenarios': ['Empty string input', 'Sorted input', 'None input (error case)'], 'coverage_assessment': 'The generated tests cover some important aspects like empty input, sorted input, and error handling. However, they lack tests for inputs with duplicate numbers or invalid inputs (non-numeric strings).', 'recommendations': [\"Test case with duplicate numbers (e.g., 'one two two three')\", \"Test case with invalid input (e.g., 'one hello three')\", \"Test case with mixed case input (e.g., 'One two THREE')\"]}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance edge case'], 'unique_scenarios': ['Passing None as input', 'Testing with a list of floats with a large difference between closest elements'], 'coverage_assessment': 'The generated test suite covers some important aspects like error handling and edge cases, but lacks diversity in input data and does not test for boundary conditions like empty lists or lists with only one element.', 'recommendations': ['Test with an empty list as input', 'Test with a list containing only one element', 'Test with a list containing duplicate elements', 'Test with a list containing negative numbers', 'Test with a list containing non-numeric values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance edge case with only two elements'], 'unique_scenarios': ['Handling None input', 'Rescaling with only two numbers', 'Rescaling with duplicate numbers'], 'coverage_assessment': 'The generated test suite covers some important aspects like error handling and edge cases with only two elements. However, it lacks tests for other edge cases like lists with negative numbers, lists with only one element, or lists with duplicate numbers.', 'recommendations': ['Test with lists containing negative numbers', 'Test with lists containing only one element', 'Test with lists containing duplicate numbers', 'Test with lists containing non-numeric values']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (empty list)'], 'unique_scenarios': ['Handling None input', 'Empty list input', 'List with non-integer values'], 'coverage_assessment': 'The tests cover basic functionality, edge cases, and error handling, but lack comprehensive coverage of boundary conditions and complex input scenarios.', 'recommendations': ['Test with large lists to assess performance', 'Test with nested lists or other complex data structures', 'Test with integer values at the limits of the int data type', 'Test with non-list iterables as input']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Performance testing', 'Error handling for invalid input'], 'unique_scenarios': ['Empty string', 'Long string', 'Non-string input'], 'coverage_assessment': 'The generated tests cover some important aspects like empty strings, long strings, and invalid inputs. However, they lack tests for other edge cases like strings with special characters or Unicode strings.', 'recommendations': ['Test strings with special characters (e.g., punctuation, whitespace)', 'Test Unicode strings', 'Test strings with different character encodings']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (test_largest_divisor_perf)'], 'unique_scenarios': ['Testing with None input', 'Testing with prime number input (7)'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and edge cases (prime numbers), but lack comprehensive coverage of different input ranges and boundary conditions.', 'recommendations': ['Test cases for negative inputs', 'Test cases for inputs with multiple large divisors (e.g., 24)', 'Test cases for boundary conditions like 0 and 1', 'Test cases for large inputs to check for performance and edge cases']}\n",
            "{'novelty_score': 0.4, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (factorize_perf)'], 'unique_scenarios': ['Testing with None input', 'Testing with large numbers'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and performance, but lack comprehensive coverage of edge cases and boundary conditions.', 'recommendations': ['Test with negative numbers', 'Test with 1 and 0 as inputs', 'Test with prime numbers', 'Test with numbers with repeated prime factors', 'Test with very large numbers to check for potential integer overflow']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (empty list)'], 'unique_scenarios': ['Empty list input', 'List with duplicates', 'List without duplicates', 'Invalid input (None)'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases like empty lists and invalid inputs. However, they do not test for other potential edge cases like lists with negative numbers or large lists.', 'recommendations': ['Test with lists containing negative numbers', 'Test with very large lists to check for performance issues', 'Test with lists containing duplicate values at the beginning, middle, and end']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing for TypeError when passing None as input', 'Testing performance with an empty string'], 'unique_scenarios': ['Passing None as input', 'Passing an empty string', 'Passing a long string with mixed cases'], 'coverage_assessment': 'The generated tests cover some important aspects like handling empty strings, long strings, and type errors. However, they do not cover some basic cases like strings with only uppercase or lowercase characters.', 'recommendations': ['Add test cases for strings with only uppercase characters', 'Add test cases for strings with only lowercase characters', 'Add test cases for strings with special characters or non-alphabetic characters']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Testing for TypeError when passing None', 'Testing with a larger list of strings'], 'unique_scenarios': ['Passing None to the function', 'Passing a list with 5 string elements'], 'coverage_assessment': 'The tests cover the basic functionality of concatenating lists of strings, as well as an important edge case of passing None. However, they do not cover other potential edge cases or boundary conditions.', 'recommendations': ['Test with an empty string in the list', 'Test with non-string types in the list', 'Test with a large number of strings in the list', 'Test with strings containing special characters or whitespace']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Testing for TypeError when passing None as input', 'Testing performance with an empty list'], 'unique_scenarios': ['Passing None as input', 'Passing an empty list', \"Testing with a mix of strings that start with and don't start with the prefix\"], 'coverage_assessment': 'The generated tests cover some important aspects like error handling, edge cases (empty list), and a mix of valid and invalid inputs. However, they do not cover some other edge cases like passing non-string inputs or testing with different prefix lengths.', 'recommendations': ['Test with non-string inputs (e.g., integers, lists) to ensure proper error handling', 'Test with prefixes of different lengths (e.g., empty string, single character, long prefix)', \"Test with strings containing the prefix in the middle or end (e.g., 'abcdef' with prefix 'cd')\"]}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for performance', 'Tests for error handling'], 'unique_scenarios': ['Empty list input', 'Invalid input type (None)'], 'coverage_assessment': 'The generated test suite covers some important aspects like edge cases (empty list) and error handling (invalid input type), but lacks diversity in positive test cases and does not cover boundary conditions.', 'recommendations': ['Test cases with lists containing only positive numbers', 'Test cases with lists containing only negative numbers', 'Test cases with lists containing both positive and negative numbers, including 0', 'Test cases with large lists to check performance', 'Test cases with boundary values like maximum and minimum integers']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests performance for a large prime number', 'Tests error handling for invalid input (None)'], 'unique_scenarios': ['Testing a large prime number', 'Testing with None input', 'Testing with a large composite number'], 'coverage_assessment': 'The generated tests cover some novel aspects like performance with large numbers and error handling, but lack comprehensive coverage of edge cases and boundary conditions.', 'recommendations': ['Test with negative numbers', 'Test with non-integer inputs', 'Test with boundary cases like 2 and 3', 'Test with prime numbers close to the square root of the maximum integer value']}\n",
            "{'novelty_score': 0.4, 'novel_aspects': ['Tests for error handling (TypeError)'], 'unique_scenarios': ['Testing for TypeError when passing None as input'], 'coverage_assessment': \"The generated tests cover some aspects of the function's behavior, but lack comprehensive coverage of edge cases and boundary conditions.\", 'recommendations': ['Test cases for polynomials with only one coefficient (constant functions)', 'Test cases for polynomials with all coefficients zero', 'Test cases for polynomials with large coefficients (to check for overflow/underflow)', 'Test cases for polynomials with large degree (to check for performance)', 'Test cases for polynomials with even and odd number of coefficients (to check the requirement of even number of coefficients)', 'Test cases for polynomials with multiple zero points (to check if only one zero point is returned)']}\n",
            "{'novelty_score': 0.6, 'novel_aspects': ['Tests for error handling (TypeError)', 'Tests for performance (comparing against itself)'], 'unique_scenarios': ['Passing None as input to test error handling', 'Passing the same list to test performance'], 'coverage_assessment': 'The generated tests cover some important aspects like error handling and performance, but lack comprehensive coverage of edge cases and boundary conditions.', 'recommendations': ['Test with empty lists', 'Test with lists containing duplicate values', 'Test with very large lists to check for performance issues', 'Test with lists containing non-integer values']}\n",
            "{'mean_novelty_score': 0.6058823529411764, 'median_novelty_score': 0.6, 'min_novelty_score': 0.4, 'max_novelty_score': 0.7, 'std_dev': 0.06485964553201261, 'total_entries_analyzed': 34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write the contents of semcoder_novelty_results and deep_seek_novelty_results to their own respective files that I can then download\n",
        "\n",
        "import json\n",
        "\n",
        "# Assuming deep_seek_novelty_results and semcoder_novelty_results are lists of dictionaries\n",
        "# as produced by your analyze_novelty_with_claude function.\n",
        "\n",
        "\n",
        "def write_results_to_file(results, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "\n",
        "write_results_to_file(deep_seek_novelty_results, 'deep_seek_novelty_results.json')\n",
        "write_results_to_file(semcoder_novelty_results, 'semcoder_novelty_results.json')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('deep_seek_novelty_results.json')\n",
        "files.download('semcoder_novelty_results.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OTQS9FXJ4y20",
        "outputId": "c063ceca-bcff-4b80-aeee-adfe30f344cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c1c14d0e-ef9f-4b16-b82e-8b904e2bbe00\", \"deep_seek_novelty_results.json\", 33777)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79476efe-fc40-411d-9e24-8ba4be834f83\", \"semcoder_novelty_results.json\", 30099)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring with Patterns"
      ],
      "metadata": {
        "id": "pNPwVBMn7ZLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class CoveragePatternAnalyzer:\n",
        "    \"\"\"Analyzes test coverage patterns focusing on types of test cases.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.patterns = {\n",
        "            'edge_cases': {\n",
        "                'empty_input': r'assert.*(?:empty|\\[\\]|\\{\\}|\\(\\)|\"\"|\\'\\'|\\b==\\s*\\[\\]|\\b==\\s*\"\"|\\b==\\s*\\'\\')',\n",
        "                'null_input': r'assert.*(?:None|null)',\n",
        "                'single_element': r'assert.*\\[[^,\\]]+\\]'\n",
        "            },\n",
        "            'boundary_testing': {\n",
        "                'zero_values': r'assert.*\\b0\\b',\n",
        "                'negative_values': r'assert.*-\\d+',\n",
        "                'large_values': r'assert.*\\d{5,}'\n",
        "            },\n",
        "            'error_handling': {\n",
        "                'exception_testing': r'with\\s+pytest\\.raises\\([^)]+\\)',\n",
        "                'invalid_input': r'assert.*(invalid|wrong|incorrect|bad)'\n",
        "            },\n",
        "            'functionality': {\n",
        "                'typical_case': r'assert.*normal|typical|standard',\n",
        "                'complex_input': r'assert.*(?:\\[.*,.*,.*\\]|\\{.*:.*,.*:.*\\}|\\(.*,.*,.*\\))'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _extract_assertions(self, test_code: str) -> List[str]:\n",
        "        \"\"\"Extract assertions with improved handling of multi-line and truncated assertions.\"\"\"\n",
        "        lines = test_code.split('\\n')\n",
        "        assertions = []\n",
        "        current_assertion = None\n",
        "        in_raises_block = False\n",
        "        bracket_count = 0\n",
        "        paren_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Skip empty lines\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Start of pytest.raises block\n",
        "            if 'pytest.raises' in line:\n",
        "                in_raises_block = True\n",
        "                current_assertion = line\n",
        "                paren_count = line.count('(') - line.count(')')\n",
        "                if paren_count == 0:\n",
        "                    assertions.append(current_assertion)\n",
        "                    current_assertion = None\n",
        "                    in_raises_block = False\n",
        "                continue\n",
        "\n",
        "            # Start of regular assertion\n",
        "            if line.startswith('assert'):\n",
        "                current_assertion = line\n",
        "                bracket_count = line.count('[') - line.count(']')\n",
        "                paren_count = line.count('(') - line.count(')')\n",
        "                if bracket_count == 0 and paren_count == 0:\n",
        "                    assertions.append(current_assertion)\n",
        "                    current_assertion = None\n",
        "                continue\n",
        "\n",
        "            # Continue previous assertion\n",
        "            if current_assertion:\n",
        "                current_assertion += ' ' + line\n",
        "                if in_raises_block:\n",
        "                    paren_count += line.count('(') - line.count(')')\n",
        "                    if paren_count == 0:\n",
        "                        assertions.append(current_assertion)\n",
        "                        current_assertion = None\n",
        "                        in_raises_block = False\n",
        "                else:\n",
        "                    bracket_count += line.count('[') - line.count(']')\n",
        "                    paren_count += line.count('(') - line.count(')')\n",
        "                    if bracket_count == 0 and paren_count == 0:\n",
        "                        assertions.append(current_assertion)\n",
        "                        current_assertion = None\n",
        "\n",
        "        # Handle any remaining incomplete assertion\n",
        "        if current_assertion:\n",
        "            assertions.append(current_assertion + ' ...')\n",
        "\n",
        "        return assertions\n",
        "\n",
        "    def analyze_test_suite(self, test_code: str) -> Dict:\n",
        "        \"\"\"Analyze a test suite and return detailed coverage metrics.\"\"\"\n",
        "        assertions = self._extract_assertions(test_code)\n",
        "        for assertion in assertions:\n",
        "          print(assertion)\n",
        "        total_assertions = len(assertions)\n",
        "        if total_assertions == 0:\n",
        "            return {'error': 'No assertions found'}\n",
        "\n",
        "        # Track which patterns match each assertion\n",
        "        assertion_patterns = {i: set() for i in range(total_assertions)}\n",
        "        pattern_counts = defaultdict(lambda: defaultdict(int))\n",
        "        uncategorized_assertions = []\n",
        "\n",
        "        # Analyze each assertion\n",
        "        for i, assertion in enumerate(assertions):\n",
        "            matches_found = False\n",
        "            for category, patterns in self.patterns.items():\n",
        "                for name, pattern in patterns.items():\n",
        "                    if re.search(pattern, assertion):\n",
        "                        assertion_patterns[i].add(f\"{category}:{name}\")\n",
        "                        pattern_counts[category][name] += 1\n",
        "                        matches_found = True\n",
        "\n",
        "            if not matches_found:\n",
        "                uncategorized_assertions.append(assertion)\n",
        "\n",
        "        # Calculate metrics\n",
        "        results = {}\n",
        "        for category, patterns in pattern_counts.items():\n",
        "            category_assertions = len([i for i in assertion_patterns.values()\n",
        "                                    if any(p.startswith(f\"{category}:\") for p in i)])\n",
        "            results[category] = {\n",
        "                'total_matches': category_assertions,\n",
        "                'coverage_ratio': category_assertions / total_assertions,\n",
        "                'pattern_breakdown': dict(patterns)\n",
        "            }\n",
        "\n",
        "        # Add overall metrics\n",
        "        results['overall'] = {\n",
        "            'total_assertions': total_assertions,\n",
        "            'patterns_per_assertion': sum(len(p) for p in assertion_patterns.values()) / total_assertions,\n",
        "            'pattern_coverage': len([p for p in sum([list(p.values()) for p in pattern_counts.values()], []) if p > 0]) / \\\n",
        "                              len(sum([list(p.values()) for p in self.patterns.values()], [])),\n",
        "            'uncategorized': len(uncategorized_assertions),\n",
        "            'uncategorized_assertions': uncategorized_assertions\n",
        "        }\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "GxuebA5STYbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_analyzer = CoveragePatternAnalyzer()"
      ],
      "metadata": {
        "id": "NlySCWNo5cna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_extracted_test_suites_str = \"\\n\\n\".join(deepseek_extracted_test_suites)"
      ],
      "metadata": {
        "id": "Sn43flYS8cv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_seek_pattern_analyzer_results = pattern_analyzer.analyze_test_suite(deepseek_extracted_test_suites_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY9xpGGY8SNP",
        "outputId": "4285a7d3-1e4d-4d44-eeea-22f26236ea65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
            "assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
            "with pytest.raises(TypeError):\n",
            "assert separate_paren_groups('(()()) ((())) () ((())()())') == ['()', '(())', '(()())', '((()))', '(((())))']\n",
            "assert separate_paren_groups('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
            "with pytest.raises(TypeError):\n",
            "assert below_zero([]) == False\n",
            "assert below_zero([1, -2, 2, -2, 5, -5, 4, -4]) == True\n",
            "with pytest.raises(TypeError):\n",
            "assert abs(mean_absolute_deviation([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n",
            "assert abs(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n",
            "with pytest.raises(TypeError):\n",
            "assert intersperse([], 7) == []\n",
            "assert intersperse([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\n",
            "assert intersperse([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n",
            "with pytest.raises(TypeError):\n",
            "assert parse_nested_parens('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n",
            "assert parse_nested_parens('(()(())((())))') == [4]\n",
            "with pytest.raises(TypeError):\n",
            "assert filter_by_substring([], 'john') == []\n",
            "assert filter_by_substring(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
            "assert filter_by_substring(['xxx', 'asd', 'aaaxxy', 'john doe', 'xxxAAA', 'xxx'], 'xx') == ['xxx', 'aaaxxy', 'xxxAAA', 'xxx']\n",
            "assert filter_by_substring(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
            "assert filter_by_substring(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
            "with pytest.raises(TypeError):\n",
            "assert sum_product([]) == (0, 1)\n",
            "assert sum_product([1, 1, 1]) == (3, 1)\n",
            "assert sum_product([100, 0]) == (100, 0)\n",
            "assert sum_product([3, 5, 7]) == (3 + 5 + 7, 3 * 5 * 7)\n",
            "assert sum_product([10]) == (10, 10)\n",
            "assert rolling_max([]) == []\n",
            "assert rolling_max([1]) == [1]\n",
            "assert rolling_max([1, 2, 3, 4, 5]) == [1, 2, 3, 4, 5]\n",
            "assert rolling_max([5, 4, 3, 2, 1]) == [5, 4, 3, 2, 1]\n",
            "assert rolling_max(list(range(1, 10**6+1))) == list(range(1, 10**6+1))\n",
            "assert rolling_max([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "assert rolling_max([0, -1, -2, -3, -2, -1, 0]) == [0, 0, 0, 0, 0, 0, 0]\n",
            "assert rolling_max([1]*1000000) == [1]*1000000\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "assert is_palindrome('') == True\n",
            "assert is_palindrome('a') == True\n",
            "assert is_palindrome('aa') == True\n",
            "assert is_palindrome('ab') == False\n",
            "assert make_palindrome('') == ''\n",
            "assert make_palindrome('x') == 'x'\n",
            "assert make_palindrome('xyz') == 'xyzyx'\n",
            "assert make_palindrome('xyx') == 'xyx'\n",
            "assert make_palindrome('jerry') == 'jerryrrej'\n",
            "assert make_palindrome('') == ''\n",
            "assert make_palindrome('a' * 1000) == 'a' * 1000 + 'a' * 1000\n",
            "assert make_palindrome('jerry') == 'jerryrrej'\n",
            "with pytest.raises(TypeError):\n",
            "assert string_xor('111000', '101010') == '010010'\n",
            "assert string_xor('0101', '0000') == '0101'\n",
            "with pytest.raises(TypeError):\n",
            "assert longest([]) == None\n",
            "assert longest(['a', 'b', 'c']) == 'a'\n",
            "assert longest(['a', 'bb', 'ccc']) == 'ccc'\n",
            "assert longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\n",
            "assert time.time() - start_time < 0.1  # The function should complete in less than 0.1 seconds\n",
            "with pytest.raises(TypeError):\n",
            "assert greatest_common_divisor(3, 7) == 1\n",
            "assert greatest_common_divisor(10, 15) == 5\n",
            "assert greatest_common_divisor(49, 14) == 7\n",
            "assert greatest_common_divisor(144, 60) == 12\n",
            "assert greatest_common_divisor(0, 1) == 1\n",
            "assert greatest_common_divisor(1, 0) == 1\n",
            "assert greatest_common_divisor(0, 0) == 0\n",
            "assert greatest_common_divisor(2, 1) == 1\n",
            "assert greatest_common_divisor(1, 2) == 1\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "assert all_prefixes('') == []\n",
            "assert all_prefixes('WWW') == ['W', 'WW', 'WWW']\n",
            "with pytest.raises(TypeError):\n",
            "assert string_sequence(0) == '0'\n",
            "assert string_sequence(10000) == '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000'\n",
            "assert string_sequence(1) == '0 1'\n",
            "assert string_sequence(10) == '0 1 2 3 4 5 6 7 8 9 10'\n",
            "assert count_distinct_characters('') == 0\n",
            "assert count_distinct_characters('abcde') == 5\n",
            "assert count_distinct_characters('abcde' + 'cade' + 'CADE') == 5\n",
            "assert count_distinct_characters('aaaaAAAAaaaa') == 1\n",
            "assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\n",
            "assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\n",
            "with pytest.raises(TypeError):\n",
            "assert parse_music('') == []\n",
            "assert parse_music('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\n",
            "with pytest.raises(TypeError):\n",
            "assert how_many_times('', 'x') == 0\n",
            "assert how_many_times('xyxyxyx', 'x') == 4\n",
            "assert how_many_times('cacacacac', 'cac') == 4\n",
            "assert how_many_times('john doe', 'john') == 1\n",
            "assert how_many_times('abcabcabcabc', 'abc') == 5\n",
            "assert how_many_times('aaaaaaaaaa', 'aa') == 5\n",
            "assert how_many_times('john doe', 'john') == 1\n",
            "assert how_many_times('abcabcabcabc', 'abc') == 5\n",
            "assert how_many_times('aaaaaaaaaa', 'aa') == 5\n",
            "assert how_many_times('aaaaaaaaaaa', 'aaaaaaaaaa') == 2\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "assert sort_numbers('') == ''\n",
            "assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\n",
            "with pytest.raises(TypeError):\n",
            "assert sort_numbers('three one five') == 'one three five'\n",
            "assert sort_numbers('three five nine') == 'three five nine'\n",
            "assert sort_numbers('five zero four seven nine eight') == 'zero four five seven eight nine'\n",
            "assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\n",
            "assert find_closest_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) == (2.0, 2.2)\n",
            "assert find_closest_elements([1.1, 2.2, 3.1, 4.1, 5.1]) == (2.2, 3.1)\n",
            "with pytest.raises(TypeError):\n",
            "assert rescale_to_unit([2.0, 49.9]) == [0.0, 1.0]\n",
            "assert rescale_to_unit([12.0, 11.0, 15.0, 13.0, 14.0]) == [0.25, 0.0, 1.0, 0.5, 0.75]\n",
            "assert filter_integers([3, 'c', 3, 3, 'a', 'b']) == [3, 3, 3]\n",
            "with pytest.raises(TypeError):\n",
            "assert strlen('') == 0\n",
            "assert strlen('asdasnakj') == 9\n",
            "with pytest.raises(TypeError):\n",
            "assert largest_divisor(3) == 1\n",
            "assert largest_divisor(49) == 7\n",
            "with pytest.raises(TypeError):\n",
            "assert factorize(2) == [2]\n",
            "assert factorize(4) == [2, 2]\n",
            "assert factorize(8) == [2, 2, 2]\n",
            "assert factorize(3 * 19) == [3, 19]\n",
            "assert factorize(3 * 19 * 3 * 19) == [3, 3, 19, 19]\n",
            "assert factorize(3 * 19 * 3 * 19 * 3 * 19) == [3, 3, 3, 19, 19, 19]\n",
            "assert factorize(3 * 19 * 19 * 19) == [3, 19, 19, 19]\n",
            "assert factorize(3 * 2 * 3) == [2, 3, 3]\n",
            "assert factorize(1) == []\n",
            "assert factorize(0) == []\n",
            "assert factorize(-1) == []\n",
            "assert factorize(product) == factors\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(TypeError):\n",
            "with pytest.raises(ValueError):\n",
            "assert remove_duplicates([]) == []\n",
            "assert remove_duplicates([1, 2, 3, 2, 4, 3, 5]) == [1, 2, 3, 4, 5]\n",
            "with pytest.raises(TypeError):\n",
            "assert flip_case('') == ''\n",
            "assert flip_case('Hello') == 'hELLO'\n",
            "assert flip_case('Hello!') == 'hELLO!'\n",
            "assert flip_case('These violent delights have violent ends') == 'tHESE VIOLENT DELIGHTS HAVE VIOLENT ENDS'\n",
            "with pytest.raises(TypeError):\n",
            "assert concatenate([]) == ''\n",
            "assert concatenate(['a']) == 'a'\n",
            "assert concatenate(['a', 'b', 'c']) == 'abc'\n",
            "assert concatenate(['a'] * 10000) == 'a' * 10000\n",
            "assert concatenate(['a', ' ', 'b', 'c']) == 'a bc'\n",
            "assert concatenate(['a', '@', '#', 'c']) == 'a@#c'\n",
            "assert filter_by_prefix([], 'a') == []\n",
            "assert filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a') == ['abc', 'array']\n",
            "assert len(filter_by_prefix(large_list, 'a')) == 2\n",
            "assert filter_by_prefix(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
            "with pytest.raises(TypeError):\n",
            "assert get_positive([-1, -2, 4, 5, 6]) == [4, 5, 6]\n",
            "assert get_positive([]) == []\n",
            "with pytest.raises(TypeError):\n",
            "assert is_prime(6) == False\n",
            "assert is_prime(101) == True\n",
            "assert is_prime(11) == True\n",
            "assert is_prime(13441) == False # 13441 is not a prime number, it's divisible by 113 and 114\n",
            "assert is_prime(61) == True\n",
            "assert is_prime(4) == False\n",
            "assert is_prime(1) == False\n",
            "assert is_prime(0) == False  # 0 is not a prime number\n",
            "assert is_prime(-1) == False  # negative numbers are not prime numbers\n",
            "assert is_prime(13441 * 19) == False  # product of a prime number and a non-prime number is not a prime number\n",
            "assert is_prime(5 * 17) == False  # product of two prime numbers is a prime number\n",
            "assert is_prime(11 * 7) == False  # product of two prime numbers is a prime number\n",
            "assert math.fabs(poly(coeffs, solution)) < 1e-4\n",
            "assert math.fabs(find_zero(coeffs) - 1.7678) < 1e-4  # The root of this polynomial is approximately 1.7678\n",
            "with pytest.raises(TypeError):\n",
            "assert tuple(sort_third([1, 2, 3])) == tuple(sort_third([1, 2, 3]))\n",
            "assert tuple(sort_third([5, 6, 3, 4, 8, 9, 2, 1])) == tuple([2, 6, 3, 4, 8, 9, 5, 1])\n",
            "with pytest.raises(TypeError):\n",
            "assert unique([5, 3, 5, 2, 3, 3, 9, 0, 123]) == [0, 2, 3, 5, 9, 123]\n",
            "assert unique([]) == []  # Empty list\n",
            "assert unique([1]) == [1]  # Single element\n",
            "assert unique([1, 1, 1, 1]) == [1]  # All elements are the same\n",
            "with pytest.raises(TypeError):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in deep_seek_pattern_analyzer_results:\n",
        "  print(item)\n",
        "  print(deep_seek_pattern_analyzer_results[item])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3_OtH6G_buo",
        "outputId": "c0d8debd-1912-4d66-f79f-a8b78f161b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boundary_testing\n",
            "{'total_matches': 35, 'coverage_ratio': 0.18617021276595744, 'pattern_breakdown': {'zero_values': 24, 'negative_values': 9, 'large_values': 6}}\n",
            "functionality\n",
            "{'total_matches': 48, 'coverage_ratio': 0.2553191489361702, 'pattern_breakdown': {'complex_input': 48}}\n",
            "error_handling\n",
            "{'total_matches': 40, 'coverage_ratio': 0.2127659574468085, 'pattern_breakdown': {'exception_testing': 40}}\n",
            "edge_cases\n",
            "{'total_matches': 37, 'coverage_ratio': 0.19680851063829788, 'pattern_breakdown': {'empty_input': 29, 'single_element': 9, 'null_input': 1}}\n",
            "overall\n",
            "{'total_assertions': 188, 'patterns_per_assertion': 0.8829787234042553, 'pattern_coverage': 0.8, 'uncategorized': 53, 'uncategorized_assertions': [\"assert is_palindrome('a') == True\", \"assert is_palindrome('aa') == True\", \"assert is_palindrome('ab') == False\", \"assert make_palindrome('x') == 'x'\", \"assert make_palindrome('xyz') == 'xyzyx'\", \"assert make_palindrome('xyx') == 'xyx'\", \"assert make_palindrome('jerry') == 'jerryrrej'\", \"assert make_palindrome('a' * 1000) == 'a' * 1000 + 'a' * 1000\", \"assert make_palindrome('jerry') == 'jerryrrej'\", \"assert string_xor('0101', '0000') == '0101'\", 'assert greatest_common_divisor(3, 7) == 1', 'assert greatest_common_divisor(10, 15) == 5', 'assert greatest_common_divisor(49, 14) == 7', 'assert greatest_common_divisor(144, 60) == 12', 'assert greatest_common_divisor(2, 1) == 1', 'assert greatest_common_divisor(1, 2) == 1', \"assert count_distinct_characters('abcde') == 5\", \"assert count_distinct_characters('abcde' + 'cade' + 'CADE') == 5\", \"assert count_distinct_characters('aaaaAAAAaaaa') == 1\", \"assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\", \"assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\", \"assert how_many_times('xyxyxyx', 'x') == 4\", \"assert how_many_times('cacacacac', 'cac') == 4\", \"assert how_many_times('john doe', 'john') == 1\", \"assert how_many_times('abcabcabcabc', 'abc') == 5\", \"assert how_many_times('aaaaaaaaaa', 'aa') == 5\", \"assert how_many_times('john doe', 'john') == 1\", \"assert how_many_times('abcabcabcabc', 'abc') == 5\", \"assert how_many_times('aaaaaaaaaa', 'aa') == 5\", \"assert how_many_times('aaaaaaaaaaa', 'aaaaaaaaaa') == 2\", \"assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\", \"assert sort_numbers('three one five') == 'one three five'\", \"assert sort_numbers('three five nine') == 'three five nine'\", \"assert sort_numbers('five zero four seven nine eight') == 'zero four five seven eight nine'\", \"assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\", \"assert strlen('asdasnakj') == 9\", 'assert largest_divisor(3) == 1', 'assert largest_divisor(49) == 7', 'assert factorize(4) == [2, 2]', 'assert factorize(3 * 19) == [3, 19]', 'assert factorize(product) == factors', \"assert flip_case('Hello') == 'hELLO'\", \"assert flip_case('Hello!') == 'hELLO!'\", \"assert flip_case('These violent delights have violent ends') == 'tHESE VIOLENT DELIGHTS HAVE VIOLENT ENDS'\", \"assert len(filter_by_prefix(large_list, 'a')) == 2\", 'assert is_prime(6) == False', 'assert is_prime(101) == True', 'assert is_prime(11) == True', 'assert is_prime(61) == True', 'assert is_prime(4) == False', 'assert is_prime(1) == False', 'assert is_prime(5 * 17) == False  # product of two prime numbers is a prime number', 'assert is_prime(11 * 7) == False  # product of two prime numbers is a prime number']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_extracted_test_suites_str = \"\\n\\n\".join(semcoder_extracted_test_suites)"
      ],
      "metadata": {
        "id": "qu52Ve05-2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semcoder_pattern_analyzer_results = pattern_analyzer.analyze_test_suite(semcoder_extracted_test_suites_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3oktFKL-6lI",
        "outputId": "26d5b810-1c49-4f82-988c-4848bdd20872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
            "assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
            "with pytest.raises(TypeError):\n",
            "assert separate_paren_groups('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
            "with pytest.raises(TypeError):\n",
            "assert truncate_number(3.5) == 0.5\n",
            "assert abs(truncate_number(123.456) - 0.456) < 1e-6\n",
            "with pytest.raises(TypeError):\n",
            "assert below_zero([]) == False\n",
            "assert below_zero([1, -2, 2, -2, 5, -5, 4, -4]) == True\n",
            "with pytest.raises(TypeError):\n",
            "assert abs(mean_absolute_deviation([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n",
            "assert abs(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n",
            "with pytest.raises(TypeError):\n",
            "assert intersperse([], 7) == []\n",
            "assert intersperse([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n",
            "with pytest.raises(TypeError):\n",
            "assert parse_nested_parens('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n",
            "assert parse_nested_parens('(()(())((())))') == [4]\n",
            "with pytest.raises(TypeError):\n",
            "assert filter_by_substring([], 'john') == []\n",
            "assert filter_by_substring(['grunt', 'trumpet', 'prune', 'gruesome'], 'run') == ['grunt', 'prune']\n",
            "with pytest.raises(TypeError):\n",
            "assert sum_product([]) == (0, 1)\n",
            "assert sum_product([10]) == (10, 10)\n",
            "with pytest.raises(TypeError):\n",
            "assert rolling_max([]) == []\n",
            "assert rolling_max([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "with pytest.raises(TypeError):\n",
            "assert make_palindrome('') == ''\n",
            "assert make_palindrome('jerry') == 'jerryrrej'\n",
            "with pytest.raises(TypeError):\n",
            "assert string_xor('111000', '101010') == '010010'\n",
            "assert string_xor('0101', '0000') == '0101'\n",
            "with pytest.raises(TypeError):\n",
            "assert longest([]) == None\n",
            "assert longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\n",
            "with pytest.raises(TypeError):\n",
            "assert greatest_common_divisor(3, 7) == 1\n",
            "assert greatest_common_divisor(144, 60) == 12\n",
            "with pytest.raises(TypeError):\n",
            "assert all_prefixes('') == []\n",
            "assert all_prefixes('WWW') == ['W', 'WW', 'WWW']\n",
            "with pytest.raises(TypeError):\n",
            "assert string_sequence(0) == '0'\n",
            "assert string_sequence(10) == '0 1 2 3 4 5 6 7 8 9 10'\n",
            "with pytest.raises(TypeError):\n",
            "assert count_distinct_characters('') == 0\n",
            "assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\n",
            "with pytest.raises(TypeError):\n",
            "assert parse_music('') == []\n",
            "assert parse_music('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\n",
            "with pytest.raises(TypeError):\n",
            "assert how_many_times('', 'x') == 0\n",
            "assert how_many_times('john doe', 'john') == 1\n",
            "with pytest.raises(TypeError):\n",
            "assert sort_numbers('') == ''\n",
            "assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\n",
            "with pytest.raises(TypeError):\n",
            "assert find_closest_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) == (3.9, 4.0)\n",
            "assert find_closest_elements([1.1, 2.2, 3.1, 4.1, 5.1]) == (2.2, 3.1)\n",
            "with pytest.raises(TypeError):\n",
            "assert rescale_to_unit([2.0, 49.9]) == [0.0, 1.0]\n",
            "assert rescale_to_unit([12.0, 11.0, 15.0, 13.0, 14.0]) == [0.25, 0.0, 1.0, 0.5, 0.75]\n",
            "with pytest.raises(TypeError):\n",
            "assert filter_integers([]) == []\n",
            "assert filter_integers([3, 'c', 3, 3, 'a', 'b']) == [3, 3, 3]\n",
            "with pytest.raises(TypeError):\n",
            "assert strlen('') == 0\n",
            "assert strlen('asdasnakj') == 9\n",
            "with pytest.raises(TypeError):\n",
            "assert largest_divisor(3) == 1\n",
            "assert largest_divisor(49) == 7\n",
            "with pytest.raises(TypeError):\n",
            "assert factorize(2) == [2]\n",
            "assert factorize(3 * 2 * 3) == [2, 3, 3]\n",
            "with pytest.raises(TypeError):\n",
            "assert remove_duplicates([]) == []\n",
            "assert remove_duplicates([1, 2, 3, 2, 4, 3, 5]) == [1, 4, 5]\n",
            "with pytest.raises(TypeError):\n",
            "assert flip_case('') == ''\n",
            "assert flip_case('These violent delights have violent ends') == 'tHESE VIOLENT DELIGHTS HAVE VIOLENT ENDS'\n",
            "with pytest.raises(TypeError):\n",
            "assert concatenate([]) == ''\n",
            "assert concatenate(['x', 'y', 'z', 'w', 'k']) == 'xyzwk'\n",
            "with pytest.raises(TypeError):\n",
            "assert filter_by_prefix([], 'john') == []\n",
            "assert filter_by_prefix(['xxx', 'asd', 'xxy', 'john doe', 'xxxAAA', 'xxx'], 'xxx') == ['xxx', 'xxxAAA', 'xxx']\n",
            "with pytest.raises(TypeError):\n",
            "assert get_positive([-1, -2, 4, 5, 6]) == [4, 5, 6]\n",
            "assert get_positive([]) == []\n",
            "with pytest.raises(TypeError):\n",
            "assert is_prime(6) == False\n",
            "assert is_prime(13441 * 19) == False\n",
            "with pytest.raises(TypeError):\n",
            "assert math.fabs(poly(coeffs, solution)) < 1e-4\n",
            "assert math.fabs(poly(coeffs, solution)) < 1e-4\n",
            "with pytest.raises(TypeError):\n",
            "assert tuple(sort_third([1, 2, 3])) == tuple(sort_third([1, 2, 3]))\n",
            "assert tuple(sort_third([5, 6, 3, 4, 8, 9, 2, 1])) == tuple([2, 6, 3, 4, 8, 9, 5, 1])\n",
            "with pytest.raises(TypeError):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in semcoder_pattern_analyzer_results:\n",
        "  print(item)\n",
        "  print(semcoder_pattern_analyzer_results[item])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gbCoWfUHGq5",
        "outputId": "4128517a-7250-4c59-a738-01664833332a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boundary_testing\n",
            "{'total_matches': 21, 'coverage_ratio': 0.2079207920792079, 'pattern_breakdown': {'zero_values': 15, 'negative_values': 7, 'large_values': 2}}\n",
            "functionality\n",
            "{'total_matches': 25, 'coverage_ratio': 0.24752475247524752, 'pattern_breakdown': {'complex_input': 25}}\n",
            "error_handling\n",
            "{'total_matches': 34, 'coverage_ratio': 0.33663366336633666, 'pattern_breakdown': {'exception_testing': 34}}\n",
            "edge_cases\n",
            "{'total_matches': 24, 'coverage_ratio': 0.2376237623762376, 'pattern_breakdown': {'empty_input': 22, 'single_element': 3, 'null_input': 1}}\n",
            "overall\n",
            "{'total_assertions': 101, 'patterns_per_assertion': 1.0792079207920793, 'pattern_coverage': 0.8, 'uncategorized': 12, 'uncategorized_assertions': [\"assert make_palindrome('jerry') == 'jerryrrej'\", \"assert string_xor('0101', '0000') == '0101'\", 'assert greatest_common_divisor(3, 7) == 1', 'assert greatest_common_divisor(144, 60) == 12', \"assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\", \"assert how_many_times('john doe', 'john') == 1\", \"assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\", \"assert strlen('asdasnakj') == 9\", 'assert largest_divisor(3) == 1', 'assert largest_divisor(49) == 7', \"assert flip_case('These violent delights have violent ends') == 'tHESE VIOLENT DELIGHTS HAVE VIOLENT ENDS'\", 'assert is_prime(6) == False']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-4 Results"
      ],
      "metadata": {
        "id": "n-zTCUcPqVG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28 #the latest version is acting crazy weird- ugh rollback"
      ],
      "metadata": {
        "id": "J7UIRuVY5yUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_humaneval_plus_tests(\"gpt-4\", deep_seek_tokenizer=None, num_total_tests=100) #gpt-4 doesn't have reasoning training; only gpt-4o- interesting case comparison"
      ],
      "metadata": {
        "id": "RqivJLJgt7Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4_extracted_test_suites = process_file_path(\"/content/gpt-4_test_case_generation_results.txt\")"
      ],
      "metadata": {
        "id": "RnPzHwt--LKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_test_suite(\"gpt-4\",dataset, 21, gpt_4_extracted_test_suites)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR92q3lV_0l5",
        "outputId": "af723a26-839f-425d-885e-a217b3247f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM 0:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
            "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
            "    given threshold.\n",
            "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
            "    False\n",
            "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
            "    True\n",
            "    \"\"\"\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                distance = abs(elem - elem2)\n",
            "                if distance < threshold:\n",
            "                    return True\n",
            "\n",
            "    return False\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_has_close_elements_1():\n",
            "    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
            "test_has_close_elements_1()\n",
            "def test_has_close_elements_2():\n",
            "    assert has_close_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
            "test_has_close_elements_2()\n",
            "def test_has_close_elements_3():\n",
            "    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
            "test_has_close_elements_3()\n",
            "def test_has_close_elements_4():\n",
            "    assert has_close_elements([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
            "test_has_close_elements_4()\n",
            "def test_has_close_elements_5():\n",
            "    assert has_close_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
            "test_has_close_elements_5()\n",
            "def test_has_close_elements_6():\n",
            "    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
            "test_has_close_elements_6()\n",
            "def test_has_close_elements_7():\n",
            "    assert has_close_elements([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
            "test_has_close_elements_7()\n",
            "def test_has_close_elements_error_1():\n",
            "    with pytest.raises(TypeError):\n",
            "        has_close_elements(None, 0.5)\n",
            "test_has_close_elements_error_1()\n",
            "def test_has_close_elements_error_2():\n",
            "    with pytest.raises(TypeError):\n",
            "        has_close_elements([1.0, \"2.0\", 3.0], 0.5)\n",
            "test_has_close_elements_error_2()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n",
            "PROBLEM 1:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def separate_paren_groups(paren_string: str) -> List[str]:\n",
            "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
            "    separate those group into separate strings and return the list of those.\n",
            "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
            "    Ignore any spaces in the input string.\n",
            "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
            "    ['()', '(())', '(()())']\n",
            "    \"\"\"\n",
            "    result = []\n",
            "    current_string = []\n",
            "    current_depth = 0\n",
            "\n",
            "    for c in paren_string:\n",
            "        if c == '(':\n",
            "            current_depth += 1\n",
            "            current_string.append(c)\n",
            "        elif c == ')':\n",
            "            current_depth -= 1\n",
            "            current_string.append(c)\n",
            "\n",
            "            if current_depth == 0:\n",
            "                result.append(''.join(current_string))\n",
            "                current_string.clear()\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_separate_paren_groups_perf():\n",
            "    assert separate_paren_groups('(()()) ((())) () ((())()())') == ['(()())', '((()))', '()', '((())()())']\n",
            "test_separate_paren_groups_perf()\n",
            "def test_separate_paren_groups_edge():\n",
            "    assert separate_paren_groups('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
            "    assert separate_paren_groups('()') == ['()']  # single group\n",
            "    assert separate_paren_groups('') == []  # empty string\n",
            "    assert separate_paren_groups('   ') == []  # string with spaces only\n",
            "    assert separate_paren_groups('( )( )') == ['()', '()']  # several single groups\n",
            "test_separate_paren_groups_edge()\n",
            "def test_separate_paren_groups_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        separate_paren_groups(None)  # test with None\n",
            "    with pytest.raises(TypeError):\n",
            "        separate_paren_groups(123)  # test with integer\n",
            "    with pytest.raises(TypeError):\n",
            "        separate_paren_groups(['(', ')'])  # test with list\n",
            "test_separate_paren_groups_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 2:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def truncate_number(number: float) -> float:\n",
            "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
            "    and integer part (largest integer smaller than given number) and decimals\n",
            "    (leftover part always smaller than 1).\n",
            "\n",
            "    Return the decimal part of the number.\n",
            "    >>> truncate_number(3.5)\n",
            "    0.5\n",
            "    \"\"\"\n",
            "    return number % 1.0\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_truncate_number():\n",
            "    assert truncate_number(3.5) == 0.5\n",
            "test_truncate_number()\n",
            "def test_truncate_number_edge():\n",
            "    assert abs(truncate_number(123.456789123) - Decimal('0.456789123')) < 1e-6\n",
            "test_truncate_number_edge()\n",
            "def test_truncate_number_integer():\n",
            "    assert truncate_number(100) == 0.0\n",
            "test_truncate_number_integer()\n",
            "def test_truncate_number_small():\n",
            "    assert truncate_number(0.0000001) == 0.0000001\n",
            "test_truncate_number_small()\n",
            "def test_truncate_number_performance():\n",
            "    assert abs(truncate_number(1.123456789123456789) - Decimal('0.123456789123456789')) < 1e-6\n",
            "test_truncate_number_performance()\n",
            "def test_truncate_number_error_none():\n",
            "    with pytest.raises(TypeError):\n",
            "        truncate_number(None)\n",
            "test_truncate_number_error_none()\n",
            "def test_truncate_number_error_str():\n",
            "    with pytest.raises(TypeError):\n",
            "        truncate_number(\"123.456\")\n",
            "test_truncate_number_error_str()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 3:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def below_zero(operations: List[int]) -> bool:\n",
            "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
            "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
            "    at that point function should return True. Otherwise it should return False.\n",
            "    >>> below_zero([1, 2, 3])\n",
            "    False\n",
            "    >>> below_zero([1, 2, -4, 5])\n",
            "    True\n",
            "    \"\"\"\n",
            "    balance = 0\n",
            "\n",
            "    for op in operations:\n",
            "        balance += op\n",
            "        if balance < 0:\n",
            "            return True\n",
            "\n",
            "    return False\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "def test_below_zero_empty():\n",
            "    assert below_zero([]) == False\n",
            "test_below_zero_empty()\n",
            "def test_below_zero_no_below_zero():\n",
            "    assert below_zero([1, 2, -3, 1, 2, -3]) == False\n",
            "test_below_zero_no_below_zero()\n",
            "def test_below_zero_below_zero():\n",
            "    assert below_zero([1, 2, -4, 5, 6]) == True\n",
            "test_below_zero_below_zero()\n",
            "def test_below_zero_zero_balance():\n",
            "    assert below_zero([1, -1, 2, -2, 5, -5, 4, -4]) == False\n",
            "test_below_zero_zero_balance()\n",
            "def test_below_zero_negative_balance():\n",
            "    assert below_zero([1, -1, 2, -2, 5, -5, 4, -5]) == True\n",
            "test_below_zero_negative_balance()\n",
            "def test_below_zero_edge():\n",
            "    assert below_zero([1, -2, 2, -2, 5, -5, 4, -4]) == True\n",
            "test_below_zero_edge()\n",
            "def test_below_zero_error():\n",
            "    try:\n",
            "        below_zero(None)\n",
            "        assert False, \"Expected TypeError\"\n",
            "    except TypeError:\n",
            "        assert True\n",
            "test_below_zero_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n",
            "PROBLEM 4:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
            "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
            "    around the mean of this dataset.\n",
            "    Mean Absolute Deviation is the average absolute difference between each\n",
            "    element and a centerpoint (mean in this case):\n",
            "    MAD = average | x - x_mean |\n",
            "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
            "    1.0\n",
            "    \"\"\"\n",
            "    mean = sum(numbers) / len(numbers)\n",
            "    return sum(abs(x - mean) for x in numbers) / len(numbers)\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_mean_absolute_deviation_perf():\n",
            "    large_list = list(range(1, 1001))  # large list for performance testing\n",
            "    expected = sum(abs(x - (1001 / 2)) for x in large_list) / 1000\n",
            "    assert abs(mean_absolute_deviation(large_list) - expected) < 1e-6\n",
            "test_mean_absolute_deviation_perf()\n",
            "def test_mean_absolute_deviation_edge():\n",
            "    # testing with an empty list\n",
            "    assert mean_absolute_deviation([]) == 0\n",
            "    # testing with a single element\n",
            "    assert mean_absolute_deviation([5.0]) == 0\n",
            "test_mean_absolute_deviation_edge()\n",
            "def test_mean_absolute_deviation_error():\n",
            "    # testing with a None as input\n",
            "    with pytest.raises(TypeError):\n",
            "        mean_absolute_deviation(None)\n",
            "    # testing with a string as input\n",
            "    with pytest.raises(TypeError):\n",
            "        mean_absolute_deviation(\"1, 2, 3\")\n",
            "test_mean_absolute_deviation_error()\n",
            "def test_mean_absolute_deviation_example():\n",
            "    assert abs(mean_absolute_deviation([1.0, 2.0, 3.0]) - 2.0/3.0) < 1e-6\n",
            "    assert abs(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]) - 1.0) < 1e-6\n",
            "    assert abs(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0, 5.0]) - 6.0/5.0) < 1e-6\n",
            "test_mean_absolute_deviation_example()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 5:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def intersperse(numbers: List[int], delimeter: int) -> List[int]:\n",
            "    \"\"\" Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n",
            "    >>> intersperse([], 4)\n",
            "    []\n",
            "    >>> intersperse([1, 2, 3], 4)\n",
            "    [1, 4, 2, 4, 3]\n",
            "    \"\"\"\n",
            "    if not numbers:\n",
            "        return []\n",
            "\n",
            "    result = []\n",
            "\n",
            "    for n in numbers[:-1]:\n",
            "        result.append(n)\n",
            "        result.append(delimeter)\n",
            "\n",
            "    result.append(numbers[-1])\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_intersperse_empty():\n",
            "    assert intersperse([], 7) == []\n",
            "test_intersperse_empty()\n",
            "def test_intersperse_normal():\n",
            "    assert intersperse([5, 6, 3, 2], 8) == [5, 8, 6, 8, 3, 8, 2]\n",
            "test_intersperse_normal()\n",
            "def test_intersperse_same_numbers():\n",
            "    assert intersperse([2, 2, 2], 2) == [2, 2, 2, 2, 2]\n",
            "test_intersperse_same_numbers()\n",
            "def test_intersperse_performance():\n",
            "    import time\n",
            "    start = time.time()\n",
            "    intersperse(list(range(10**6)), 7)\n",
            "    assert time.time() - start < 1  # Should run in under 1 second\n",
            "test_intersperse_performance()\n",
            "def test_intersperse_edge():\n",
            "    assert intersperse([2], 2) == [2]\n",
            "test_intersperse_edge()\n",
            "def test_intersperse_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        intersperse(None, 4)\n",
            "test_intersperse_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 6:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def parse_nested_parens(paren_string: str) -> List[int]:\n",
            "    \"\"\" Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
            "    For each of the group, output the deepest level of nesting of parentheses.\n",
            "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
            "\n",
            "    >>> parse_nested_parens('(()()) ((())) () ((())()())')\n",
            "    [2, 3, 1, 3]\n",
            "    \"\"\"\n",
            "    def parse_paren_group(s):\n",
            "        depth = 0\n",
            "        max_depth = 0\n",
            "        for c in s:\n",
            "            if c == '(':\n",
            "                depth += 1\n",
            "                max_depth = max(depth, max_depth)\n",
            "            else:\n",
            "                depth -= 1\n",
            "\n",
            "        return max_depth\n",
            "\n",
            "    return [parse_paren_group(x) for x in paren_string.split(' ') if x]\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_parse_nested_parens():\n",
            "    assert parse_nested_parens('(()()) ((())) () ((())()())') == [2, 3, 1, 3]\n",
            "    assert parse_nested_parens('() (()) ((())) (((())))') == [1, 2, 3, 4]\n",
            "    assert parse_nested_parens('(()(())((())))') == [4]\n",
            "test_parse_nested_parens()\n",
            "def test_parse_nested_parens_perf():\n",
            "    long_string = '(()()) ' * 10000  # Repeat the string 10000 times\n",
            "    expected_output = [2] * 10000  # Expect the output to be list of 10000 twos\n",
            "    assert parse_nested_parens(long_string) == expected_output\n",
            "test_parse_nested_parens_perf()\n",
            "def test_parse_nested_parens_edge():\n",
            "    assert parse_nested_parens('()') == [1]  # Single level of nesting\n",
            "    assert parse_nested_parens('') == []  # Empty string\n",
            "test_parse_nested_parens_edge()\n",
            "def test_parse_nested_parens_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        parse_nested_parens(None)\n",
            "    with pytest.raises(TypeError):\n",
            "        parse_nested_parens(123)  # Non-string input\n",
            "test_parse_nested_parens_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 7:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def filter_by_substring(strings: List[str], substring: str) -> List[str]:\n",
            "    \"\"\" Filter an input list of strings only for ones that contain given substring\n",
            "    >>> filter_by_substring([], 'a')\n",
            "    []\n",
            "    >>> filter_by_substring(['abc', 'bacd', 'cde', 'array'], 'a')\n",
            "    ['abc', 'bacd', 'array']\n",
            "    \"\"\"\n",
            "    return [x for x in strings if substring in x]\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_filter_by_substring_perf():\n",
            "    import time\n",
            "    import string\n",
            "    import random\n",
            "    # Generate a large list of random strings\n",
            "    strings = [''.join(random.choices(string.ascii_lowercase, k=5)) for _ in range(100000)]\n",
            "    start_time = time.time()\n",
            "    filter_by_substring(strings, 'a')\n",
            "    elapsed_time = time.time() - start_time\n",
            "    assert elapsed_time < 1  # for example, ensure the function returns within one second\n",
            "test_filter_by_substring_perf()\n",
            "def test_filter_by_substring_edge():\n",
            "    # Test with strings that contains numbers, symbols and non-ascii characters\n",
            "    assert filter_by_substring(['123', '@@@', ''], '1') == ['123']\n",
            "    assert filter_by_substring(['123', '@@@', ''], '@') == ['@@@']\n",
            "    assert filter_by_substring(['123', '@@@', ''], '') == ['']\n",
            "    # Test with empty string as substring\n",
            "    assert filter_by_substring(['abc', 'def', 'ghi'], '') == ['abc', 'def', 'ghi']\n",
            "test_filter_by_substring_edge()\n",
            "def test_filter_by_substring_error():\n",
            "    # Test with non-list input\n",
            "    with pytest.raises(TypeError):\n",
            "        filter_by_substring('abc', 'a')\n",
            "    # Test with non-string elements in the input list\n",
            "    with pytest.raises(TypeError):\n",
            "        filter_by_substring(['abc', 123, 'def'], 'a')\n",
            "    # Test with non-string substring\n",
            "    with pytest.raises(TypeError):\n",
            "        filter_by_substring(['abc', 'def', 'ghi'], 123)\n",
            "test_filter_by_substring_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 8:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def sum_product(numbers: List[int]) -> Tuple[int, int]:\n",
            "    \"\"\" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.\n",
            "    Empty sum should be equal to 0 and empty product should be equal to 1.\n",
            "    >>> sum_product([])\n",
            "    (0, 1)\n",
            "    >>> sum_product([1, 2, 3, 4])\n",
            "    (10, 24)\n",
            "    \"\"\"\n",
            "    sum_value = 0\n",
            "    prod_value = 1\n",
            "\n",
            "    for n in numbers:\n",
            "        sum_value += n\n",
            "        prod_value *= n\n",
            "    return sum_value, prod_value\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_sum_product_empty():\n",
            "    assert sum_product([]) == (0, 1)\n",
            "test_sum_product_empty()\n",
            "def test_sum_product_single():\n",
            "    assert sum_product([10]) == (10, 10)\n",
            "test_sum_product_single()\n",
            "def test_sum_product_multiple():\n",
            "    assert sum_product([1, 2, 3, 4]) == (10, 24)\n",
            "test_sum_product_multiple()\n",
            "def test_sum_product_zeros():\n",
            "    assert sum_product([0, 0, 0]) == (0, 0)\n",
            "test_sum_product_zeros()\n",
            "def test_sum_product_negative():\n",
            "    assert sum_product([-1, -2, -3]) == (-6, -6)\n",
            "test_sum_product_negative()\n",
            "def test_sum_product_mixed():\n",
            "    assert sum_product([-1, 2, -3, 4]) == (2, 24)\n",
            "test_sum_product_mixed()\n",
            "def test_sum_product_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        sum_product(None)\n",
            "test_sum_product_error()\n",
            "def test_sum_product_performance():\n",
            "    import time\n",
            "    start_time = time.time()\n",
            "    assert sum_product(list(range(1, 1000000))) == (499999500000, 0)\n",
            "    end_time = time.time() - start_time\n",
            "    assert end_time < 5\n",
            "test_sum_product_performance()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 9:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def rolling_max(numbers: List[int]) -> List[int]:\n",
            "    \"\"\" From a given list of integers, generate a list of rolling maximum element found until given moment\n",
            "    in the sequence.\n",
            "    >>> rolling_max([1, 2, 3, 2, 3, 4, 2])\n",
            "    [1, 2, 3, 3, 3, 4, 4]\n",
            "    \"\"\"\n",
            "    running_max = None\n",
            "    result = []\n",
            "\n",
            "    for n in numbers:\n",
            "        if running_max is None:\n",
            "            running_max = n\n",
            "        else:\n",
            "            running_max = max(running_max, n)\n",
            "\n",
            "        result.append(running_max)\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_rolling_max_empty():\n",
            "    assert rolling_max([]) == []\n",
            "test_rolling_max_empty()\n",
            "def test_rolling_max_increasing():\n",
            "    assert rolling_max([1, 2, 3, 4]) == [1, 2, 3, 4]\n",
            "test_rolling_max_increasing()\n",
            "def test_rolling_max_decreasing():\n",
            "    assert rolling_max([4, 3, 2, 1]) == [4, 4, 4, 4]\n",
            "test_rolling_max_decreasing()\n",
            "def test_rolling_max_random():\n",
            "    assert rolling_max([3, 2, 3, 100, 3]) == [3, 3, 3, 100, 100]\n",
            "test_rolling_max_random()\n",
            "def test_rolling_max_performance():\n",
            "    import random\n",
            "    numbers = random.choices(range(1, 100), k=10000)\n",
            "    assert rolling_max(numbers) == sorted(numbers)\n",
            "test_rolling_max_performance()\n",
            "def test_rolling_max_edge():\n",
            "    assert rolling_max([100] * 10000) == [100] * 10000\n",
            "test_rolling_max_edge()\n",
            "def test_rolling_max_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        rolling_max(None)\n",
            "test_rolling_max_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 10:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def is_palindrome(string: str) -> bool:\n",
            "    \"\"\" Test if given string is a palindrome \"\"\"\n",
            "    return string == string[::-1]\n",
            "\n",
            "\n",
            "def make_palindrome(string: str) -> str:\n",
            "    \"\"\" Find the shortest palindrome that begins with a supplied string.\n",
            "    Algorithm idea is simple:\n",
            "    - Find the longest postfix of supplied string that is a palindrome.\n",
            "    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\n",
            "    >>> make_palindrome('')\n",
            "    ''\n",
            "    >>> make_palindrome('cat')\n",
            "    'catac'\n",
            "    >>> make_palindrome('cata')\n",
            "    'catac'\n",
            "    \"\"\"\n",
            "    if not string:\n",
            "        return ''\n",
            "\n",
            "    beginning_of_suffix = 0\n",
            "\n",
            "    while not is_palindrome(string[beginning_of_suffix:]):\n",
            "        beginning_of_suffix += 1\n",
            "\n",
            "    return string + string[:beginning_of_suffix][::-1]\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_is_palindrome():\n",
            "    assert is_palindrome('racecar') == True\n",
            "    assert is_palindrome('python') == False\n",
            "    assert is_palindrome('') == True\n",
            "    assert is_palindrome('a') == True\n",
            "    with pytest.raises(TypeError):\n",
            "        is_palindrome(None)\n",
            "test_is_palindrome()\n",
            "def test_make_palindrome_perf():\n",
            "    import time\n",
            "    start_time = time.time()\n",
            "    make_palindrome('a'*10**6)  # Test with a large input\n",
            "    assert (time.time() - start_time) < 5  # The function should finish within 5 seconds\n",
            "test_make_palindrome_perf()\n",
            "def test_make_palindrome_edge():\n",
            "    assert make_palindrome('a') == 'a'\n",
            "    assert make_palindrome('ab') == 'aba'\n",
            "    assert make_palindrome('abc') == 'abcba'\n",
            "    assert make_palindrome('jerry') == 'jerryrrej'\n",
            "test_make_palindrome_edge()\n",
            "def test_make_palindrome_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        make_palindrome(None)\n",
            "    with pytest.raises(TypeError):\n",
            "        make_palindrome(123)\n",
            "    with pytest.raises(TypeError):\n",
            "        make_palindrome(['a', 'b'])\n",
            "test_make_palindrome_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 11:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def string_xor(a: str, b: str) -> str:\n",
            "    \"\"\" Input are two strings a and b consisting only of 1s and 0s.\n",
            "    Perform binary XOR on these inputs and return result also as a string.\n",
            "    >>> string_xor('010', '110')\n",
            "    '100'\n",
            "    \"\"\"\n",
            "    def xor(i, j):\n",
            "        if i == j:\n",
            "            return '0'\n",
            "        else:\n",
            "            return '1'\n",
            "\n",
            "    return ''.join(xor(x, y) for x, y in zip(a, b))\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_string_xor():\n",
            "    assert string_xor('111000', '101010') == '010010'\n",
            "    assert string_xor('1', '1') == '0'\n",
            "    assert string_xor('0101', '0000') == '0101'\n",
            "test_string_xor()\n",
            "def test_string_xor_perf():\n",
            "    assert string_xor('1' * 100000, '0' * 100000) == '1' * 100000\n",
            "test_string_xor_perf()\n",
            "def test_string_xor_edge():\n",
            "    assert string_xor('', '') == ''\n",
            "    assert string_xor('0', '0') == '0'\n",
            "    assert string_xor('1', '1') == '0'\n",
            "test_string_xor_edge()\n",
            "def test_string_xor_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        string_xor(None, '0')\n",
            "    with pytest.raises(TypeError):\n",
            "        string_xor('0', None)\n",
            "    with pytest.raises(ValueError):\n",
            "        string_xor('2', '0')\n",
            "    with pytest.raises(ValueError):\n",
            "        string_xor('0', '2')\n",
            "test_string_xor_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 12:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List, Optional\n",
            "\n",
            "\n",
            "def longest(strings: List[str]) -> Optional[str]:\n",
            "    \"\"\" Out of list of strings, return the longest one. Return the first one in case of multiple\n",
            "    strings of the same length. Return None in case the input list is empty.\n",
            "    >>> longest([])\n",
            "\n",
            "    >>> longest(['a', 'b', 'c'])\n",
            "    'a'\n",
            "    >>> longest(['a', 'bb', 'ccc'])\n",
            "    'ccc'\n",
            "    \"\"\"\n",
            "    if not strings:\n",
            "        return None\n",
            "\n",
            "    maxlen = max(len(x) for x in strings)\n",
            "    for s in strings:\n",
            "        if len(s) == maxlen:\n",
            "            return s\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "def test_longest_perf():\n",
            "    assert longest([\"a\"*i for i in range(10000)]) == \"a\"*9999\n",
            "test_longest_perf()\n",
            "def test_longest_edge():\n",
            "    assert longest(['x', 'yyy', 'zzzz', 'www', 'kkkk', 'abc']) == 'zzzz'\n",
            "    assert longest(['a', 'b', 'c']) == 'a'\n",
            "    assert longest(['a', 'bb', 'ccc']) == 'ccc'\n",
            "test_longest_edge()\n",
            "def test_longest_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        longest(None)\n",
            "    with pytest.raises(TypeError):\n",
            "        longest(123)\n",
            "test_longest_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 13:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def greatest_common_divisor(a: int, b: int) -> int:\n",
            "    \"\"\" Return a greatest common divisor of two integers a and b\n",
            "    >>> greatest_common_divisor(3, 5)\n",
            "    1\n",
            "    >>> greatest_common_divisor(25, 15)\n",
            "    5\n",
            "    \"\"\"\n",
            "    while b:\n",
            "        a, b = b, a % b\n",
            "    return a\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "def test_greatest_common_divisor():\n",
            "    assert greatest_common_divisor(3, 7) == 1\n",
            "    assert greatest_common_divisor(10, 15) == 5\n",
            "    assert greatest_common_divisor(49, 14) == 7\n",
            "    assert greatest_common_divisor(144, 60) == 12\n",
            "test_greatest_common_divisor()\n",
            "def test_greatest_common_divisor_perf():\n",
            "    import time\n",
            "    start = time.time()\n",
            "    assert greatest_common_divisor(10**6, 10**5) == 10**5\n",
            "    assert time.time() - start < 1, \"Performance test failed\"\n",
            "test_greatest_common_divisor_perf()\n",
            "def test_greatest_common_divisor_edge():\n",
            "    assert greatest_common_divisor(0, 1) == 1\n",
            "    assert greatest_common_divisor(1, 0) == 1\n",
            "    assert greatest_common_divisor(0, 0) == 0\n",
            "test_greatest_common_divisor_edge()\n",
            "def test_greatest_common_divisor_error():\n",
            "    import pytest\n",
            "    with pytest.raises(TypeError):\n",
            "        greatest_common_divisor(None, 2)\n",
            "    with pytest.raises(TypeError):\n",
            "        greatest_common_divisor(2, None)\n",
            "    with pytest.raises(TypeError):\n",
            "        greatest_common_divisor(None, None)\n",
            "test_greatest_common_divisor_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 14:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def all_prefixes(string: str) -> List[str]:\n",
            "    \"\"\" Return list of all prefixes from shortest to longest of the input string\n",
            "    >>> all_prefixes('abc')\n",
            "    ['a', 'ab', 'abc']\n",
            "    \"\"\"\n",
            "    result = []\n",
            "\n",
            "    for i in range(len(string)):\n",
            "        result.append(string[:i+1])\n",
            "    return result\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_all_prefixes_empty_string():\n",
            "    assert all_prefixes('') == []\n",
            "test_all_prefixes_empty_string()\n",
            "def test_all_prefixes_single_character():\n",
            "    assert all_prefixes('a') == ['a']\n",
            "test_all_prefixes_single_character()\n",
            "def test_all_prefixes_multiple_characters():\n",
            "    assert all_prefixes('abcd') == ['a', 'ab', 'abc', 'abcd']\n",
            "test_all_prefixes_multiple_characters()\n",
            "def test_all_prefixes_same_character():\n",
            "    assert all_prefixes('aaaa') == ['a', 'aa', 'aaa', 'aaaa']\n",
            "test_all_prefixes_same_character()\n",
            "def test_all_prefixes_perf():\n",
            "    long_string = 'a' * 10**6\n",
            "    result = all_prefixes(long_string)\n",
            "    assert len(result) == len(long_string)\n",
            "    assert result[0] == 'a'\n",
            "    assert result[-1] == long_string\n",
            "test_all_prefixes_perf()\n",
            "def test_all_prefixes_edge():\n",
            "    assert all_prefixes('WWW') == ['W', 'WW', 'WWW']\n",
            "test_all_prefixes_edge()\n",
            "def test_all_prefixes_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        all_prefixes(None)\n",
            "test_all_prefixes_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 15:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def string_sequence(n: int) -> str:\n",
            "    \"\"\" Return a string containing space-delimited numbers starting from 0 upto n inclusive.\n",
            "    >>> string_sequence(0)\n",
            "    '0'\n",
            "    >>> string_sequence(5)\n",
            "    '0 1 2 3 4 5'\n",
            "    \"\"\"\n",
            "    return ' '.join([str(x) for x in range(n + 1)])\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_string_sequence_perf():\n",
            "    # Performance Test\n",
            "    # Testing with large input value to check the performance of the function\n",
            "    assert string_sequence(1000000) == ' '.join(map(str, range(1000001)))\n",
            "test_string_sequence_perf()\n",
            "def test_string_sequence_edge():\n",
            "    # Edge Case Tests\n",
            "    # Testing with negative value, 0 and 1 as they are edge cases\n",
            "    assert string_sequence(-1) == ''\n",
            "    assert string_sequence(0) == '0'\n",
            "    assert string_sequence(1) == '0 1'\n",
            "test_string_sequence_edge()\n",
            "def test_string_sequence_error():\n",
            "    # Error Test\n",
            "    # Testing with None and a string as the function only accepts integers\n",
            "    with pytest.raises(TypeError):\n",
            "        string_sequence(None)\n",
            "    with pytest.raises(TypeError):\n",
            "        string_sequence(\"test\")\n",
            "test_string_sequence_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': True}\n",
            "PROBLEM 16:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def count_distinct_characters(string: str) -> int:\n",
            "    \"\"\" Given a string, find out how many distinct characters (regardless of case) does it consist of\n",
            "    >>> count_distinct_characters('xyzXYZ')\n",
            "    3\n",
            "    >>> count_distinct_characters('Jerry')\n",
            "    4\n",
            "    \"\"\"\n",
            "    return len(set(string.lower()))\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "def test_count_distinct_characters_perf():\n",
            "    # Long string of repeated characters\n",
            "    assert count_distinct_characters('a' * 1000000) == 1\n",
            "    # Long string of unique characters\n",
            "    assert count_distinct_characters(''.join(chr(i) for i in range(65, 91)) * 10000) == 26\n",
            "test_count_distinct_characters_perf()\n",
            "def test_count_distinct_characters_edge():\n",
            "    # Single character string\n",
            "    assert count_distinct_characters('a') == 1\n",
            "    # String with space\n",
            "    assert count_distinct_characters('ab c') == 3\n",
            "    # String with numbers\n",
            "    assert count_distinct_characters('abc123') == 6\n",
            "    # String with punctuation\n",
            "    assert count_distinct_characters('abc.!') == 5\n",
            "    # String with multiple instances of same character\n",
            "    assert count_distinct_characters('Jerry jERRY JeRRRY') == 5\n",
            "test_count_distinct_characters_edge()\n",
            "def test_count_distinct_characters_error():\n",
            "    # None as input\n",
            "    with pytest.raises(TypeError):\n",
            "        count_distinct_characters(None)\n",
            "    # Integer as input\n",
            "    with pytest.raises(TypeError):\n",
            "        count_distinct_characters(123)\n",
            "    # List as input\n",
            "    with pytest.raises(TypeError):\n",
            "        count_distinct_characters(['a', 'b', 'c'])\n",
            "test_count_distinct_characters_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 17:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def parse_music(music_string: str) -> List[int]:\n",
            "    \"\"\" Input to this function is a string representing musical notes in a special ASCII format.\n",
            "    Your task is to parse this string and return list of integers corresponding to how many beats does each\n",
            "    not last.\n",
            "\n",
            "    Here is a legend:\n",
            "    'o' - whole note, lasts four beats\n",
            "    'o|' - half note, lasts two beats\n",
            "    '.|' - quater note, lasts one beat\n",
            "\n",
            "    >>> parse_music('o o| .| o| o| .| .| .| .| o o')\n",
            "    [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\n",
            "    \"\"\"\n",
            "    note_map = {'o': 4, 'o|': 2, '.|': 1}\n",
            "    return [note_map[x] for x in music_string.split(' ') if x]\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "def test_parse_music_perf():\n",
            "    long_string = 'o ' * 50000 + 'o| ' * 50000 + '.| ' * 50000\n",
            "    expected_output = [4]*50000 + [2]*50000 + [1]*50000\n",
            "    assert parse_music(long_string) == expected_output\n",
            "test_parse_music_perf()\n",
            "def test_parse_music_edge():\n",
            "    assert parse_music('o') == [4]\n",
            "    assert parse_music('o|') == [2]\n",
            "    assert parse_music('.|') == [1]\n",
            "    assert parse_music('o o| .| o| o| .| .| .| .| o o') == [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\n",
            "test_parse_music_edge()\n",
            "def test_parse_music_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        parse_music(None)\n",
            "    with pytest.raises(TypeError):\n",
            "        parse_music(123)\n",
            "    with pytest.raises(TypeError):\n",
            "        parse_music([1,2,3])\n",
            "test_parse_music_error()\n",
            "def test_parse_music_value():\n",
            "    assert parse_music('o o| .| o| o| .| .| .| .| o o') == [4, 2, 1, 2, 2, 1, 1, 1, 1, 4, 4]\n",
            "    assert parse_music('o o o o') == [4, 4, 4, 4]\n",
            "    assert parse_music('.| .| .| .|') == [1, 1, 1, 1]\n",
            "    assert parse_music('o| o| .| .| o o o o') == [2, 2, 1, 1, 4, 4, 4, 4]\n",
            "    assert parse_music('o| .| o| .| o o| o o|') == [2, 1, 2, 1, 4, 2, 4, 2]\n",
            "test_parse_music_value()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 18:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "\n",
            "\n",
            "def how_many_times(string: str, substring: str) -> int:\n",
            "    \"\"\" Find how many times a given substring can be found in the original string. Count overlaping cases.\n",
            "    >>> how_many_times('', 'a')\n",
            "    0\n",
            "    >>> how_many_times('aaa', 'a')\n",
            "    3\n",
            "    >>> how_many_times('aaaa', 'aa')\n",
            "    3\n",
            "    \"\"\"\n",
            "    times = 0\n",
            "\n",
            "    for i in range(len(string) - len(substring) + 1):\n",
            "        if string[i:i+len(substring)] == substring:\n",
            "            times += 1\n",
            "\n",
            "    return times\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_how_many_times_examples():\n",
            "    assert how_many_times('', 'x') == 0\n",
            "    assert how_many_times('xyxyxyx', 'x') == 4\n",
            "    assert how_many_times('cacacacac', 'cac') == 4\n",
            "    assert how_many_times('john doe', 'john') == 1\n",
            "test_how_many_times_examples()\n",
            "def test_how_many_times_perf():\n",
            "    import string\n",
            "    import random\n",
            "    letters = string.ascii_lowercase\n",
            "    random_str = ''.join(random.choice(letters) for i in range(10000))\n",
            "    assert how_many_times(random_str, 'a') >= 0\n",
            "test_how_many_times_perf()\n",
            "def test_how_many_times_edge():\n",
            "    assert how_many_times('', '') == 0\n",
            "    assert how_many_times('a', 'a') == 1\n",
            "    assert how_many_times('aaaaa', 'a') == 5\n",
            "    assert how_many_times('aaaaa', 'aa') == 4\n",
            "    assert how_many_times('aaaaa', 'aaa') == 3\n",
            "    assert how_many_times('aaaaa', 'aaaa') == 2\n",
            "    assert how_many_times('aaaaa', 'aaaaa') == 1\n",
            "    assert how_many_times('aaaaa', 'aaaaaa') == 0\n",
            "test_how_many_times_edge()\n",
            "def test_how_many_times_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        how_many_times(None, 'a')\n",
            "    with pytest.raises(TypeError):\n",
            "        how_many_times('a', None)\n",
            "    with pytest.raises(TypeError):\n",
            "        how_many_times(None, None)\n",
            "test_how_many_times_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 19:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List\n",
            "\n",
            "\n",
            "def sort_numbers(numbers: str) -> str:\n",
            "    \"\"\" Input is a space-delimited string of numberals from 'zero' to 'nine'.\n",
            "    Valid choices are 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight' and 'nine'.\n",
            "    Return the string with numbers sorted from smallest to largest\n",
            "    >>> sort_numbers('three one five')\n",
            "    'one three five'\n",
            "    \"\"\"\n",
            "    value_map = {\n",
            "        'zero': 0,\n",
            "        'one': 1,\n",
            "        'two': 2,\n",
            "        'three': 3,\n",
            "        'four': 4,\n",
            "        'five': 5,\n",
            "        'six': 6,\n",
            "        'seven': 7,\n",
            "        'eight': 8,\n",
            "        'nine': 9\n",
            "    }\n",
            "    return ' '.join(sorted([x for x in numbers.split(' ') if x], key=lambda x: value_map[x]))\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "import pytest\n",
            "\n",
            "def test_sort_numbers_perf():\n",
            "    long_input = ' '.join(['nine']*10**6)   # repeat 'nine' a million times\n",
            "    assert sort_numbers(long_input) == long_input\n",
            "test_sort_numbers_perf()\n",
            "def test_sort_numbers_edge():\n",
            "    assert sort_numbers('six five four three two one zero') == 'zero one two three four five six'\n",
            "    assert sort_numbers('zero one two three four five six') == 'zero one two three four five six'\n",
            "    assert sort_numbers('one') == 'one'\n",
            "    assert sort_numbers('') == ''\n",
            "test_sort_numbers_edge()\n",
            "def test_sort_numbers_error():\n",
            "    with pytest.raises(TypeError):\n",
            "        sort_numbers(None)\n",
            "    with pytest.raises(TypeError):\n",
            "        sort_numbers(123)\n",
            "    with pytest.raises(TypeError):\n",
            "        sort_numbers(['one', 'two', 'three'])\n",
            "test_sort_numbers_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n",
            "PROBLEM 20:\n",
            "\n",
            "CANONICAL SOLUTION:\n",
            "\n",
            "from typing import List, Tuple\n",
            "\n",
            "\n",
            "def find_closest_elements(numbers: List[float]) -> Tuple[float, float]:\n",
            "    \"\"\" From a supplied list of numbers (of length at least two) select and return two that are the closest to each\n",
            "    other and return them in order (smaller number, larger number).\n",
            "    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.2])\n",
            "    (2.0, 2.2)\n",
            "    >>> find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0])\n",
            "    (2.0, 2.0)\n",
            "    \"\"\"\n",
            "    closest_pair = None\n",
            "    distance = None\n",
            "\n",
            "    for idx, elem in enumerate(numbers):\n",
            "        for idx2, elem2 in enumerate(numbers):\n",
            "            if idx != idx2:\n",
            "                if distance is None:\n",
            "                    distance = abs(elem - elem2)\n",
            "                    closest_pair = tuple(sorted([elem, elem2]))\n",
            "                else:\n",
            "                    new_distance = abs(elem - elem2)\n",
            "                    if new_distance < distance:\n",
            "                        distance = new_distance\n",
            "                        closest_pair = tuple(sorted([elem, elem2]))\n",
            "\n",
            "    return closest_pair\n",
            "\n",
            "\n",
            "CLEANED TESTS:\n",
            "\n",
            "def test_find_closest_elements():\n",
            "    assert find_closest_elements([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) == (3.9, 4.0)\n",
            "    assert find_closest_elements([1.0, 2.0, 5.9, 4.0, 5.0]) == (5.0, 5.9)\n",
            "    assert find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.2]) == (2.0, 2.2)\n",
            "    assert find_closest_elements([1.0, 2.0, 3.0, 4.0, 5.0, 2.0]) == (2.0, 2.0)\n",
            "    assert find_closest_elements([1.1, 2.2, 3.1, 4.1, 5.1]) == (2.2, 3.1)\n",
            "test_find_closest_elements()\n",
            "def test_find_closest_elements_perf():\n",
            "    import time\n",
            "    start_time = time.time()\n",
            "    find_closest_elements([float(i)/100 for i in range(10000)])\n",
            "    assert time.time() - start_time < 1  # function should return within 1 second\n",
            "test_find_closest_elements_perf()\n",
            "def test_find_closest_elements_edge():\n",
            "    assert find_closest_elements([1.1, 1.100001]) == (1.1, 1.100001)\n",
            "    assert find_closest_elements([1.1, 1.1]) == (1.1, 1.1)\n",
            "test_find_closest_elements_edge()\n",
            "def test_find_closest_elements_error():\n",
            "    import pytest\n",
            "    with pytest.raises(TypeError):\n",
            "        find_closest_elements(None)\n",
            "    with pytest.raises(ValueError):\n",
            "        find_closest_elements([1.0])  # should raise ValueError as the list must be of length at least two\n",
            "test_find_closest_elements_error()\n",
            "RESULT:\n",
            "{'syntax_valid': True, 'execution_success': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4_coverage_analyzer = TestCoverageAnalyzer()\n",
        "gpt_4_coverage_results = []\n",
        "for index, test_suite in enumerate(gpt_4_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    solution_file, test_file = gpt_4_coverage_analyzer.create_test_files(solution, test_suite, temp_dir)\n",
        "    result = gpt_4_coverage_analyzer.run_coverage_analysis(solution_file, test_file, temp_dir)\n",
        "    if 'line_coverage' in result:\n",
        "      gpt_4_coverage_results.append(result)\n",
        "print(calculate_aggregate_metrics(gpt_4_coverage_results, \"line_coverage\"))"
      ],
      "metadata": {
        "id": "tsGeq_KU-XjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_4_novelty_results = []\n",
        "for index, test_suite in enumerate(gpt_4_extracted_test_suites):\n",
        "  solution = dataset['test'][\"prompt\"][index] + dataset['test'][\"canonical_solution\"][index]\n",
        "  original_tests = dataset['test'][\"test\"][index]\n",
        "  result = analyze_novelty_with_claude(solution, test_suite, original_tests)\n",
        "  print(result)\n",
        "  gpt_4_novelty_results.append(result)\n",
        "print(calculate_aggregate_metrics(gpt_4_novelty_results, \"novelty_score\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0r4yMKr_Lk0",
        "outputId": "ba69dbc8-dda6-44c6-f4f5-e5f61847015b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling with invalid inputs (None, string)', 'Tests for edge cases with threshold values close to 0 and 1'], 'unique_scenarios': ['Tests with duplicate numbers in the input list', 'Tests with numbers very close to the threshold value'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including typical cases, edge cases, and error handling. However, some important edge cases and boundary conditions are still missing.', 'recommendations': ['Test with an empty list input', 'Test with a list containing only one element', 'Test with a list containing only duplicate elements', 'Test with a threshold value of 0', 'Test with a threshold value of a very large number']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling with invalid input types (None, int, list)', 'Tests for edge cases like empty string and string with only spaces'], 'unique_scenarios': ['Testing with None input', 'Testing with integer input', 'Testing with list input', 'Testing with empty string', 'Testing with string containing only spaces'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including typical cases, edge cases, and error handling. However, they do not cover nested parentheses or unbalanced parentheses cases.', 'recommendations': [\"Add tests for nested parentheses cases, e.g., '((()))'\", \"Add tests for unbalanced parentheses cases, e.g., '(())'\", 'Add tests for strings with non-parenthesis characters']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for edge cases like very small numbers and integers', 'Tests for error handling with invalid inputs like None and strings', 'Tests for precision and performance with many decimal places'], 'unique_scenarios': ['Truncating a number with many decimal places', 'Passing None as input', 'Passing a string as input', 'Truncating a very small number', 'Truncating an integer'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including edge cases, error handling, and precision/performance. However, some additional tests for negative numbers and very large numbers could improve coverage further.', 'recommendations': ['Add tests for negative numbers', 'Add tests for very large numbers (e.g., greater than sys.maxsize)', 'Consider adding tests for different floating-point representations (e.g., NaN, Infinity)']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for error handling (passing None as input)', 'Tests for edge case where balance reaches 0 but never goes below'], 'unique_scenarios': ['Empty list input', 'List with no negative balance', 'List with negative balance', 'List where balance reaches 0 but never goes below', 'List where balance goes below 0', 'Edge case where balance reaches -1', 'Error handling (passing None as input)'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases and error handling. However, it could benefit from additional tests for larger input lists and more diverse combinations of positive and negative values.', 'recommendations': ['Test with larger input lists (e.g., 100 or 1000 elements)', 'Test with input lists containing only positive values', 'Test with input lists containing only negative values', 'Test with input lists containing a mix of large positive and negative values']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Performance testing with a large input list', 'Testing with invalid input types (None, string)', 'Testing with edge cases (empty list, single element list)'], 'unique_scenarios': ['Large input list for performance testing', 'Empty list as input', 'Single element list as input', 'None as input', 'String as input'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases, invalid inputs, and performance testing. It complements the original test suite well.', 'recommendations': ['Test with lists containing negative numbers and/or floating-point numbers with more precision', 'Test with lists containing duplicate values', 'Test with very large or very small floating-point numbers to check for potential overflow/underflow issues']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Performance testing', 'Error handling for invalid input types'], 'unique_scenarios': ['Empty list input', 'List with same elements', 'List with a single element', 'Large input list for performance testing', 'Invalid input type (None)'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases like empty lists, lists with single or repeated elements, and large inputs for performance testing. It also introduces novel aspects like error handling for invalid input types. However, it lacks tests for negative delimiter values or other potential edge cases.', 'recommendations': ['Test with negative delimiter values', 'Test with non-integer delimiter values', 'Test with mixed data types in the input list']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with a large input string', 'Tests for error handling with invalid input types', 'Tests for edge cases like empty string and single level of nesting'], 'unique_scenarios': ['Large input string to test performance', 'Invalid input types like None and integer', 'Empty string input', 'Single level of nesting'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including typical cases, edge cases, performance, and error handling. It provides good coverage of the function's behavior.\", 'recommendations': ['Test cases with nested parentheses of varying depths to ensure correct handling of different nesting levels', 'Test cases with mixed valid and invalid input strings to ensure proper handling of invalid inputs']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests performance with a large input list', 'Tests with non-string inputs (numbers, symbols, non-ASCII characters)', 'Tests with empty string as substring', 'Tests error handling for invalid input types'], 'unique_scenarios': ['Performance test with large input list', 'Edge cases with non-string inputs and empty substring', 'Error handling for invalid input types'], 'coverage_assessment': 'The generated tests cover a good range of scenarios, including performance, edge cases, and error handling. However, they lack some common test cases for the main functionality.', 'recommendations': ['Add tests for the main functionality with typical string inputs and substrings', 'Test cases with substrings that are not present in any input string', 'Test cases with duplicate strings in the input list', 'Test cases with very long strings and substrings']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with large input', 'Tests for error handling (TypeError)', 'Tests for edge cases like all zeros and single element list'], 'unique_scenarios': ['Empty list', 'Single element list', 'List with all zeros', 'List with negative numbers', 'List with mixed positive and negative numbers', 'Large list for performance testing', 'Invalid input (None) to test error handling'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including edge cases, boundary conditions, and performance testing. It provides good coverage of the function's behavior.\", 'recommendations': ['Test with lists containing duplicate numbers', 'Test with extremely large or small integer values (overflow/underflow)', 'Test with non-integer input types to ensure proper error handling']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with large input', 'Tests for error handling (passing None)', 'Tests for edge case (all elements equal)'], 'unique_scenarios': ['Empty list', 'Increasing sequence', 'Decreasing sequence', 'Random sequence', 'Large input (10000 elements)', 'All elements equal (edge case)', 'Invalid input (None)'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including edge cases, error handling, and performance testing. It provides good coverage of the function's behavior.\", 'recommendations': ['Test with negative numbers', 'Test with duplicate numbers', 'Test with mixed positive and negative numbers']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for performance with large input', 'Tests for error handling with invalid input types'], 'unique_scenarios': ['Testing with large input string', 'Testing with None, integer, and list inputs', 'Testing with empty string and single character string'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases, error handling, and performance testing. However, it lacks some important test cases for the make_palindrome function.', 'recommendations': ['Test cases for strings that are already palindromes', 'Test cases with strings containing non-alphabetic characters', 'Test cases with Unicode strings']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with large input strings', 'Tests for error handling with invalid inputs (None, non-binary characters)'], 'unique_scenarios': ['Empty string inputs', 'Single character inputs', 'Large input strings', 'Invalid inputs (None, non-binary characters)'], 'coverage_assessment': \"The generated test suite covers a good range of scenarios, including edge cases, performance, and error handling. It provides a comprehensive assessment of the function's behavior.\", 'recommendations': ['Test cases with unequal length input strings', 'Test cases with mixed binary and non-binary characters in the input strings']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests performance with large input list', 'Tests error handling for invalid input types', 'Tests additional edge cases like empty list and lists with duplicate lengths'], 'unique_scenarios': ['Large input list with strings of increasing length', 'Input with None and integer types', 'Empty list input', 'Lists with duplicate string lengths'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including edge cases, error handling, and performance testing. It provides good coverage of the function's behavior.\", 'recommendations': ['Test with lists containing empty strings', 'Test with lists containing mixed string lengths, including duplicates', 'Test with non-string iterable inputs (e.g., lists of integers)']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Performance testing', 'Error handling and type checking', 'Edge cases with zero values'], 'unique_scenarios': ['Large input values to test performance', 'Handling None inputs to test error cases', 'Zero inputs to test edge cases'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including typical use cases, edge cases, performance, and error handling. It provides good coverage of the function's behavior.\", 'recommendations': ['Test cases with negative input values', 'Test cases with floating-point input values (if applicable)']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for empty string input', 'Tests for single character input', 'Tests for same character input', 'Performance test for large input', 'Tests for error handling (TypeError)'], 'unique_scenarios': ['Empty string input', 'Single character input', 'Same character input', 'Large input for performance testing', 'Invalid input (None) to test error handling'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases like empty and single character inputs, as well as performance and error handling. However, it lacks tests for inputs with special characters or non-ASCII characters.', 'recommendations': ['Add tests for inputs with special characters (e.g., punctuation, whitespace)', 'Add tests for inputs with non-ASCII characters (e.g., Unicode)', 'Consider adding tests for negative scenarios (e.g., invalid input types)']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Performance testing with large input value', 'Error handling tests for invalid input types', 'Edge case testing for negative input and 0/1 values'], 'unique_scenarios': ['Testing with large input value (1000000) to check performance', 'Testing with negative input value (-1)', 'Testing with None and string input types to check error handling'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases, error handling, and performance testing. It provides a more comprehensive coverage compared to the original test suite.', 'recommendations': ['Test with very large negative input values to further validate error handling', 'Test with floating-point input values to ensure proper type handling', 'Test with extremely large input values (beyond 1000000) to stress test performance limits']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with large inputs (long strings of repeated and unique characters)', 'Tests for error handling with invalid input types (None, integer, list)'], 'unique_scenarios': ['Long string of repeated characters', 'Long string of unique characters', 'Single character string', 'String with space', 'String with numbers', 'String with punctuation', 'String with multiple instances of same character', 'None as input', 'Integer as input', 'List as input'], 'coverage_assessment': \"The generated tests cover a wide range of scenarios, including edge cases, boundary conditions, and error handling. They provide good coverage of the function's behavior and expected outputs.\", 'recommendations': ['Test with an empty string input', 'Test with a string containing non-ASCII characters', 'Test with a string containing mixed cases and whitespace characters']}\n",
            "{'novelty_score': 0.7, 'novel_aspects': ['Tests for performance with large input strings', 'Tests for error handling with invalid input types', 'More comprehensive testing of different note combinations'], 'unique_scenarios': ['Extremely long input string to test performance', 'Invalid input types like None, integer, and list', 'Additional test cases for different note combinations'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases, error handling, and different note combinations. However, it lacks tests for empty input and some additional edge cases.', 'recommendations': ['Add test case for empty input string', 'Test case with invalid note characters in the input string', 'Test case with leading/trailing whitespace in the input string']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with a large input string', 'Tests for error handling with invalid inputs (None)', 'Tests for edge cases with empty strings and strings of different lengths'], 'unique_scenarios': ['Large random string input to test performance', 'Empty string as input', 'Single character string as input', 'String with repeated substrings of different lengths', 'Invalid inputs (None) to test error handling'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including edge cases, performance, and error handling. It provides good coverage of the function's behavior.\", 'recommendations': ['Test with non-string inputs (e.g., integers, lists) to ensure proper error handling', 'Test with Unicode strings and non-ASCII characters', 'Test with very large substrings that exceed the length of the input string']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests for performance with large input', 'Tests for error handling with invalid input types', 'Tests for empty input'], 'unique_scenarios': [\"Large input with repeated 'nine' string\", 'Invalid input types like None, integer, and list', 'Empty string input'], 'coverage_assessment': 'The generated test suite covers a good range of scenarios, including edge cases like empty input, and error handling for invalid input types. It also introduces a performance test with a large input. However, it lacks tests for inputs with duplicate numbers or inputs with invalid number strings.', 'recommendations': [\"Add tests for inputs with duplicate numbers (e.g., 'one two one three')\", \"Add tests for inputs with invalid number strings (e.g., 'ten', 'eleven')\", 'Add tests for inputs with mixed valid and invalid number strings']}\n",
            "{'novelty_score': 0.8, 'novel_aspects': ['Tests performance by measuring execution time', 'Tests error handling for invalid inputs (None, list with length < 2)', 'Tests edge cases with very close numbers and duplicate numbers'], 'unique_scenarios': ['Large input list to test performance', 'Input list with None value', 'Input list with length 1', 'Input list with very close numbers', 'Input list with duplicate numbers'], 'coverage_assessment': \"The generated test suite covers a wide range of scenarios, including typical cases, edge cases, error handling, and performance. It provides good coverage of the function's behavior.\", 'recommendations': ['Test with empty list input', 'Test with list containing non-numeric values', 'Test with very large numbers or numbers close to the limits of float representation']}\n",
            "{'mean_novelty_score': 0.7571428571428571, 'median_novelty_score': 0.8, 'min_novelty_score': 0.7, 'max_novelty_score': 0.8, 'std_dev': 0.05070925528371104, 'total_entries_analyzed': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_results_to_file(gpt_4_novelty_results, 'gpt_4_novelty_results.json')\n",
        "files.download('gpt_4_novelty_results.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lf9GWmVy_JFK",
        "outputId": "505c4fa2-4c37-457f-bc10-e4457f068ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0815b87a-ebf2-4bb0-b29c-ac2260a5cca9\", \"gpt_4_novelty_results.json\", 21486)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}